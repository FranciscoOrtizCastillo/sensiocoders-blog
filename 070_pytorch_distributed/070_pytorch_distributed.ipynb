{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/070_pytorch_distributed/070_pytorch_distributed.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pytorch Distribuido"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En este post aprenderemos a utilizar algunas de las estrategias que nos ofrece `Pytorch` para entrenar redes neuronales de manera distribuída. \n",
    "\n",
    "Empezamos entrenando un clasificador de imágenes en el dataset [EuroSAT](https://github.com/phelber/eurosat)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def setup(path='./data', test_size=0.2, random_state=42):\n",
    "\n",
    "    classes = sorted(os.listdir(path))\n",
    "\n",
    "    print(\"Generating images and labels ...\")\n",
    "    images, encoded = [], []\n",
    "    for ix, label in enumerate(classes):\n",
    "        _images = os.listdir(f'{path}/{label}')\n",
    "        images += [f'{path}/{label}/{img}' for img in _images]\n",
    "        encoded += [ix]*len(_images)\n",
    "    print(f'Number of images: {len(images)}')\n",
    "\n",
    "     # train / val split\n",
    "    print(\"Generating train / val splits ...\")\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "        images,\n",
    "        encoded,\n",
    "        stratify=encoded,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(\"Training samples: \", len(train_labels))\n",
    "    print(\"Validation samples: \", len(val_labels))\n",
    "    \n",
    "    return classes, train_images, train_labels, val_images, val_labels\n",
    "\n",
    "classes, train_images, train_labels, val_images, val_labels = setup('./data')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:24:37.760093Z",
     "start_time": "2021-07-07T14:24:37.379618Z"
    },
    "code_folding": [
     3
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from skimage import io \n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        img = io.imread(self.images[ix])[...,(3,2,1)]\n",
    "        img = torch.tensor(img / 4000, dtype=torch.float).clip(0,1).permute(2,0,1)  \n",
    "        label = torch.tensor(self.labels[ix], dtype=torch.long)        \n",
    "        return img, label\n",
    "    \n",
    "ds = {\n",
    "    'train': Dataset(train_images, train_labels),\n",
    "    'val': Dataset(val_images, val_labels)\n",
    "}\n",
    "\n",
    "batch_size = 1024\n",
    "dl = {\n",
    "    'train': torch.utils.data.DataLoader(ds['train'], batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True),\n",
    "    'val': torch.utils.data.DataLoader(ds['val'], batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:24:38.378790Z",
     "start_time": "2021-07-07T14:24:37.762201Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import timm\n",
    "\n",
    "model_names = timm.list_models('tf_efficientnet_b5*')\n",
    "model_names"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:24:38.627357Z",
     "start_time": "2021-07-07T14:24:38.380806Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_outputs=10, use_amp=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('tf_efficientnet_b5', pretrained=True, num_classes=n_outputs)\n",
    "        self.use_amp = use_amp\n",
    "\n",
    "    def forward(self, x, log=False):\n",
    "        if log:\n",
    "            print(x.shape)\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            return self.model(x)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:24:38.632819Z",
     "start_time": "2021-07-07T14:24:38.629013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def step(model, batch, device):\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    y_hat = model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    acc = (torch.argmax(y_hat, axis=1) == y).sum().item() / y.size(0)\n",
    "    return loss, acc\n",
    "\n",
    "def train_amp(model, dl, optimizer, epochs=10, device=\"cpu\", use_amp = True, prof=None, end=0):\n",
    "    model.to(device)\n",
    "    hist = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    for e in range(1, epochs+1):\n",
    "        # train\n",
    "        model.train()\n",
    "        l, a = [], []\n",
    "        bar = tqdm(dl['train'])\n",
    "        stop=False\n",
    "        for batch_idx, batch in enumerate(bar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # AMP\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                loss, acc = step(model, batch, device)\n",
    "            scaler.scale(loss).backward()\n",
    "            # gradient clipping \n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            l.append(loss.item())\n",
    "            a.append(acc)\n",
    "            bar.set_description(f\"training... loss {np.mean(l):.4f} acc {np.mean(a):.4f}\")\n",
    "            # profiling\n",
    "            if prof:\n",
    "                if batch_idx >= end:\n",
    "                    stop = True\n",
    "                    break\n",
    "                prof.step()  \n",
    "        hist['loss'].append(np.mean(l))\n",
    "        hist['acc'].append(np.mean(a))\n",
    "        if stop:\n",
    "            break\n",
    "        # eval\n",
    "        model.eval()\n",
    "        l, a = [], []\n",
    "        bar = tqdm(dl['val'])\n",
    "        with torch.no_grad():\n",
    "            for batch in bar:\n",
    "                loss, acc = step(model, batch, device)\n",
    "                l.append(loss.item())\n",
    "                a.append(acc)\n",
    "                bar.set_description(f\"evluating... loss {np.mean(l):.4f} acc {np.mean(a):.4f}\")\n",
    "        hist['val_loss'].append(np.mean(l))\n",
    "        hist['val_acc'].append(np.mean(a))\n",
    "        # log\n",
    "        log = f'Epoch {e}/{epochs}'\n",
    "        for k, v in hist.items():\n",
    "            log += f' {k} {v[-1]:.4f}'\n",
    "        print(log)\n",
    "        \n",
    "    return hist"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:24:38.701805Z",
     "start_time": "2021-07-07T14:24:38.634018Z"
    },
    "code_folding": [
     3
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "hist = train_amp(model, dl, optimizer, epochs=3, device=\"cpu\", use_amp=False)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:25:19.497142Z",
     "start_time": "2021-07-07T14:24:38.702801Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "hist = train_amp(model, dl, optimizer, epochs=3, device=\"cuda\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training... loss 1.3855 acc 0.7170: 100%|██████████| 22/22 [00:14<00:00,  1.50it/s]\n",
      "evluating... loss 5.1790 acc 0.3385: 100%|██████████| 6/6 [00:06<00:00,  1.07s/it]\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3 loss 1.3855 acc 0.7170 val_loss 5.1790 val_acc 0.3385\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training... loss 0.1676 acc 0.9461: 100%|██████████| 22/22 [00:12<00:00,  1.78it/s]\n",
      "evluating... loss 0.3942 acc 0.8925: 100%|██████████| 6/6 [00:03<00:00,  1.77it/s]\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2/3 loss 0.1676 acc 0.9461 val_loss 0.3942 val_acc 0.8925\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training... loss 0.0728 acc 0.9755: 100%|██████████| 22/22 [00:12<00:00,  1.77it/s]\n",
      "evluating... loss 0.2299 acc 0.9440: 100%|██████████| 6/6 [00:03<00:00,  1.75it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3/3 loss 0.0728 acc 0.9755 val_loss 0.2299 val_acc 0.9440\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usando las técnicas aprendidas en posts anteriores conseguimos entrenar nuestro modelo a unos 12 segundos por epoch. Nada mal para tratarse de una `EfficientNetB5`. A partir de aquí, las siguientes mejoras van a consistir en entrenar nuestro modelo de manera distribuida, es decir, usando varias `GPUs`. Vamos a ver algunos ejemplos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Parallel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esta estrategia consiste en copiar el modelo en cada una de las `GPUs` disponibles y dividir el *batch* entre ellas. Si tenemos 2 `GPUs` cada una verá la mitad de un batch, y por consiguiente deberíamos poder entrenar el doble de rápido (en teoría, a la práctica no es así ya que al introducir el entrenamiento distribuido se requiren de nuevas operaciones, copias entre `GPUs` y sincornizaciones que necesitas sus recursos)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model = Model()\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  model = torch.nn.DataParallel(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:29:51.610476Z",
     "start_time": "2021-07-07T14:29:51.208671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model.cuda()\n",
    "\n",
    "# cada gpu recibe la mitad del batch !\n",
    "output = model(torch.randn(32, 3, 32, 32).cuda(), log=True)\n",
    "\n",
    "output.size()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([16, 3, 32, 32])torch.Size([16, 3, 32, 32])\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:29:51.750885Z",
     "start_time": "2021-07-07T14:29:51.612034Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para aprovechar al máximo esta estrategia, multiplica el tamaño del *batch* óptimo por el número de `GPUs` disponibles."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "batch_size = 1024 * 2\n",
    "\n",
    "dl = {\n",
    "    'train': torch.utils.data.DataLoader(ds['train'], batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True),\n",
    "    'val': torch.utils.data.DataLoader(ds['val'], batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True)\n",
    "}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "hist = train_amp(model, dl, optimizer, epochs=3, device=\"cuda\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training... loss 2.2098 acc 0.6020: 100%|██████████| 11/11 [00:08<00:00,  1.24it/s]\n",
      "evluating... loss 8.0039 acc 0.2243: 100%|██████████| 3/3 [00:03<00:00,  1.25s/it]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3 loss 2.2098 acc 0.6020 val_loss 8.0039 val_acc 0.2243\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training... loss 0.2349 acc 0.9272: 100%|██████████| 11/11 [00:08<00:00,  1.30it/s]\n",
      "evluating... loss 4.9922 acc 0.4731: 100%|██████████| 3/3 [00:03<00:00,  1.22s/it]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2/3 loss 0.2349 acc 0.9272 val_loss 4.9922 val_acc 0.4731\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training... loss 0.0772 acc 0.9744: 100%|██████████| 11/11 [00:08<00:00,  1.29it/s]\n",
      "evluating... loss 1.2790 acc 0.7334: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3/3 loss 0.0772 acc 0.9744 val_loss 1.2790 val_acc 0.7334\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:30:21.259851Z",
     "start_time": "2021-07-07T14:29:51.876905Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perfecto, hemos conseguido reducir en un 25% el tiempo por *epoch*. Por contra, al usar un *batch size* más grande, la convergencia es más lenta (se podría ajustar el *learning rate* para compensar). Como puedes ver esta estrategia no es del todo óptima ya que no hemos conseguido doblar la velocidad. Esto es debido al funcionamiento de la estrategia `Data Parallel`, que requiere varias copias de datos y sincronización entre `GPUs`. Podemos mejorarlo con la siguiente estrategia."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distributed Data Parallel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta estrategia vamos a considerar cada `GPU` como un proceso independiente, lo cual evitará muchas copias de datos y sincronizaciones pesadas. Igual que antes, cada `GPU` tendrá una copia del modelo y verá una parte proporcional del dataset. Por ejemplo, si tenemos dos `GPUs` cada una verá la mitad de los datos. Debido al funcionamiento de esta estrategia no puede ejecutarse en un *notebook*, por lo que se implementa en el siguiente [script](https://github.com/juansensio/blog/blob/master/070_pytorch_distributed/ddp.py)."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-29T11:26:24.099098Z",
     "start_time": "2021-06-29T11:26:23.441434Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Parallel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por último vamos a ver otra estrategia para entrenar modelos en varias `GPUs`. En este caso, será el propio modelo el que esté distribuido, algunas capas estarán en una `GPU` mientras que otras vivirán en otra `GPU`. Para ello tendremos que especificar en qué `GPU` vive cada parte del modelo y comunicar los tensores correspondientes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class ModelParallel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, gpu1, gpu2, n_outputs=10, use_amp=True):\n",
    "        super().__init__()\n",
    "        resnet = timm.create_model('resnet50', pretrained=True, num_classes=n_outputs)\n",
    "        self.backbone1 = torch.nn.Sequential(*list(resnet.children())[:6]).to(gpu1)\n",
    "        self.backbone2 = torch.nn.Sequential(*list(resnet.children())[6:]).to(gpu2)\n",
    "        self.use_amp = use_amp\n",
    "        self.gpu1 = gpu1 \n",
    "        self.gpu2 = gpu2\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "            x = x.to(self.gpu1)\n",
    "            x = self.backbone1(x)\n",
    "            x = x.to(self.gpu2)\n",
    "            x = self.backbone2(x)\n",
    "            return x"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:25:50.435393Z",
     "start_time": "2021-07-07T14:25:50.431971Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "gpu1 = torch.device('cuda:0')\n",
    "gpu2 = torch.device('cuda:1')\n",
    "\n",
    "modelParallel = ModelParallel(gpu1, gpu2)\n",
    "\n",
    "output = modelParallel(torch.randn(32, 3, 32, 32))\n",
    "\n",
    "output.size()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/juan/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448255797/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:25:50.769261Z",
     "start_time": "2021-07-07T14:25:50.436137Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "modelParallel.backbone1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:25:50.773869Z",
     "start_time": "2021-07-07T14:25:50.770279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "modelParallel.backbone2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act3): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (2): SelectAdaptivePool2d (pool_type=avg, flatten=True)\n",
       "  (3): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:25:50.782890Z",
     "start_time": "2021-07-07T14:25:50.774781Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def step_mp(model, batch, device):\n",
    "    x, y = batch\n",
    "    y = y.to(device)\n",
    "    y_hat = model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    acc = (torch.argmax(y_hat, axis=1) == y).sum().item() / y.size(0)\n",
    "    return loss, acc\n",
    "\n",
    "def train_mp(model, dl, optimizer, device, epochs=10, use_amp = True, prof=None, end=0):\n",
    "    hist = {'loss': [], 'acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    for e in range(1, epochs+1):\n",
    "        # train\n",
    "        model.train()\n",
    "        l, a = [], []\n",
    "        bar = tqdm(dl['train'])\n",
    "        stop=False\n",
    "        for batch_idx, batch in enumerate(bar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # AMP\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                loss, acc = step(model, batch, device)\n",
    "            scaler.scale(loss).backward()\n",
    "            # gradient clipping \n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            l.append(loss.item())\n",
    "            a.append(acc)\n",
    "            bar.set_description(f\"training... loss {np.mean(l):.4f} acc {np.mean(a):.4f}\")\n",
    "            # profiling\n",
    "            if prof:\n",
    "                if batch_idx >= end:\n",
    "                    stop = True\n",
    "                    break\n",
    "                prof.step()  \n",
    "        hist['loss'].append(np.mean(l))\n",
    "        hist['acc'].append(np.mean(a))\n",
    "        if stop:\n",
    "            break\n",
    "        # eval\n",
    "        model.eval()\n",
    "        l, a = [], []\n",
    "        bar = tqdm(dl['val'])\n",
    "        with torch.no_grad():\n",
    "            for batch in bar:\n",
    "                loss, acc = step(model, batch, device)\n",
    "                l.append(loss.item())\n",
    "                a.append(acc)\n",
    "                bar.set_description(f\"evluating... loss {np.mean(l):.4f} acc {np.mean(a):.4f}\")\n",
    "        hist['val_loss'].append(np.mean(l))\n",
    "        hist['val_acc'].append(np.mean(a))\n",
    "        # log\n",
    "        log = f'Epoch {e}/{epochs}'\n",
    "        for k, v in hist.items():\n",
    "            log += f' {k} {v[-1]:.4f}'\n",
    "        print(log)\n",
    "        \n",
    "    return hist"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:25:50.792295Z",
     "start_time": "2021-07-07T14:25:50.783809Z"
    },
    "code_folding": [
     0,
     8
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "batch_size = 1024\n",
    "dl = {\n",
    "    'train': torch.utils.data.DataLoader(ds['train'], batch_size=batch_size, shuffle=True, num_workers=20, pin_memory=True),\n",
    "    'val': torch.utils.data.DataLoader(ds['val'], batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True)\n",
    "}\n",
    "modelParallel = ModelParallel(gpu1, gpu2)\n",
    "optimizer = torch.optim.Adam(modelParallel.parameters(), lr=1e-3)\n",
    "hist = train_mp(modelParallel, dl, optimizer, gpu2, epochs=3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training... loss 0.4446 acc 0.8822: 100%|██████████| 22/22 [00:06<00:00,  3.17it/s]\n",
      "evluating... loss 1.2834 acc 0.7487: 100%|██████████| 6/6 [00:03<00:00,  1.92it/s]\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3 loss 0.4446 acc 0.8822 val_loss 1.2834 val_acc 0.7487\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training... loss 0.1088 acc 0.9663: 100%|██████████| 22/22 [00:06<00:00,  3.19it/s]\n",
      "evluating... loss 0.5246 acc 0.8588: 100%|██████████| 6/6 [00:03<00:00,  1.94it/s]\n",
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2/3 loss 0.1088 acc 0.9663 val_loss 0.5246 val_acc 0.8588\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "training... loss 0.0846 acc 0.9739: 100%|██████████| 22/22 [00:06<00:00,  3.21it/s]\n",
      "evluating... loss 0.5750 acc 0.8438: 100%|██████████| 6/6 [00:03<00:00,  1.91it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3/3 loss 0.0846 acc 0.9739 val_loss 0.5750 val_acc 0.8438\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T14:26:12.850604Z",
     "start_time": "2021-07-07T14:25:50.793193Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se puede combinar con DDP, pero en este caso ya necesitaremos varias máquinas con varias `GPUs` cada una, dificultando también su correcta implementación. Por suerte, la librería `Pytorch Lightning` implementa todas estas estrategias de manera transparente para que podamos usarlas sin apenas cambios en nuestro código, pero eso lo veremos en el siguiente post."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "interpreter": {
   "hash": "dd3c0ff7553675f8399160a12fbc0c6054e9524c8e841ec60a211865d03cacc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}