{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increasing-scoop",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/066_ssl/ssl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-april",
   "metadata": {},
   "source": [
    "# Aprendizaje Auto-Supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-sharp",
   "metadata": {},
   "source": [
    "Hasta ahora hemos visto un montón de ejemplos y aplicaciones de redes neuronales, y en todas ellas hemos utilizado el mismo algoritmo de aprendizaje: el aprendizaje supervisado. Este tipo de aprendizaje se caracteriza por el acceso a un conjunto de datos etiquetado, consistente en pares de ejemplos de entradas y salidas. Un ejemplo muy sencillo de entender es el entrenamiento de un clasificador de imágenes, por ejemplo usando el dataset MNIST, que está compuesto por un conjunto de imágenes y cada una de ellas viene acompañada por su correspondiente etiqueta. Esto nos permite comparar para cada imagen del dataset la salida de nuestra red con la etiqueta real, el *ground truth*, y ajustar los parámetros internos del modelo de manera iterativa para que, poco a poco, las salidas sean lo más parecidas a las etiquetas.\n",
    "\n",
    "Si bien este proceso es muy utilizado y ha dado muy buenos resultados en muchas aplicaciones, existen muchas otras en las que tales datasets etiquetados simplemente no existen o son escasos debido al coste y la complejidad de elaborarlos. Un ejemplo es el entrenamiento de redes neuronales para tareas de visión artificial en imágenes de satélite. Si queremos, por ejemplo, un detector de coches para calcular la ocupación de aparcamientos necesitaremos, primero, comprar imágenes de alta resolución (que pueden llegar a costar varios miles de euros cada una) y, segundo, etiquetar todos los coches que encontremos en las imágenes (lo cual puede llevar un buen rato). Es en este punto en el que nos preguntamos: ¿es posible entrenar redes neuronales sin etiquetas, solo a partir de datos? A este campo se le conoce como aprendizaje no supervisado, *unsupervised learning* en inglés, y es lo que vamos a explorar en este y los siguientes posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-float",
   "metadata": {},
   "source": [
    "## La analogía del pastel de *Yan Lecun*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-cause",
   "metadata": {},
   "source": [
    "Uno de los investigadores más influyentes en el ámbito del aprendizaje no supervisado, y del mundo del aprendizaje profundo en general, es Yann Lecun. Para entender la potencia de esta forma de aprendizaje presentó una analogía en la que, si el aprendizaje fuese un pastel, el aprendizaje supervisado solo correspondería al recubrimiento, mientras que el aprendizaje no supervisado sería el interior (y el aprendizaje por refuerzo, la cereza de arriba 😝)\n",
    "\n",
    "![](https://miro.medium.com/max/4416/1*bvMhd_xpVxfJYoKXYp5hug.png)\n",
    "\n",
    "Esto nos da una idea de la importancia de ser capaces de entrenar nuestros modelos sin necesidad de un dataset etiquetado. Poder llevar a cabo este mecanismo de manera efectiva abriría la puerta a muchas aplicaciones hoy en día inconcebibles.\n",
    "\n",
    "Si bien en el campo del *machine learning* existen diferentes algoritmos no supervisados, como algoritmos de clustering (K-Means, DBSCAN, ...), estimación de probabilidad (Gaussian Mixtures) o reducción de dimensionalidad (PCA y otros), los algoritmos no supervisados para *deep learning* son a día de hoy un tema de investigación muy activa, siendo los algoritmos de aprendizaje auto-supervisado (*self-supervised learning*) los más comunes por sus buenos resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-pledge",
   "metadata": {},
   "source": [
    "## Aprendizaje Auto-Supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-robertson",
   "metadata": {},
   "source": [
    "El aprendizaje auto-supervisado consiste en el entrenamiento de modelos que aprenden representaciones invariantes a distorsiones de la misma entrada. Esto significa que si un modelo es alimentado por una imagen en color y la misma imagen en blanco y negro, la representación interna (las *features* que nos daría el modelo justo antes del clasificador) debería ser igual, o lo más parecida posible. Diferentes métodos se basan en esta idea para construir, de una manera u otra, una función de pérdida que compare estas representaciones y minimize la diferencia entre pares de transformaciones que provengan de la misma imagen, mientras que maximice la diferencia entre imágenes diferentes. Algunos de los métodos más conocidos son:\n",
    "\n",
    "- Moco\n",
    "- SwAV\n",
    "- SimCLR\n",
    "\n",
    "En este post vamos a ver un ejemplo utilizando un método simple y reciente (a la hora de elaborar este post): [Barlow Twins](https://arxiv.org/abs/2103.03230). Pero antes, vamos a demostrar la necesidad de esta técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-relation",
   "metadata": {},
   "source": [
    "## CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-failing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:24:47.271593Z",
     "start_time": "2021-05-11T10:24:47.267578Z"
    }
   },
   "source": [
    "Vamos a entrenar una red neuronal convolucional sencilla para la clasificación de imágenes con el dataset [CIFAR10](https://arxiv.org/abs/2103.03230).\n",
    "\n",
    "> El dataset CIFAR10 es muy utilizado, por lo que podrás usarlo directamente desde `torchvision`. Sin embargo, como vamos a hacer cosas un poco más exóticas más adelante, aquí lo descargamos desde la fuente original y haremos nuestro propio `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "through-favor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T10:32:17.041486Z",
     "start_time": "2021-05-11T10:32:16.924468Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-452967de1773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
