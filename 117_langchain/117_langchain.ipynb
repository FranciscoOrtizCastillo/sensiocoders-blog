{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/117_langchain/117_langchain.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain ü¶úüîó"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el despegue de los modelos de lenguaje que estamos viviendo en este momento cientos de nuevas herramientas y aplicaciones est√°n apareciendo para aprovechar el poder de estas redes neuronales. Una de ellas parece destacar por encima del resto, y √©sta es [LangChain](https://docs.langchain.com/docs/). En este post vamos a ver qu√© es y c√≥mo podemos usarla."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬øQu√© es LangChain?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seg√∫n su [documentaci√≥n](https://docs.langchain.com/docs/), Langchain es un entorno de desarrollo de aplicaciones basadas en modelos de lenguajes. Las herramientas proporcionadas por LangChain permiten, por un lado, conectar modelos de lenguaje con otras fuentes de datos (como por ejemplo tus porpios documentos, bases de datos o emails) y, por otro lado, permitir a estos modelos interactuar con su entorno (por ejemplo, enviando emails o llamando a APIs web). Langchain ofrece librer√≠as en Python y Javascript para facilitar el desarrollo de estas aplicaciones, en este post nos centraremos en la librer√≠a de Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un ejemplo pr√°ctico"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos viendo un ejemplo pr√°ctico de c√≥mo usar LangChain para proporcionar informaci√≥n sobre un documento, y luego entraremos en detalle de los diferentes componentes y c√≥mo funcionan.\n",
    "\n",
    "> Vamos a usar como documento el art√≠culo [On the Measure of Intelligence](https://arxiv.org/pdf/1911.01547.pdf), de Fran√ßois Chollet (2019)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que necesitamos es instalar la librer√≠a de LangChain:\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.160'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero necesitaremos un modelo. Para ello usaremos [Huggingface](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", \n",
    "#     task=\"text-generation\", \n",
    "#     model_kwargs={\"temperature\": 0.9, \"max_length\": 1024},\n",
    "#     # model_kwargs={\"temperature\": 0.9, \"max_length\": 1024, 'device_map': 'auto'},\n",
    "#     # device=0\n",
    "# )\n",
    "\n",
    "# OJO! max_length tiene que ser suficiente como para tener el documento (chuck) + el prompt + el system prompt + respuesta generada !!!\n",
    "llm = HuggingFacePipeline.from_model_id(model_id=\"bigscience/bloom-1b7\", task=\"text-generation\", model_kwargs={\"temperature\": 0, \"max_length\": 2048, 'device_map': 'sequential'}, device=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es generar nuestro `prompt`. Para ello usaremos el template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# template = \"\"\"<|prompter|>{question}<|endoftext|><|assistant|>\"\"\"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos nuestro modelo y prompt, podemos crear nuestra primera `chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (1024) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, let's look at the first letter of the name. The first letter of Sensio is S. The second letter of Sensio is I. The third letter of Sensio is O. The fourth letter of Sensio is I. The fifth letter of Sensio is O. The sixth letter of Sensio is I. The seventh letter of Sensio is O. The eighth letter of Sensio is I. The ninth letter of Sensio is O. The tenth letter of Sensio is I. The eleventh letter of Sensio is O. The twelfth letter of Sensio is I. The thirteenth letter of Sensio is O. The fourteenth letter of Sensio is I. The fifteenth letter of Sensio is O. The sixteenth letter of Sensio is I. The seventeenth letter of Sensio is O. The eighteenth letter of Sensio is I. The nineteenth letter of Sensio is O. The twentieth letter of Sensio is I. The twenty-first letter of Sensio is O. The twenty-second letter of Sensio is I. The twenty-third letter of Sensio is O. The twenty-fourth letter of Sensio is I. The twenty-fifth letter of Sensio is O. The twenty-sixth letter of Sensio is I. The twenty-seventh letter of Sensio is O. The twenty-eighth letter of Sensio is I. The twenty-ninth letter of Sensio is O. The thirty-first letter of Sensio is I. The thirty-second letter of Sensio is O. The thirty-third letter of Sensio is I. The thirty-fourth letter of Sensio is O. The thirty-fifth letter of Sensio is I. The thirty-sixth letter of Sensio is O. The thirty-seventh letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is I. The thirty-eighth letter of Sensio is O. The thirty-ninth letter of Sensio is\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"¬øQui√©n es Juan Sensio?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a intentar sacar informaci√≥n de nuestro pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1911.01547.pdf\")\n",
    "# loader = PyPDFLoader('1911.01547.pdf')\n",
    "# pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the Measure of Intelligence\\nFranc ¬∏ois Chollet\\x03\\nGoogle, Inc.\\nfchollet@google.com\\nNovember 5, 2019\\nAbstract\\nTo make deliberate progress towards more intelligent and more human-like artiÔ¨Åcial\\nsystems, we need to be following an appropriate feedback signal: we need to be able to\\ndeÔ¨Åne and evaluate intelligence in a way that enables comparisons between two systems,\\nas well as comparisons with humans. Over the past hundred years, there has been an abun-\\ndance of attempts to deÔ¨Åne and measure intelligence, across both the Ô¨Åelds of psychology\\nand AI. We summarize and critically assess these deÔ¨Ånitions and evaluation approaches,\\nwhile making apparent the two historical conceptions of intelligence that have implicitly\\nguided them. We note that in practice, the contemporary AI community still gravitates to-\\nwards benchmarking intelligence by comparing the skill exhibited by AIs and humans at\\nspeciÔ¨Åc tasks, such as board games and video games. We argue that solely measuring skill\\nat any given task falls short of measuring intelligence, because skill is heavily modulated\\nby prior knowledge and experience: unlimited priors or unlimited training data allow ex-\\nperimenters to ‚Äúbuy‚Äù arbitrary levels of skills for a system, in a way that masks the system‚Äôs\\nown generalization power. We then articulate a new formal deÔ¨Ånition of intelligence based\\non Algorithmic Information Theory, describing intelligence as skill-acquisition efÔ¨Åciency\\nand highlighting the concepts of scope ,generalization difÔ¨Åculty ,priors , and experience , as\\ncritical pieces to be accounted for in characterizing intelligent systems. Using this deÔ¨Å-\\nnition, we propose a set of guidelines for what a general AI benchmark should look like.\\nFinally, we present a new benchmark closely following these guidelines, the Abstraction\\nand Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as\\npossible to innate human priors. We argue that ARC can be used to measure a human-like\\nform of general Ô¨Çuid intelligence and that it enables fair general intelligence comparisons\\nbetween AI systems and humans.\\n\\x03I thank Jos ¬¥e Hern ¬¥andez-Orallo, Julian Togelius, Christian Szegedy, and Martin Wicke for their valuable com-\\nments on the draft of this document.\\n1arXiv:1911.01547v2  [cs.AI]  25 Nov 2019Contents\\nI Context and history 3\\nI.1 Need for an actionable deÔ¨Ånition and measure of intelligence . . . . . . . . 3\\nI.2 DeÔ¨Åning intelligence: two divergent visions . . . . . . . . . . . . . . . . . 4\\nI.2.1 Intelligence as a collection of task-speciÔ¨Åc skills . . . . . . . . . . . . 5\\nI.2.2 Intelligence as a general learning ability . . . . . . . . . . . . . . . . . 6\\nI.3 AI evaluation: from measuring skills to measuring broad abilities . . . . . . 7\\nI.3.1 Skill-based, narrow AI evaluation . . . . . . . . . . . . . . . . . . . . 7\\nI.3.2 The spectrum of generalization: robustness, Ô¨Çexibility, generality . . . 9\\nI.3.3 Measuring broad abilities and general intelligence: the psychometrics\\nperspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\nI.3.4 Integrating AI evaluation and psychometrics . . . . . . . . . . . . . . 14\\nI.3.5 Current trends in broad AI evaluation . . . . . . . . . . . . . . . . . . 16\\nII A new perspective 18\\nII.1 Critical assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nII.1.1 Measuring the right thing: evaluating skill alone does not move us\\nforward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nII.1.2 The meaning of generality: grounding the g factor . . . . . . . . . . . 20\\nII.1.3 Separating the innate from the acquired: insights from developmental\\npsychology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\nII.2 DeÔ¨Åning intelligence: a formal synthesis . . . . . . . . . . . . . . . . . . . 27\\nII.2.1 Intelligence as skill-acquisition efÔ¨Åciency . . . . . . . . . . . . . . . . 27\\nII.2.2 Computation efÔ¨Åciency, time efÔ¨Åciency, energy efÔ¨Åciency, and risk ef-\\nÔ¨Åciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\nII.2.3 Practical implications . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\nII.3 Evaluating intelligence in this light . . . . . . . . . . . . . . . . . . . . . . 43\\nII.3.1 Fair comparisons between intelligent systems . . . . . . . . . . . . . . 43\\nII.3.2 What to expect of an ideal intelligence benchmark . . . . . . . . . . . 45\\nIII A benchmark proposal: the ARC dataset 46\\nIII.1 Description and goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nIII.1.1 What is ARC? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nIII.1.2 Core Knowledge priors . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\nIII.1.3 Key differences with psychometric intelligence tests . . . . . . . . . . 50\\nIII.1.4 What a solution to ARC may look like, and what it would imply for AI\\napplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\nIII.2 Weaknesses and future reÔ¨Ånements . . . . . . . . . . . . . . . . . . . . . . 53\\nIII.3 Possible alternatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\\nIII.3.1 Repurposing skill benchmarks to measure broad generalization . . . . 55\\nIII.3.2 Open-ended adversarial or collaborative approaches . . . . . . . . . . 55\\n2I Context and history\\nI.1 Need for an actionable deÔ¨Ånition and measure of intelligence\\nThe promise of the Ô¨Åeld of AI, spelled out explicitly at its inception in the 1950s and re-\\npeated countless times since, is to develop machines that possess intelligence comparable\\nto that of humans. But AI has since been falling short of its ideal: although we are able to\\nengineer systems that perform extremely well on speciÔ¨Åc tasks, they have still stark limi-\\ntations, being brittle, data-hungry, unable to make sense of situations that deviate slightly\\nfrom their training data or the assumptions of their creators, and unable to repurpose them-\\nselves to deal with novel tasks without signiÔ¨Åcant involvement from human researchers.\\nIf the only successes of AI have been in developing narrow, task-speciÔ¨Åc systems, it\\nis perhaps because only within a very narrow and grounded context have we been able\\ntodeÔ¨Åne our goal sufÔ¨Åciently precisely, and to measure progress in an actionable way.\\nGoal deÔ¨Ånitions and evaluation benchmarks are among the most potent drivers of scientiÔ¨Åc\\nprogress. To make progress towards the promise of our Ô¨Åeld, we need precise, quantitative\\ndeÔ¨Ånitions and measures of intelligence ‚Äì in particular human-like general intelligence.\\nThese would not be merely deÔ¨Ånitions and measures meant to describe or characterize\\nintelligence, but precise, explanatory deÔ¨Ånitions meant to serve as a North Star, an objective\\nfunction showing the way towards a clear target, capable of acting as a reliable measure of\\nour progress and as a way to identify and highlight worthwhile new approaches that may\\nnot be immediately applicable, and would otherwise be discounted.\\nFor instance, common-sense dictionary deÔ¨Ånitions of intelligence may be useful to\\nmake sure we are talking about the same concepts, but they are not useful for our pur-\\npose, as they are not actionable, explanatory, or measurable. Similarly, the Turing Test\\n[91] and its many variants (e.g. Total Turing Test and Loebner Prize [75]) are not useful\\nas a driver of progress (and have in fact served as a red herring1), since such tests com-\\npletely opt out of objectively deÔ¨Åning and measuring intelligence, and instead outsource the\\ntask to unreliable human judges who themselves do not have clear deÔ¨Ånitions or evaluation\\nprotocols.\\nIt is a testimony to the immaturity of our Ô¨Åeld that the question of what we mean when\\nwe talk about intelligence still doesn‚Äôt have a satisfying answer. What‚Äôs worse, very little\\nattention has been devoted to rigorously deÔ¨Åning it or benchmarking our progress towards\\nit. Legg and Hutter noted in a 2007 survey of intelligence deÔ¨Ånitions and evaluation meth-\\nods [53]: ‚Äúto the best of our knowledge, no general survey of tests and deÔ¨Ånitions has been\\npublished‚Äù . A decade later, in 2017, Hern ¬¥andez-Orallo released an extensive survey of\\nevaluation methods [36] as well as a comprehensive book on AI evaluation [37]. Results\\nand recommendations from both of these efforts have since been largely ignored by the\\ncommunity.\\nWe believe this lack of attention is a mistake, as the absence of widely-accepted ex-\\n1Turing‚Äôs imitation game was largely meant as an argumentative device in a philosophical discussion, not as a\\nliteral test of intelligence. Mistaking it for a test representative of the goal of the Ô¨Åeld of AI has been an ongoing\\nproblem.\\n3plicit deÔ¨Ånitions has been substituted with implicit deÔ¨Ånitions and biases that stretch back\\ndecades. Though invisible, these biases are still structuring many research efforts today, as\\nillustrated by our Ô¨Åeld‚Äôs ongoing fascination with outperforming humans at board games or\\nvideo games (a trend we discuss in I.3.5 and II.1). The goal of this document is to point\\nout the implicit assumptions our Ô¨Åeld has been working from, correct some of its most\\nsalient biases, and provide an actionable formal deÔ¨Ånition and measurement benchmark for\\nhuman-like general intelligence, leveraging modern insight from developmental cognitive\\npsychology.\\nI.2 DeÔ¨Åning intelligence: two divergent visions\\nLooked at in one way, everyone knows what\\nintelligence is; looked at in another way, no\\none does.\\nRobert J. Sternberg, 2000\\nMany formal and informal deÔ¨Ånitions of intelligence have been proposed over the past\\nfew decades, although there is no existing scientiÔ¨Åc consensus around any single deÔ¨Ånition.\\nSternberg & Detterman noted in 1986 [87] that when two dozen prominent psychologists\\nwere asked to deÔ¨Åne intelligence, they all gave somewhat divergent answers. In the context\\nof AI research, Legg and Hutter [53] summarized in 2007 no fewer than 70 deÔ¨Ånitions from\\nthe literature into a single statement: ‚ÄúIntelligence measures an agent‚Äôs ability to achieve\\ngoals in a wide range of environments. ‚Äù\\nThis summary points to two characterizations, which are nearly universally ‚Äì but of-\\nten separately ‚Äì found in deÔ¨Ånitions of intelligence: one with an emphasis on task-speciÔ¨Åc\\nskill ( ‚Äúachieving goals‚Äù ), and one focused on generality and adaptation ( ‚Äúin a wide range\\nof environments‚Äù ). In this view, an intelligent agent would achieve high skill across many\\ndifferent tasks (for instance, achieving high scores across many different video games). Im-\\nplicitly here, the tasks may not necessarily be known in advance: to truly achieve generality,\\nthe agent would have to be able to learn to handle new tasks (skill acquisition).\\nThese two characterizations map to Catell‚Äôs 1971 theory of Ô¨Çuid and crystallized intel-\\nligence (Gf-Gc) [13], which has become one of the pillars of the dominant theory of human\\ncognitive abilities, the Cattell-Horn-Caroll theory (CHC) [62]. They also relate closely to\\ntwo opposing views of the nature of the human mind that have been deeply inÔ¨Çuential in\\ncognitive science since the inception of the Ô¨Åeld [85]: one view in which the mind is a\\nrelatively static assembly of special-purpose mechanisms developed by evolution, only ca-\\npable of learning what is it programmed to acquire, and another view in which the mind is\\na general-purpose ‚Äúblank slate‚Äù capable of turning arbitrary experience into knowledge and\\nskills, and that could be directed at any problem.\\n4A central point of this document is to make explicit and critically assess this dual deÔ¨Å-\\nnition that has been implicitly at the foundation of how we have been conceptualizing and\\nevaluating intelligence in the context of AI research: crystallized skill on one hand, skill-\\nacquisition ability on the other. Understanding this intellectual context and its ongoing\\ninÔ¨Çuence is a necessary step before we can propose a formal deÔ¨Ånition of intelligence from\\na modern perspective.\\nI.2.1 Intelligence as a collection of task-speciÔ¨Åc skills\\nIn the distant future I see open Ô¨Åelds for far\\nmore important researches. Psychology will be\\nbased on a new foundation, that of the\\nnecessary acquirement of each mental power\\nand capacity by gradation.\\nCharles Darwin, 1859\\nThe evolutionary psychology view of human nature is that much of the human cognitive\\nfunction is the result of special-purpose adaptations that arose to solve speciÔ¨Åc problems\\nencountered by humans throughout their evolution (see e.g. [19, 74]) ‚Äì an idea which orig-\\ninated with Darwin [21] and that coalesced in the 1960s and 1970s. Around the same time\\nthat these ideas were gaining prominence in cognitive psychology, early AI researchers,\\nperhaps seeing in electronic computers an analogue of the mind, mainly gravitated towards\\na view of intelligence as a set of static program-like routines, heavily relying on logical\\noperators, and storing learned knowledge in a database-like memory.\\nThis vision of the mind as a wide collection of vertical, relatively static programs that\\ncollectively implement ‚Äúintelligence‚Äù, was most prominently endorsed by inÔ¨Çuential AI\\npioneer Marvin Minsky (see e.g. The Society of Mind , 1986 [63]). This view gave rise\\nto deÔ¨Ånitions of intelligence and evaluation protocols for intelligence that are focused on\\ntask-speciÔ¨Åc performance. This is perhaps best illustrated by Minsky‚Äôs 1968 deÔ¨Ånition of\\nAI:‚ÄúAI is the science of making machines capable of performing tasks that would require\\nintelligence if done by humans‚Äù2. It was then widely accepted within the AI community\\nthat the ‚Äúproblem of intelligence‚Äù would be solved if only we could encode human skills\\ninto formal rules and encode human knowledge into explicit databases.\\nThis view of intelligence was once so dominant that ‚Äúlearning‚Äù (discounted as pure\\nmemorization) was often not even mentioned at all in AI textbooks until the mid-1980s.\\nEven McCarthy, a rare advocate for generality in AI, believed that the key to achieving\\ngenerality was better knowledge bases [60]. This deÔ¨Ånition and evaluation philosophy\\nfocused entirely on skill at narrow tasks normally handled by humans has led to a striking\\n2Note the lingering inÔ¨Çuence of the Turing Test.\\n5paradox, as pointed out by Hern ¬¥andez-Orallo [36] in his 2017 survey: the Ô¨Åeld of artiÔ¨Åcial\\nintelligence has been very successful in developing artiÔ¨Åcial systems that perform these\\ntasks without featuring intelligence , a trend that continues to this day.\\nI.2.2 Intelligence as a general learning ability\\nPresumably the child brain is something like a\\nnotebook as one buys it from the stationer‚Äôs.\\nRather little mechanism, and lots of blank\\nsheets.\\nAlan Turing, 1950\\nIn contrast, a number of researchers have taken the position that intelligence lies in the\\ngeneral ability to acquire new skills through learning; an ability that could be directed to a\\nwide range of previously unknown problems ‚Äì perhaps even any problem at all. Contrast\\nMinsky‚Äôs task-focused deÔ¨Ånition of AI with the following one, paraphrased from McCarthy\\n[60] by Hern ¬¥andez-Orallo: ‚ÄúAI is the science and engineering of making machines do tasks\\nthey have never seen and have not been prepared for beforehand‚Äù [36].\\nThe notion that machines could acquire new skills through a learning process similar\\nto that of human children was initially laid out by Turing in his 1950 paper [91]. In 1958,\\nFriedberg noted astutely: ‚ÄúIf we are ever to make a machine that will speak, understand\\nor translate human languages, solve mathematical problems with imagination, practice a\\nprofession or direct an organization, either we must reduce these activities to a science so\\nexact that we can tell a machine precisely how to go about doing them or we must develop\\na machine that can do things without being told precisely how‚Äù [26]. But although the\\nidea of generality through learning was given signiÔ¨Åcant consideration at the birth of the\\nÔ¨Åeld, and has long been championed by pioneers like McCarthy and Papert, it lay largely\\ndormant until the resurgence of machine learning in the 1980s.\\nThis view of intelligence echoes another long-standing conception of human nature\\nthat has had a profound inÔ¨Çuence on the history of cognitive science, contrasting with the\\nevolutionary psychology perspective: Locke‚Äôs Tabula Rasa (blank slate), a vision of the\\nmind as a Ô¨Çexible, adaptable, highly general process that turns experience into behavior,\\nknowledge, and skills. This conception of the human mind can be traced back to Aristotle\\n(De Anima , c. 350BC, perhaps the Ô¨Årst treatise of psychology [3]), was embraced and\\npopularized by Enlightenment thinkers such as Hobbes [42], Locke [56], and Rousseau\\n[78]. It has more recently found renewed vitality within cognitive psychology (e.g. [79])\\nand in AI via connectionism (e.g. [41]).\\nWith the resurgence of machine learning in the 1980s, its rise to intellectual dominance\\nin the 2000s, and its peak as an intellectual quasi-monopoly in AI in the late 2010s via\\n6Deep Learning, a connectionist-inspired Tabula Rasa is increasingly becoming the domi-\\nnant philosophical framework in which AI research is taking place. Many researchers are\\nimplicitly conceptualizing the mind via the metaphor of a ‚Äúrandomly initialized neural net-\\nwork‚Äù that starts blank and that derives its skills from ‚Äútraining data‚Äù ‚Äì a cognitive fallacy\\nthat echoes early AI researchers a few decades prior who conceptualized the mind as a kind\\nof mainframe computer equipped with clever subroutines. We see the world through the\\nlens of the tools we are most familiar with.\\nToday, it is increasingly apparent that both of these views of the nature of human in-\\ntelligence ‚Äì either a collection of special-purpose programs or a general-purpose Tabula\\nRasa ‚Äì are likely incorrect, which we discuss in II.1.3, along with implications for artiÔ¨Åcial\\nintelligence.\\nI.3 AI evaluation: from measuring skills to measuring broad\\nabilities\\nThese two conceptualizations of intelligence ‚Äì along with many other intermediate views\\ncombining elements from each side ‚Äì have inÔ¨Çuenced a host of approaches for evaluating\\nintelligence in machines, in humans, and more rarely in both at the same time, which we\\ndiscuss below. Note that this document is not meant as an extensive survey of AI evaluation\\nmethods ‚Äì for such a survey, we recommend Hern ¬¥andez-Orallo 2017 [37]. Other notable\\nprevious surveys include Cohen and Howe 1988 [69] and Legg and Hutter 2007 [53].\\nI.3.1 Skill-based, narrow AI evaluation\\nIn apparent accordance with Minsky‚Äôs goal for AI, the major successes of the Ô¨Åeld have\\nbeen in building special-purpose systems capable of handling narrow, well-described tasks,\\nsometimes at above human-level performance. This success has been driven by perfor-\\nmance measures quantifying the skill of a system at a given task (e.g. how well an AI\\nplays chess, how well an image classiÔ¨Åer recognizes cats from dogs). There is no single,\\nformalized way to do skill-based evaluation. Historically successful approaches include:\\n\\x0fHuman review: having human judges observe the system‚Äôs input-output response\\nand score it. This is the idea behind the Turing test and its variants. This evaluation\\nmode is rarely used in practice, due to being expensive, impossible to automate, and\\nsubjective. Some human-facing AI systems (in particular commercial chatbots) use\\nit as one of multiple evaluation mechanics.\\n\\x0fWhite-box analysis: inspecting the implementation of the system to determine its\\ninput-output response and score it. This is most relevant for algorithms solving a\\nfully-described task in a fully-described environment where all possible inputs can\\nbe explicitly enumerated or described analytically (e.g. an algorithm that solves the\\ntraveling salesman problem or that plays the game ‚ÄúConnect Four‚Äù), and would often\\ntake the form of an optimality proof.\\n7\\x0fPeer confrontation: having the system compete against either other AIs or humans.\\nThis is the preferred mode of evaluation for player-versus-player games, such as\\nchess.\\n\\x0fBenchmarks: having the system produce outputs for a ‚Äútest set‚Äù of inputs (or envi-\\nronments) for which the desired outcome is known, and score the response.\\nBenchmarks in particular have been a major driver of progress in AI, because they are\\nreproducible (the test set is Ô¨Åxed), fair (the test set is the same for everyone), scalable (it\\nis inexpensive to run the evaluation many times), easy to set up, and Ô¨Çexible enough to be\\napplicable to a wide range of possible tasks. Benchmarks have often been most impactful\\nin the context of a competition between different research teams, such as the ILSVRC chal-\\nlenge for large-scale image recognition (ImageNet) [22] or the DARPA Grand Challenge\\nfor autonomous driving [11]. A number of private and community-led initiatives have been\\nstarted on the premise that such benchmark-based competitions speed up progress (e.g.\\nKaggle (kaggle.com), as well as academic alternatives such as ChaLearn (chalearn.org),\\nthe Hutter prize, etc.), while some government organizations use competitions to deliber-\\nately trigger technological breakthroughs (e.g. DARPA, NIST).\\nThese successes demonstrate the importance of setting clear goals and adopting objec-\\ntive measures of performance that are shared across the research community . However,\\noptimizing for a single metric or set of metrics often leads to tradeoffs and shortcuts when\\nit comes to everything that isn‚Äôt being measured and optimized for (a well-known effect\\non Kaggle, where winning models are often overly specialized for the speciÔ¨Åc benchmark\\nthey won and cannot be deployed on real-world versions of the underlying problem). In the\\ncase of AI, the focus on achieving task-speciÔ¨Åc performance while placing no conditions\\nonhow the system arrives at this performance has led to systems that, despite performing\\nthe target tasks well, largely do not feature the sort of human intelligence that the Ô¨Åeld of\\nAI set out to build .\\nThis has been interpreted by McCorduck as an ‚ÄúAI effect‚Äù where goalposts move every\\ntime progress in AI is made: ‚Äúevery time somebody Ô¨Ågured out how to make a computer\\ndo somethingplay good checkers, solve simple but relatively informal problemsthere was\\na chorus of critics to say, ‚Äòthat‚Äôs not thinking‚Äô ‚Äù [61]. Similarly, Reed notes: ‚ÄúWhen we\\nknow how a machine does something ‚Äòintelligent‚Äô, it ceases to be regarded as intelligent.\\nIf I beat the world‚Äôs chess champion, I‚Äôd be regarded as highly bright. ‚Äù [77]. This inter-\\npretation arises from overly anthropocentric assumptions. As humans, we can only display\\nhigh skill at a speciÔ¨Åc task if we have the ability to efÔ¨Åciently acquire skills in general ,\\nwhich corresponds to intelligence as characterized in II. No one is born knowing chess, or\\npredisposed speciÔ¨Åcally for playing chess. Thus, if a human plays chess at a high level,\\nwe can safely assume that this person is intelligent, because we implicitly know that they\\nhad to use their general intelligence to acquire this speciÔ¨Åc skill over their lifetime, which\\nreÔ¨Çects their general ability to acquire many other possible skills in the same way. But the\\nsame assumption does not apply to a non-human system that does not arrive at competence\\nthe way humans do. If intelligence lies in the process of acquiring skills , then there is\\nno task X such that skill at X demonstrates intelligence , unless X is actually a meta-task\\n8involving skill-acquisition across a broad range of tasks. The ‚ÄúAI effect‚Äù characterization\\nis confusing the process of intelligence (such as the intelligence displayed by researchers\\ncreating a chess-playing program) with the artifact produced by this process (the resulting\\nchess-playing program), due to these two concepts being fundamentally intertwined in the\\ncase of humans. We discuss this further in II.1.\\nTask-speciÔ¨Åc performance is a perfectly appropriate and effective measure of success if\\nand only if handling the task as initially speciÔ¨Åed is the end goal of the system ‚Äì in other\\nwords, if our measure of performance captures exactly what we expect of the system. How-\\never, it is deÔ¨Åcient if we need systems that can show autonomy in handling situations that\\nthe system creator did not plan for, that can dynamically adapt to changes in the task ‚Äì or in\\nthe context of the task ‚Äì without further human intervention, or that can be repurposed for\\nother tasks. Meanwhile, robustness and Ô¨Çexibility are increasingly being perceived as im-\\nportant requirements for certain broader subÔ¨Åelds of AI, such as L5 self-driving, domestic\\nrobotics, or personal assistants; there is even increasing interest in generality itself (e.g. de-\\nvelopmental robotics [4], artiÔ¨Åcial general intelligence [28]). This points to a need to move\\nbeyond skill-based evaluation for such endeavours, and to Ô¨Ånd ways to evaluate robustness\\nand Ô¨Çexibility, especially in a cross-task setting, up to generality. But what do we really\\nmean when we talk about robustness, Ô¨Çexibility, and generality?\\nI.3.2 The spectrum of generalization: robustness, Ô¨Çexibility, generality\\nEven though such machines might do some\\nthings as well as we do them, or perhaps even\\nbetter, they would inevitably fail in others,\\nwhich would reveal they were acting not\\nthrough understanding, but only from the\\ndisposition of their organs.\\nRen¬¥e Descartes, 1637\\nThe resurgence of machine learning in the 1980s has led to an interest in formally\\ndeÔ¨Åning, measuring, and maximizing generalization . Generalization is a concept that pre-\\ndates machine learning, originally developed to characterize how well a statistical model\\nperforms on inputs that were not part of its training data. In recent years, the success of\\nDeep Learning [52], as well as increasingly frequent run-ins with its limitations (see e.g.\\n[51, 16, 59]), have triggered renewed interest in generalization theory in the context of\\nmachine learning (see e.g. [102, 67, 45, 70, 17, 49]). The notion of generalization can be\\nformally deÔ¨Åned in various contexts (in particular, statistical learning theory [92] provides a\\nwidely-used formal deÔ¨Ånition that is relevant for machine learning, and we provide a more\\ngeneral formalization in II.2). We can informally deÔ¨Åne ‚Äúgeneralization‚Äù or ‚Äúgeneralization\\npower‚Äù for any AI system to broadly mean ‚Äúthe ability to handle situations (or tasks) that\\n9differ from previously encountered situations‚Äù .\\nThe notion of ‚Äúpreviously encountered situation‚Äù is somewhat ambiguous, so we should\\ndistinguish between two types of generalization:\\n\\x0fSystem-centric generalization : this is the ability of a learning system to handle\\nsituations it has not itself encountered before. The formal notion of generalization\\nerror in statistical learning theory would belong here.\\n‚ÄìFor instance, if an engineer develops a machine learning classiÔ¨Åcation algorithm\\nand Ô¨Åts it on a training set of Nsamples, the ‚Äúgeneralization‚Äù of this learning al-\\ngorithm would refer to its classiÔ¨Åcation error over images not part of the training\\nset.\\n‚ÄìNote that the generalization power of this algorithm may be in part due to prior\\nknowledge injected by the developer of the system. This prior knowledge is\\nignored by this measure of generalization.\\n\\x0fDeveloper-aware generalization : this is the ability of a system, either learning or\\nstatic, to handle situations that neither the system nor the developer of the system\\nhave encountered before.\\n‚ÄìFor instance, if an engineer uses a ‚Äúdevelopment set‚Äù of Nsamples to create a\\nstatic classiÔ¨Åcation algorithm that uses hard-coded heuristic rules, the ‚Äúgeneral-\\nization‚Äù of this static algorithm would refer to its classiÔ¨Åcation error over images\\nnot part of the ‚Äúdevelopment set‚Äù.\\n‚ÄìNote that ‚Äúdeveloper-aware generalization‚Äù is equivalent to ‚Äúsystem-centric gen-\\neralization‚Äù if we include the developer of the system as part of the system.\\n‚ÄìNote that ‚Äúdeveloper-aware generalization‚Äù accounts for any prior knowledge\\nthat the developer of the system has injected into it. ‚ÄúSystem-centric generaliza-\\ntion‚Äù does not.\\nIn addition, we Ô¨Ånd it useful to qualitatively deÔ¨Åne degrees of generalization for information-\\nprocessing systems:\\n\\x0fAbsence of generalization : The notion of generalization as we have informally de-\\nÔ¨Åned above fundamentally relies on the related notions of novelty and uncertainty: a\\nsystem can only generalize to novel information that could not be known in advance\\nto either the system or its creator. AI systems in which there is no uncertainty do not\\ndisplay generalization. For instance, a program that plays tic-tac-toe via exhaustive\\niteration cannot be said to ‚Äúgeneralize‚Äù to all board conÔ¨Ågurations. Likewise, a sort-\\ning algorithm that is proven to be correct cannot be said to ‚Äúgeneralize‚Äù to all lists of\\nintegers, much like proven mathematical statements cannot be said to ‚Äúgeneralize‚Äù to\\nall objects that match the assumptions of their proof3.\\n3This is a distinct deÔ¨Ånition from ‚Äúgeneralization‚Äù in mathematics, where ‚Äúto generalize‚Äù means to extend the\\nscope of application of a statement by weakening its assumptions.\\n10\\x0fLocal generalization, or ‚Äúrobustness‚Äù : This is the ability of a system to handle new\\npoints from a known distribution for a single task or a well-scoped set of known tasks,\\ngiven a sufÔ¨Åciently dense sampling of examples from the distribution (e.g. tolerance\\nto anticipated perturbations within a Ô¨Åxed context). For instance, an image classiÔ¨Åer\\nthat can distinguish previously unseen 150x150 RGB images containing cats from\\nthose containing dogs, after being trained on many such labeled images, can be said\\nto perform local generalization. One could characterize it as ‚Äúadaptation to known\\nunknowns within a single task or well-deÔ¨Åned set of tasks‚Äù . This is the form of\\ngeneralization that machine learning has been concerned with from the 1950s up to\\nthis day.\\n\\x0fBroad generalization, or ‚ÄúÔ¨Çexibility‚Äù : This is the ability of a system to handle a\\nbroad category of tasks and environments without further human intervention. This\\nincludes the ability to handle situations that could not have been foreseen by the\\ncreators of the system. This could be considered to reÔ¨Çect human-level ability in\\na single broad activity domain (e.g. household tasks, driving in the real world), and\\ncould be characterized as ‚Äúadaptation to unknown unknowns across a broad category\\nof related tasks‚Äù . For instance, a L5 self-driving vehicle, or a domestic robot capable\\nof passing Wozniak‚Äôs coffee cup test (entering a random kitchen and making a cup of\\ncoffee) [99] could be said to display broad generalization. Arguably, even the most\\nadvanced AI systems today do not belong in this category, although there is increasing\\nresearch interest in achieving this level.\\n\\x0fExtreme generalization : This describes open-ended systems with the ability to han-\\ndle entirely new tasks that only share abstract commonalities with previously encoun-\\ntered situations, applicable to any task and domain within a wide scope. This could\\nbe characterized as ‚Äúadaptation to unknown unknowns across an unknown range of\\ntasks and domains‚Äù. Biological forms of intelligence (humans and possibly other in-\\ntelligent species) are the only example of such a system at this time. A version of\\nextreme generalization that is of particular interest to us throughout this document\\nishuman-centric extreme generalization , which is the speciÔ¨Åc case where the scope\\nconsidered is the space of tasks and domains that Ô¨Åt within the human experience. We\\nwill refer to ‚Äúhuman-centric extreme generalization‚Äù as ‚Äúgenerality‚Äù. Importantly, as\\nwe deliberately deÔ¨Åne generality here by using human cognition as a reference frame\\n(which we discuss in II.1.2), it is only ‚Äúgeneral‚Äù in a limited sense. Do note, however,\\nthat humans display extreme generalization both in terms of system-centric gener-\\nalization (quick adaptability to highly novel situations from little experience) and\\ndeveloper-aware generalization (ability of contemporary humans to handle situations\\nthat previous humans have never experienced during their evolutionary history).\\nTo this list, we could, theoretically, add one more entry: ‚Äúuniversality‚Äù , which would\\nextend ‚Äúgenerality‚Äù beyond the scope of task domains relevant to humans, to any task that\\ncould be practically tackled within our universe (note that this is different from ‚Äúany task at\\nall‚Äù as understood in the assumptions of the No Free Lunch theorem [98, 97]). We discuss\\nin II.1.2 why we do not consider universality to be a reasonable goal for AI.\\n11Crucially, the history of AI has been one of slowly climbing up this spectrum, start-\\ning with systems that largely did not display generalization (symbolic AI), and evolving\\ntowards robust systems (machine learning) capable of local generalization. We are now\\nentering a new stage, where we seek to create Ô¨Çexible systems capable of broad generaliza-\\ntion (e.g. hybrid symbolic and machine learning systems such as self-driving vehicles, AI\\nassistants, or cognitive developmental robots). Skill-focused task-speciÔ¨Åc evaluation has\\nbeen appropriate for close-ended systems that aim at robustness in environments that only\\nfeature known unknowns, but developing systems that are capable of handling unknown\\nunknowns requires evaluating their abilities in a general sense.\\nImportantly, the spectrum of generalization outlined above seems to mirror the organi-\\nzation of humans cognitive abilities as laid out by theories of the structure of intelligence\\nin cognitive psychology. Major theories of the structure of human intelligence (CHC [62],\\ng-VPR [48]) all organize cognitive abilities in a hierarchical fashion (Ô¨Ågure 1), with three\\nstrata (in CHC): general intelligence (g factor) at the top, broad abilities in the middle, and\\nspecialized skills or test tasks at the bottom (this extends to 4 strata for g-VPR, which splits\\nbroad abilities into two layers), albeit the taxonomy of abilities differs between theories.\\nHere, ‚Äúextreme generalization‚Äù corresponds to the g factor, ‚Äúbroad generalization‚Äù across a\\ngiven domain corresponds to a broad cognitive ability, and ‚Äúlocal generalization‚Äù (as well\\nas the no-generalization case) corresponds to task-speciÔ¨Åc skill.\\nMeasuring such broad abilities (and possibly generality itself) rather than speciÔ¨Åc skills\\nhas historically been the problematic of the Ô¨Åeld of psychometrics. Could psychometrics\\ninform the evaluation of abilities in AI systems?\\nFigure 1: Hierarchical model of cognitive abilities and its mapping to the spectrum of general-\\nization.\\nNote that, in what follows:\\n\\x0fWe use ‚Äúbroad abilities‚Äù to refer to cognitive abilities that lead to broad or extreme\\ngeneralization. Developing such abilities should be the goal of any researcher inter-\\n12ested in Ô¨Çexible AI or general AI. ‚ÄúBroad abilities‚Äù is often meant in opposition to\\n‚Äúlocal generalization‚Äù.\\n\\x0fWe use ‚Äúgeneralization‚Äù to refer to the entire spectrum of generalization, starting with\\nlocal generalization.\\n\\x0fBecause human general intelligence (the g factor) is itself a very broad cognitive\\nability (the top of the hierarchy of abilities), we use the term ‚Äúintelligence‚Äù or ‚Äúgeneral\\nintelligence‚Äù to refer to extreme generalization as deÔ¨Åned above.\\nI.3.3 Measuring broad abilities and general intelligence: the psychometrics\\nperspective\\nIt seems to us that in intelligence there is a\\nfundamental faculty, the alteration or the lack\\nof which, is of the utmost importance for\\npractical life. This faculty is [...] the faculty of\\nadapting one‚Äôs self to circumstances.\\nAlfred Binet, 1916\\nIn the early days of the 20th century, Binet and Simon, looking for a formal way to\\ndistinguish children with mental disabilities from those with behavior problems, developed\\nthe Binet-Simon scale [8], the Ô¨Årst test of intelligence, founding the Ô¨Åeld of psychometrics.\\nImmediately after, Spearman observed that individual results across different, seemingly\\nunrelated types of intelligence tests were correlated, and hypothesized the existence of a\\nsingle factor of general intelligence, the g factor [83, 84]. Today, psychometrics is a well-\\nestablished subÔ¨Åeld of psychology that has arrived at some of the most reproducible results\\nof the Ô¨Åeld. Modern intelligence tests are developed by following strict standards regarding\\nreliability (low measurement error, a notion tied to reproducibility), validity (measuring\\nwhat one purports to be measuring, a notion tied to statistical consistency and predictive-\\nness), standardization, and freedom from bias ‚Äì see e.g. Classical Test Theory (CTT) [20]\\nand Item Response Theory (IRT) [34].\\nA fundamental notion in psychometrics is that intelligence tests evaluate broad cog-\\nnitive abilities as opposed to task-speciÔ¨Åc skills . Theories of the structure of intelligence\\n(such as CHC, g-VPR), which have co-evolved with psychometric testing (statistical phe-\\nnomena emerging from test results have informed these theories, and these theories have\\ninformed test design) organize these abilities in a hierarchical fashion (Ô¨Ågure 1), rather sim-\\nilarly to the spectrum of generalization we presented earlier. Importantly, an ability is an\\nabstract construct (based on theory and statistical phenomena) as opposed to a directly mea-\\nsurable, objective property of an individual mind, such as a score on a speciÔ¨Åc test. Broad\\nabilities in AI, which are also constructs, fall into the exact same evaluation problematics\\n13as cognitive abilities from psychometrics. Psychometrics approaches the quantiÔ¨Åcation of\\nabilities by using broad batteries of test tasks rather than any single task, and by analysing\\ntest results via probabilistic models. Importantly, the tasks should be previously unknown\\nto the test-taker, i.e., we assume that test-takers do not practice for intelligence tests. This\\napproach is highly relevant to AI evaluation.\\nRemarkably, in a parallel to psychometrics, there has been recent and increasing inter-\\nest across the Ô¨Åeld of AI in using broad batteries of test tasks to evaluate systems that aim\\nat greater Ô¨Çexibility. Examples include the Arcade Learning Environment for Reinforce-\\nment Learning agents [6], Project Malm ¬®O [71], the Behavior Suite [68], or the GLUE [95]\\nand SuperGLUE [94] benchmarks for natural language processing. The underlying logic\\nof these efforts is to measure something more general than skill at one speciÔ¨Åc task by\\nbroadening the set of target tasks. However, when it comes to assessing Ô¨Çexibility, a crit-\\nical defect of these multi-task benchmarks is that the set of tasks is still known in advance\\nto the developers of any test-taking system, and it is fully expected that test-taking systems\\nwill be able to practice speciÔ¨Åcally for the target tasks, leverage task-speciÔ¨Åc built-in prior\\nknowledge inherited from the system developers, leverage external knowledge obtained via\\npre-training, etc. As such, these benchmarks still appear to be highly gameable (see e.g.\\nII.1.1) ‚Äì merely widening task-speciÔ¨Åc skill evaluation to more tasks does not produce a\\nqualitatively different kind of evaluation. Such benchmarks are still looking at skills, rather\\nthan abilities, in contrast with the psychometrics approach (this is not to say that such\\nbenchmarks are not useful; merely that such static multi-task benchmarks do not directly\\nassess Ô¨Çexibility or generality).\\nIn addition to these multi-task benchmarks, a number of more ambitious test suites\\nfor cognitive abilities of AI have been proposed in the past but have not been imple-\\nmented in practice: the Newell test by Anderson and Lebiere ([2], named in reference to\\n[66]), the BICA ‚Äúcognitive decathlon‚Äù targeted at developmental robotics [65], the Turing\\nOlympics [27], and the I-Athlon [1]. Lacking concrete implementations, it is difÔ¨Åcult to\\nassess whether these projects would have been able to address the ability evaluation prob-\\nlem they set out to solve. On the other hand, two similarly-spirited but more mature test\\nsuite have emerged recently, focused on generalization capabilities as opposed to speciÔ¨Åc\\ntasks: the Animal-AI Olympics [7] (animalaiolympics.com) and the GVGAI competition\\n[72] (gvgai.net). Both take the position that AI agents should be evaluated on an unseen set\\nof tasks or games, in order to test learning or planning abilities rather than special-purpose\\nskill. Both feature a multi-game environment and an ongoing public competition.\\nI.3.4 Integrating AI evaluation and psychometrics\\nBesides efforts to broaden task-speciÔ¨Åc evaluation to batteries of multi-task tests, there\\nhave been more direct and explicit attempts to integrate AI evaluation and psychometrics.\\nA Ô¨Årst approach is to reuse existing psychometric intelligence tests, initially developed for\\nhumans, as a way to assess intelligence in AI systems ‚Äì perhaps an obvious idea if we are to\\ntake the term ‚ÄúartiÔ¨Åcial intelligence‚Äù literally. This idea was Ô¨Årst proposed by Green in 1964\\n[29], and was, around the same time, explored by Evans [24], who wrote a LISP program\\n14called ANALOGY capable of solving a geometric analogy task of the kind that may be\\nfound in a pyschometric intelligence test. Newell suggested the idea again in 1973 [66] in\\nhis seminal paper You can‚Äôt play 20 questions with Nature and win . It was proposed again\\nand reÔ¨Åned by Bringsjord et al. in the 2000s under the name ‚ÄúPsychometric AI‚Äù (PAI) [9].\\nHowever, it has since become apparent that it is possible for AI system developers to game\\nhuman intelligence tests, because the tasks used in these tests are available to the system\\ndevelopers, and thus the developers can straightforwardly solve the abstract form of these\\nproblems themselves and hard-code the solution in program form (see, for instance, [23, 80,\\n44]), much like Evans did with in the 1960s with the ANALOGY program. Effectively, in\\nthis case, it is the system developers who are solving the test problems, rather than any AI.\\nThe implicit assumptions that psychometric test designers make about human test-takers\\nturn out to be difÔ¨Åcult to enforce in the case of machines.\\nAn alternative, more promising approach is to leverage what psychometrics can teach\\nus about ability assessment and test design to create new types of benchmarks targeted\\nspeciÔ¨Åcally at evaluating broad abilities in AI systems. Along these lines, Hern ¬¥andez-\\nOrallo et al. have proposed extending psychometric evaluation to any intelligent system,\\nincluding AI agents and animals, in ‚ÄúUniversal Psychometrics‚Äù [39].\\nWe argue that several important principles of psychometrics can inform intelligence\\nevaluation in AI in the context of the development of broad AI and general AI:\\n\\x0fMeasuring abilities (representative of broad generalization and skill-acquisition efÔ¨Å-\\nciency), not skills. Abilities are distinct from skills in that they induce broad general-\\nization, i.e. they form the basis for skill across a broad range of tasks, including tasks\\nthat were previously unknown to the ability-enabled system and its developers.\\n\\x0fDoing so via batteries of tasks rather than any single task, that should be previously\\nunknown to both the test taking system and the system developers (this is necessary\\nto assess broad generalization as opposed to skill or local generalization).\\n\\x0fHaving explicit standards regarding reliability, validity, standardization , and freedom\\nfrom bias :\\n‚ÄìReliability implies that the test results for a given system should be reproducible\\nover time and across research groups.\\n‚ÄìValidity implies that what the test assesses should be clearly understood; test\\ncreators should be able to answer 1) what assumptions does the test make? 2)\\nwhat does the test predict, i.e. what broad abilities would a successful result\\ndemonstrate, and how well does the test predict these abilities? (Which should\\nideally be achieved via statistical quantiÔ¨Åcation.)\\n‚ÄìStandardization implies adopting shared benchmarks across the subset of the\\nresearch community that pursues broad AI and general AI. Standard benchmarks\\nin computer vision and natural language processing have already shown to be\\nhighly effective catalyzers of progress.\\n‚ÄìFreedom from bias implies that the test should not be biased against groups of\\ntest-takers in ways that run orthogonal to the abilities being assessed. For in-\\n15stance, a test of intelligence designed for both humans and AI should not lever-\\nage uniquely human acquired knowledge, or should not involve constraints un-\\nrelated to intelligence within which machines have unfair advantages (such as\\nfast reaction times), etc.\\nSimultaneously, we argue that certain other aspects of psychometrics may be discarded\\nin the development of new intelligence tests for AI:\\n\\x0fThe exact number and taxonomy of cognitive abilities considered, being a subject of\\nongoing debate within cognitive psychology and being perhaps overly anthropocen-\\ntric, should not be used as a strict template for artiÔ¨Åcial cognitive architectures and\\ntheir evaluation. Existing taxonomies may at best serve as a source of inspiration.\\n\\x0fA number of abilities being assessed by psychometric intelligence tests are crystal-\\nlized abilities (e.g. reading and writing), i.e. abilities that are acquired through expe-\\nrience, which are not clearly distinguishable from skills (they are effectively multi-\\npurpose skills). We argue that AI tests that seek to assess Ô¨Çexibility and generality\\nshould not consider crystallized abilities, but rather, should focus on abilities that\\nenable new skill acquisition . If a system possesses abilities that enable efÔ¨Åcient skill-\\nacquisition in a domain, the system should have no issue in developing corresponding\\nskills and crystallized abilities.\\nI.3.5 Current trends in broad AI evaluation\\nDespite a rising interest in building Ô¨Çexible systems, or even in generality itself, for the\\nmost part the AI community has not been paying much attention to psychometric evalua-\\ntion, Psychometric AI, or Universal Psychometrics. If we are to assess the contemporary\\nzeitgeist of broad AI evaluation, here is what we see.4\\nFirst, we note several positive developments. Since 2017, there is increasing awareness\\nthat one should seek to establish some form of generalization in evaluating Reinforcement\\nLearning (RL) algorithms (e.g. [50, 70, 17, 49]), which was previously a stark problem\\n[76, 35, 101, 70], as RL agents have for a long time been tested on their training data.\\nFurther, there is increasing interest in evaluating the data-efÔ¨Åciency of learning algorithms\\n(e.g. [10]), in particular in the context of RL for games such as Atari games or Minecraft\\n(e.g. [71, 33]). Lastly, as noted in I.3.3, there has been a trend towards leveraging multi-task\\nbenchmarks as a way to assess robustness and Ô¨Çexibility (e.g. [6, 71, 68, 95, 94]).\\nUnfortunately, we must also note several negatives. The robustness of the systems\\nbeing developed, in particular Deep Learning models, is often problematic (see e.g. [16,\\n59]). This is due in large part to the fact that most benchmarks do not pay much attention\\nto formally assessing robustness and quantifying generalization, and thus can be solved\\nvia ‚Äúshortcuts‚Äù that gradient descent is apt at exploiting (e.g. surface statistics such as\\ntextures in the case of computer vision [46]). Likewise, the reproducibility (reliability) of\\n4Because broad AI research is currently largely dominated by Reinforcement Learning (RL) approaches, many\\nof our observations here are speciÔ¨Åc to RL.\\n16research Ô¨Åndings is often an issue [73], especially in Reinforcement Learning, although\\nsome progress has been made on this front.\\nMost importantly, the evaluation of any ability that goes decisively beyond local gen-\\neralization is still largely a green Ô¨Åeld, and little effort has been devoted to investigate it.\\nHern ¬¥andez-Orallo noted in 2017 that ‚Äúability-oriented and general-purpose evaluation ap-\\nproaches [...] are still very incipient, and more research and discussion is needed‚Äù [36].\\nRecent attempts at broadening task-speciÔ¨Åc benchmarks by including multiple tasks do not\\nmeasure developer-aware generalization, as the tasks are all known in advance to system\\ndevelopers (as noted in I.3.3). Attempts at assessing generalization by testing RL systems\\non previously unseen game levels, like CoinRun [17] or Obstacle Tower [49], are still only\\nlooking at task-speciÔ¨Åc local generalization, by evaluating a candidate system on new sam-\\nples from a known distribution rather than using a substantially new task (as suggested in\\nIII.3). In addition, the fact the level-generation programs used are available to the AI devel-\\nopers means it is possible to ‚Äúcheat‚Äù on these benchmarks by sampling arbitrary amounts\\nof training data (cf. II.1.1).\\nFurther, contemporary research ‚Äúmoonshots‚Äù that are publicly advertised as being steps\\ntowards general intelligence appear to still be focusing on skill-based task-speciÔ¨Åc evalua-\\ntion for board games and video games (e.g. Go [82, 81] and StarCraft [93] for DeepMind,\\nDotA2 [89] for OpenAI) via highly-mediatized confrontations with top human players.\\nDespite claims of progress towards general AI in associated public communications5, such\\nevaluation does not involve any measure of generalization power, and has little-to-no over-\\nlap with the development of Ô¨Çexibility and generality, as we outline in II.1. For example,\\nalthough OpenAI‚Äôs DotA2-playing AI ‚ÄúFive‚Äù was trained on 45,000 years of play and was\\nable to beat top human players [89], it has proven very brittle, as non-champion human\\nplayers were able to Ô¨Ånd strategies to reliably beat it in a matter of days after the AI was\\nmade available for the public to play against [90]. In addition, Five did not even generalize\\nto DotA2 in the Ô¨Årst place: it could only play a restricted version of the game, with 16\\ncharacters instead of over 100. Likewise, AlphaGo and its successor AlphaZero, developed\\nin 2016 and 2017, have not yet found any application outside of board games, to the best of\\nour knowledge.\\nWe deplore this discrepancy between a focus on surpassing humans at tests of skill on\\none hand (while entirely disregarding whether the methods through which skill is achieved\\nare generalizable), and a manifest interest in developing broad abilities on the other hand ‚Äì\\nan endeavour entirely orthogonal to skill itself. We hypothesize that this discrepancy is due\\nto a lack of a clear conceptualization of intelligence, skill, and generalization, as well as a\\nlack of appropriate measures and benchmarks for broad cognitive abilities. In what follows,\\nwe expose in more detail the issue with using task-speciÔ¨Åc ‚Äúmoonshots‚Äù (e.g. achieving\\nbetter-than-human performance in a video game or board game) as stepping stones towards\\nmore general forms of AI, and we propose a formal deÔ¨Ånition of intelligence meant to be\\nactionable in the pursuit of Ô¨Çexible AI and general AI.\\n5OpenAI public statement: ‚ÄúFive is a step towards advanced AI systems which can handle the complexity and\\nuncertainty of the real world‚Äù\\n17II A new perspective\\nII.1 Critical assessment\\nII.1.1 Measuring the right thing: evaluating skill alone does not move us\\nforward\\nIn 1973, psychologist and computer science pioneer Allen Newell, worried that recent ad-\\nvances in cognitive psychology were not bringing the Ô¨Åeld any closer to a holistic theory\\nof cognition, published his seminal paper You can‚Äôt play 20 questions with nature and win\\n[66], which helped focus research efforts on cognitive architecture modelling, and provided\\nnew impetus to the longstanding quest to build a chess-playing AI that would outperform\\nany human. Twenty-four years later, In 1997, IBM‚Äôs DeepBlue beat Gary Kasparov, the\\nbest chess player in the world, bringing this quest to an end [12]. When the dust settled,\\nresearchers were left with the realization that building an artiÔ¨Åcial chess champion had not\\nactually taught them much, if anything, about human cognition. They had learned how\\nto build a chess-playing AI, and neither this knowledge nor the AI they had built could\\ngeneralize to anything other than similar board games.\\nIt may be obvious from a modern perspective that a static chess-playing program based\\non minimax and tree search would not be informative about human intelligence, nor com-\\npetitive with humans in anything other than chess. But it was not obvious in the 1970s,\\nwhen chess-playing was thought by many to capture, and require, the entire scope of ratio-\\nnal human thought. Perhaps less obvious in 2019 is that efforts to ‚Äúsolve‚Äù complex video\\ngames using modern machine learning methods still follow the same pattern. Newell wrote\\n[66]: ‚Äúwe know already from existing work [psychological studies on humans] that the\\ntask [chess] involves forms of reasoning and search and complex perceptual and memorial\\nprocesses. For more general considerations we know that it also involves planning, eval-\\nuation, means-ends analysis and redeÔ¨Ånition of the situation, as well as several varieties\\nof learning ‚Äì short-term, post-hoc analysis, preparatory analysis, study from books, etc. ‚Äù .\\nThe assumption was that solving chess would require implementing these general abilities.\\nChess does indeed involve these abilities ‚Äì in humans. But while possessing these general\\nabilities makes it possible to solve chess (and many more problems), by going from the\\ngeneral to the speciÔ¨Åc, inversely, there is no clear path from the speciÔ¨Åc to the general.\\nChess does not require any of these abilities, and can be solved by taking radical shortcuts\\nthat run orthogonal to human cognition.\\nOptimizing for single-purpose performance is useful and valid if one‚Äôs measure of suc-\\ncess can capture exactly what one seeks (as we outlined in I.3.1), e.g. if one‚Äôs end goal is\\na chess-playing machine and nothing more. But from the moment the objective is settled,\\nthe process of developing a solution will be prone to taking all shortcuts available to satisfy\\nthe objective of choice ‚Äì whether this process is gradient descent or human-driven research.\\nThese shortcuts often come with undesirable side-effects when it comes to considerations\\nnot incorporated the measure of performance. If the environment in which the system is\\nto operate is too unpredictable for an all-encompassing objective function to be deÔ¨Åned\\nbeforehand (e.g. most real-world applications of robotics, where systems face unknown\\n18unknowns), or if one aims at a general-purpose AI that could be applied to a wide range\\nof problems with no or little human engineering, then one must somehow optimize directly\\nfor Ô¨Çexibility and generality , rather than solely for performance on any speciÔ¨Åc task.\\nThis is, perhaps, a widely-accepted view today when it comes to static programs that\\nhard-code a human-designed solution. When a human engineer implements a chatbot by\\nspecifying answers for each possible query via if/else statements, we do not assume this\\nchatbot to be intelligent, and we do not expect it to generalize beyond the engineer‚Äôs speci-\\nÔ¨Åcations. Likewise, if an engineer looks at a speciÔ¨Åc IQ test task, comes up with a solution,\\nand write down this solution in program form, we do not expect the program to general-\\nize to new tasks, and we do not believe that the program displays intelligence ‚Äì the only\\nintelligence at work here is the engineer‚Äôs. The program merely encodes the crystallized\\noutput of the engineer‚Äôs thought process ‚Äì it is this process, not its output, that implements\\nintelligence. Intelligence is not demonstrated by the performance of the output program\\n(a skill), but by the fact that the same process can be applied to a vast range of previously\\nunknown problems (a general-purpose ability): the engineer‚Äôs mind is capable of extreme\\ngeneralization. Since the resulting program is merely encoding the output of that process,\\nit is no more intelligent than the ink and paper used to write down the proof of a theorem.\\nHowever, what of a program that is not hard-coded by humans, but trained from data\\nto perform a task? A learning machine certainly may be intelligent: learning is a neces-\\nsary condition to adapt to new information and acquire new skills. But being programmed\\nthrough exposure to data is no guarantee of generalization or intelligence. Hard-coding\\nprior knowledge into an AI is not the only way to artiÔ¨Åcially ‚Äúbuy‚Äù performance on the\\ntarget task without inducing any generalization power. There is another way: adding more\\ntraining data, which can augment skill in a speciÔ¨Åc vertical or task without affecting gener-\\nalization whatsoever.\\nInformation processing systems form a spectrum between two extremes: on one end,\\nstatic systems that consist entirely of hard-coded priors (such as DeepBlue or our if/else\\nchatbot example), and on the opposite end, systems that incorporate very few priors and\\nare almost entirely programmed via exposure to data (such as a hashtable or a densely-\\nconnected neural network). Most intelligent systems, including humans and animals, com-\\nbine ample amounts of both priors and experience, as we point out in II.1.3. Crucially, the\\nability to generalize is an axis that runs orthogonal to the prior/experience plane. Given a\\nlearning system capable of achieving a certain level of generalization, modifying the sys-\\ntem by incorporating more priors or more training data about the task can lead to greater\\ntask-speciÔ¨Åc performance without affecting generalization. In this case, both priors and\\nexperience serve as a way to ‚Äúgame‚Äù any given test of skill without having to display the\\nsort of general-purpose abilities that humans would rely on to acquire the same skill.\\nThis can be readily demonstrated with a simple example: consider a hashtable that uses\\na locality-sensitive hash function (e.g. nearest neighbor) to map new inputs to previously\\nseen inputs. Such a system implements a learning algorithm capable of local generalization,\\nthe extent of which is Ô¨Åxed (independent of the amount of data seen), determined only\\nby the abstraction capabilities of the hash function. This system, despite only featuring\\ntrace amounts of generalization power, is already sufÔ¨Åcient to ‚Äúsolve‚Äù any task for which\\n19unlimited training data can be generated, such as any video game. All that one has to do is\\nobtain a dense sampling of the space of situations that needs to be covered, and associate\\neach situation with an appropriate action vector.\\nAdding ever more data to a local-generalization learning system is certainly a fair strat-\\negy if one‚Äôs end goal is skill on the task considered, but it will not lead to generalization\\nbeyond the data the system has seen (the resulting system is still very brittle, e.g. Deep\\nLearning models such as OpenAI Five), and crucially, developing such systems does not\\nteach us anything about achieving Ô¨Çexibility and generality . ‚ÄúSolving‚Äù any given task with\\nbeyond-human level performance by leveraging either unlimited priors or unlimited data\\ndoes not bring us any closer to broad AI or general AI, whether the task is chess, football,\\nor any e-sport.\\nCurrent evidence (e.g. [51, 46, 16, 59, 50]) points to the fact that contemporary Deep\\nLearning models are local-generalization systems, conceptually similar to a locality-sensitive\\nhashtable ‚Äì they may be trained to achieve arbitrary levels of skill at any task, but do-\\ning so requires a dense sampling of the input-cross-target space considered (as outlined in\\n[16]), which is impractical to obtain for high-value real-world applications, such as L5 self-\\ndriving (e.g. [5] notes that 30 million training situations is not enough for a Deep Learning\\nmodel to learn to drive a car in a plain supervised setting). Hypothetically, it may be shown\\nin the future that methods derived from Deep Learning could be capable of stronger forms\\nof generalization, but demonstrating this cannot be done merely by achieving high skill,\\nsuch as beating humans at DotA2 or Starcraft given unlimited data or unlimited engineer-\\ning; instead, one should seek to precisely establish and quantify the generalization strength\\nof such systems (e.g. by considering prior-efÔ¨Åciency and data-efÔ¨Åciency in skill acquisition,\\nas well as the developer-aware generalization difÔ¨Åculty of the tasks considered). A central\\npoint of this document is to provide a formal framework for doing so (II.2 and II.3). Failing\\nto account for priors, experience, and generalization difÔ¨Åculty in our evaluation methods\\nwill prevent our Ô¨Åeld from climbing higher along the spectrum of generalization (I.3.2) and\\nfrom eventually reaching general AI.\\nIn summary, the hallmark of broad abilities (including general intelligence, as per II.1.2)\\nis the power to adapt to change, acquire skills, and solve previously unseen problems ‚Äì not\\nskill itself, which is merely the crystallized output of the process of intelligence. Testing\\nfor skill at a task that is known in advance to system developers (as is the current trend\\nin general AI research) can be gamed without displaying intelligence, in two ways: 1)\\nunlimited prior knowledge, 2) unlimited training data. To actually assess broad abilities,\\nand thus make progress toward Ô¨Çexible AI and eventually general AI, it is imperative that\\nwe control for priors ,experience , and generalization difÔ¨Åculty in our evaluation methods,\\nin a rigorous and quantitative way.\\nII.1.2 The meaning of generality: grounding the g factor\\nIt is a well-known fact of cognitive psychology that different individuals demonstrate dif-\\nferent cognitive abilities to varying degrees, albeit results across all tests of intelligence\\n20are correlated. This points to cognition being a multi-dimensional object, structured in a\\nhierarchical fashion (Ô¨Ågure 1), with a single generality factor at the top, the g factor. But is\\n‚Äúgeneral intelligence‚Äù the apex of the cognitive pyramid in an absolute sense (as is some-\\ntimes assumed by proponents of ‚ÄúArtiÔ¨Åcial General Intelligence‚Äù), or is it merely a broader\\ncognitive ability, one that would remain fairly specialized, and wouldn‚Äôt be qualitatively\\ndistinct from other abilities lower down the hierarchy? How general is human intelligence?\\nThe No Free Lunch theorem [98, 97] teaches us that any two optimization algorithms\\n(including human intelligence) are equivalent when their performance is averaged across\\nevery possible problem , i.e. algorithms should be tailored to their target problem in order to\\nachieve better-than-random performance. However, what is meant in this context by ‚Äúevery\\npossible problem‚Äù refers to a uniform distribution over problem space; the distribution of\\ntasks that would be practically relevant to our universe (which, due to its choice of laws of\\nphysics, is a specialized environment) would not Ô¨Åt this deÔ¨Ånition. Thus we may ask: is the\\nhuman g factor universal? Would it generalize to every possible task in the universe?\\nThis is a question that is largely irrelevant for psychometrics, because as a subÔ¨Åeld of\\npsychology, it makes the implicit assumption that it is concerned solely with humans and\\nthe human experience. But this question is highly relevant when it comes to AI: if there\\nis such a thing as universal intelligence, and if human intelligence is an implementation\\nof it, then this algorithm of universal intelligence should be the end goal of our Ô¨Åeld, and\\nreverse-engineering the human brain could be the shortest path to reach it. It would make\\nour Ô¨Åeld close-ended: a riddle to be solved. If, on the other hand, human intelligence is\\na broad but ad-hoc cognitive ability that generalizes to human-relevant tasks but not much\\nelse, this implies that AI is an open-ended, fundamentally anthropocentric pursuit, tied to\\na speciÔ¨Åc scope of applicability. This has implications for how we should measure it (by\\nusing human intelligence and human tasks as a reference) and for the research strategies we\\nshould follow to achieve it.\\nThe g factor, by deÔ¨Ånition, represents the single cognitive ability common to success\\nacross all intelligence tests, emerging from applying factor analysis to test results across a\\ndiversity of tests and individuals. But intelligence tests, by construction, only encompass\\ntasks that humans can perform ‚Äì tasks that are immediately recognizable and understand-\\nable by humans (anthropocentric bias), since including tasks that humans couldn‚Äôt perform\\nwould be pointless. Further, psychometrics establishes measurement validity by demon-\\nstrating predictiveness with regard to activities that humans value (e.g. scholastic success):\\nthe very idea of a ‚Äúvalid‚Äù measure of intelligence only makes sense within the frame of\\nreference of human values.\\nIn fact, the interpretation of what speciÔ¨Åc abilities make someone ‚Äúintelligent‚Äù vary\\nfrom culture to culture [100, 86, 18]. More broadly, humans have historically had a poor\\ntrack record when it comes to attributing intelligence to complex information-processing\\nagents around them, whether looking at humans from other cultures or at animals (such\\nas octopuses, dolphins, great apes, etc.). We only reluctantly open up to the possibility\\nthat systems different from ourselves may be ‚Äúintelligent‚Äù if they display relatable human-\\nlike behaviors that we associate with intelligence, such as language or tool use; behaviors\\nthat have high intrinsic complexity and high adaptability but that are not directly relatable\\n21(such as octopus camouÔ¨Çage) are not perceived as intelligent. This observation extends to\\ncollective entities (e.g. markets, companies, Science as an institution) and natural processes\\n(e.g. biological evolution). Although they can be modeled as standalone systems whose\\nabilities and behavior match broadly accepted deÔ¨Ånitions of intelligence (achieving goals\\nacross a wide range of environments, demonstrating Ô¨Çexibility and adaptability, etc.), we do\\nnot categorize these systems as intelligent, simply because they aren‚Äôt sufÔ¨Åciently human-\\nlike.\\nTo use a well-known cross-domain analogy [25]: much like ‚Äúintelligence‚Äù, the notion\\nof ‚Äúphysical Ô¨Åtness‚Äù (as it pertains to sports and other physical activities) is an intuitively-\\nunderstandable, informal, yet useful concept. Like intelligence, Ô¨Åtness is not easily re-\\nducible to any single factor (such as a person‚Äôs age or muscle mass), rather, it seems to\\nemerge from a constellation of interdependent factors. If we sought to rigorously mea-\\nsure physical Ô¨Åtness in humans, we would come up with a set of diverse tests such as\\nrunning a 100m, running a marathon, swimming, doing sit-ups, doing basketball throws,\\netc., not unlike IQ test suites. Across tests results, we would observe clusters of correla-\\ntions, corresponding to broad ‚Äúphysical abilities‚Äù strictly analogous to cognitive abilities\\n(e.g. lung capacity might be such an ‚Äúability‚Äù inducing correlations across tests). Much\\nlike in the case of cognitive abilities, experts would probably disagree and debate as to the\\nexact taxonomy of these broad abilities (is being ‚Äútall and lean‚Äù an ability, or is ‚Äútallness‚Äù a\\nstandalone factor?). And crucially, we should intuitively expect to Ô¨Ånd that all tests results\\nwould be correlated : we would observe a physical g factor, corresponding to the general\\nintuitive construct of ‚Äúphysical Ô¨Åtness‚Äù.\\nBut would this mean that human morphology and motor affordances are ‚Äúgeneral‚Äù in\\nan absolute sense, and that a very Ô¨Åt person could handle any physical task at all? Certainly\\nnot; we are not adapted for the large majority of environments that can be found in the uni-\\nverse ‚Äì from the Earth‚Äôs oceans to the surface of Venus, from the atmosphere of Jupiter to\\ninterstellar space. It is, however, striking and remarkable that human physical abilities gen-\\neralize to a far greater range of environments and tasks than the limited set of environments\\nand activities that guided their evolution. To caricature, human bodies evolved for running\\nin the East-African savanna, yet they are capable of climbing mount Everest, swimming\\nacross lakes, skydiving, playing basketball, etc. This is not a coincidence; by necessity,\\nevolution optimizes for adaptability, whether cognitive adaptability or sensorimotor adapt-\\nability. Human physical capabilities can thus be said to be ‚Äúgeneral‚Äù, but only in a limited\\nsense; when taking a broader view, humans reveal themselves to be extremely specialized,\\nwhich is to be expected given the process through which they evolved.\\nWe argue that human cognition follows strictly the same pattern as human physical\\ncapabilities: both emerged as evolutionary solutions to speciÔ¨Åc problems in speciÔ¨Åc en-\\nvironments (commonly known as ‚Äúthe four Fs‚Äù). Both were, importantly, optimized for\\nadaptability, and as a result they turn out to be applicable for a surprisingly greater range\\nof tasks and environments beyond those that guided their evolution (e.g. piano-playing,\\nsolving linear algebra problems, or swimming across the Channel) ‚Äì a remarkable fact\\nthat should be of the utmost interest to anyone interested in engineering broad or general-\\npurpose abilities of any kind. Both are multi-dimensional concepts that can be modeled as\\n22a hierarchy of broad abilities leading up to a ‚Äúgeneral‚Äù factor at the top. And crucially, both\\nare still ultimately highly specialized (which should be unsurprising given the context of\\ntheir development): much like human bodies are unÔ¨Åt for the quasi-totality of the universe\\nby volume, human intellect is not adapted for the large majority of conceivable tasks.\\nThis includes obvious categories of problems such as those requiring long-term plan-\\nning beyond a few years, or requiring large working memory (e.g. multiplying 10-digit\\nnumbers). This also includes problems for which our innate cognitive priors are unadapted;\\nfor instance, humans can be highly efÔ¨Åcient in solving certain NP-hard problems of small\\nsize when these problems present cognitive overlap with evolutionarily familiar tasks such\\nas navigation (e.g. the Euclidean Traveling Salesman Problem (TSP) with low point count\\ncan be solved by humans near-optimally in near-linear optimal time [58], using perceptual\\nstrategies), but perform poorly ‚Äì often no better than random search ‚Äì for problem instances\\nof very large size or problems with less cognitive overlap with evolutionarily familiar tasks\\n(e.g. certain non-Euclidean problems). For instance, in the TSP, human performance de-\\ngrades severely when inverting the goal from ‚ÄúÔ¨Ånding the shortest path‚Äù to ‚ÄúÔ¨Ånding the\\nlongest path‚Äù [57] ‚Äì humans perform even worse in this case than one of the simplest pos-\\nsible heuristic: farthest neighbor construction.6\\nA particularly marked human bias is dimensional bias: humans show excellent perfor-\\nmance on 2D navigation tasks and 2D shape-packing puzzles, and can still handle 3D cases\\nalbeit with greatly reduced performance, but they are effectively unable to handle 4D and\\nhigher. This fact is perhaps unsurprising given human reliance on perceptual strategies for\\nproblem-solving ‚Äì strategies which are backed by neural mechanisms speciÔ¨Åcally evolved\\nfor 2D navigation (hippocampal systems of place cells and grid cells [64]).\\nThus, a central point of this document is that ‚Äúgeneral intelligence‚Äù is not a binary\\nproperty which a system either possesses or lacks. It is a spectrum, tied to 1) a scope of\\napplication, which may be more or less broad, and 2) the degree of efÔ¨Åciency with which the\\nsystem translate its priors and experience into new skills over the scope considered, 3) the\\ndegree of generalization difÔ¨Åculty represented by different points in the scope considered\\n(see II.2). In addition, the ‚Äúvalue‚Äù of one scope of application over another is entirely\\nsubjective; we wouldn‚Äôt be interested in (and wouldn‚Äôt even perceive as intelligent) a system\\nwhose scope of application had no intersection with our own.\\nAs such, it is conceptually unsound to set ‚ÄúartiÔ¨Åcial general intelligence‚Äù in an absolute\\nsense (i.e. ‚Äúuniversal intelligence‚Äù) as a goal. To set out to build broad abilities of any kind,\\none must start from a target scope, and one must seek to achieve a well-deÔ¨Åned intelligence\\nthreshold within this scope: AI is a deeply contextual and open-ended endeavour, not a sin-\\ngle one-time riddle to be solved. However, it may in theory be possible to create human-like\\nartiÔ¨Åcial intelligence: we may gradually build systems that extend across the same scope\\nof applicability as human intelligence, and we may gradually increase their generalization\\npower within this scope until it matches that of humans. We may even build systems with\\nhigher generalization power (as there is no a priori reason to assume human cognitive ef-\\n6This does not necessarily mean that humanity as a collective is incapable of solving these problems; pooling\\nindividual humans over time or augmenting human intellect via external resources leads to increased generality,\\nalbeit this increase remains incremental, and still fundamentally differs from universality.\\n23Ô¨Åciency is an upper bound), or systems with a broader scope of application. Such systems\\nwould feature intelligence beyond that of humans.\\nIn conclusion, we propose that research on developing broad in AI systems (up to ‚Äúgen-\\neral‚Äù AI, i.e. AI with a degree of generality comparable to human intelligence) should focus\\nondeÔ¨Åning, measuring, and developing a speciÔ¨Åcally human-like form of intelligence, and\\nshould benchmark progress speciÔ¨Åcally against human intelligence (which is itself highly\\nspecialized). This isn‚Äôt because we believe that intelligence that greatly differs from our\\nown couldn‚Äôt exist or wouldn‚Äôt have value; rather, we recognize that characterizing and\\nmeasuring intelligence is a process that must be tied to a well-deÔ¨Åned scope of applica-\\ntion, and at this time, the space of human-relevant tasks is the only scope that we can\\nmeaningfully approach and assess . We thus disagree with the perspective of Universal\\nPsychometrics [39] or Legg and Hutter‚Äôs Universal Intelligence [54], which reject anthro-\\npocentrism altogether and seek to measure all intelligence against a single absolute scale.\\nAn anthropocentric frame of reference is not only legitimate, it is necessary.\\nII.1.3 Separating the innate from the acquired: insights from developmental\\npsychology\\nAdvances in developmental psychology teach us that neither of the two opposing views of\\nthe nature of the mind described in I.2 are accurate (see e.g. [85]): the human mind is not\\nmerely a collection of special-purpose programs hard-coded by evolution; it is capable of a\\nremarkable degree of generality and open-endedness, going far beyond the scope of envi-\\nronments and tasks that guided its evolution. The large majority of the skills and knowledge\\nwe possess are acquired during our lifetimes, rather than innate. Simultaneously, the mind\\nis not a single, general-purpose ‚Äúblank slate‚Äù system capable of learning anything from ex-\\nperience. Our cognition is specialized, shaped by evolution in speciÔ¨Åc ways; we are born\\nwith priors about ourselves, about the world, and about how to learn, which determine what\\ncategories of skills we can acquire and what categories of problems we can solve.\\nThese priors are not a limitation to our generalization capabilities; to the contrary, they\\nare their source, the reason why humans are capable of acquiring certain categories of skills\\nwith remarkable efÔ¨Åciency. The central message of the No Free Lunch theorem [98] is that\\nto learn from data, one must make assumptions about it ‚Äì the nature and structure of the\\ninnate assumptions made by the human mind are precisely what confers to it its powerful\\nlearning abilities.\\nWe noted in II.1.1 that an actionable measure of intelligence should, crucially, con-\\ntrol for priors andexperience . We proposed in II.1.2 that evaluating general intelligence\\nshould leverage human intelligence as a necessary frame of reference. It follows that we\\nneed a clear understanding of human cognitive priors in order to fairly evaluate general\\nintelligence between humans and machines.\\nHuman cognitive priors come in multiple forms, in particular7:\\n7The boundaries between these categories may be Ô¨Çuid; the distinction between low-level sensorimotor priors\\nand high-level knowledge priors is more one of degree than one of nature; likewise, the distinction between meta-\\n24\\x0fLow-level priors about the structure of our own sensorimotor space, e.g. reÔ¨Çexes such\\nas the vestibulo-ocular reÔ¨Çex, the palmar grasp reÔ¨Çex, etc. These priors enable infants\\n(including prior to birth) to quickly take control of their senses and bodies, and may\\neven generate simple behaviors in a limited range of situations.\\n\\x0fMeta-learning priors governing our learning strategies and capabilities for knowledge\\nacquisition. This may include, for instance, the assumption that information in the\\nuniverse follows a modular-hierarchical structure, as well as assumptions regarding\\ncausality and spatio-temporal continuity.\\n\\x0fHigh-level knowledge priors regarding objects and phenomena in our external envi-\\nronment. This may include prior knowledge of visual objectness (what deÔ¨Ånes an\\nobject), priors about orientation and navigation in 2D and 3D Euclidean spaces, goal-\\ndirectedness (expectation that our environment includes agents that behave according\\nto goals), innate notions about natural numbers, innate social intuition (e.g. theory of\\nmind), etc.\\nWhen it comes to creating artiÔ¨Åcial human-like intelligence, low-level sensorimotor pri-\\nors are too speciÔ¨Åc to be of interest (unless one seeks to build an artiÔ¨Åcial human body).\\nWhile human meta-learning priors should be of the utmost interest (understanding the\\nstrategies that the brain follows to turn experience into knowledge and skills is effectively\\nour end goal), these priors are not relevant to evaluating intelligence: they areintelligence,\\nrather than a third-party modulating factor to be controlled for. They are part of the black\\nbox that we seek to characterize.\\nIt is knowledge priors that should be accounted for when measuring a human-like form\\nof intelligence. A system that does not possess human innate knowledge priors would be at\\na critical disadvantage compared to humans when it comes to efÔ¨Åciently turning a given ex-\\nperience curriculum into skill at a given human task. Inversely, a system that has access to\\nmore extensive hard-coded knowledge about the task at hand could not be fairly compared\\nto human intelligence ‚Äì as we noted in II.1.1, unlimited priors allow system developers to\\n‚Äúbuy‚Äù unbounded performance on any given task, with no implications with regard to gen-\\neralization abilities (what we are actually trying to achieve).\\nTherefore, we propose that an actionable test of human-like general intelligence should\\nbe founded on innate human knowledge priors :\\n\\x0fThe priors should be made as close as possible to innate human knowledge priors\\nas we understand them. As our understanding of human knowledge priors improves\\nover time, so should the test evolve.\\n\\x0fThe test should assume that the system being measured possesses a speciÔ¨Åc set of\\npriors. AI systems with more extensive priors should not be benchmarked using such\\na test. AI systems with fewer priors should be understood to be at a disadvantage.\\nlearning priors and knowledge priors is subjective since knowledge facilitates skill acquisition: for instance, the\\nneural mechanism behind our capabilities to perform 2D navigation may be treated either as a specialized meta-\\nlearning prior or as a knowledge prior about the external world.\\n25\\x0fThe priors assumed by the test should be explicitly and exhaustively described. Im-\\nportantly, current psychometric intelligence tests make many assumptions about prior\\nknowledge held by the test-taker (either innate or acquired), but never explicitly de-\\nscribe these assumptions.\\n\\x0fTo make sure that humans test-takers do not bring further priors to the test, the test\\ntasks should not rely on any acquired human knowledge (i.e. any knowledge beyond\\ninnate prior knowledge). For instance, they should not rely on language or learned\\nsymbols (e.g. arrows), on acquired concepts such as ‚Äúcat‚Äù or ‚Äúdog‚Äù, or on tasks for\\nwhich humans may have trained before (e.g. chess).\\nThis leads us to a central question: what is the exact list of knowledge priors that\\nhumans are born with? This is the question that the developmental science theory of Core\\nKnowledge [85] seeks to answer. Core Knowledge identiÔ¨Åes four broad categories of innate\\nassumptions that form the foundations of human cognition, and which are largely shared\\nby our non-human relatives8:\\n\\x0fObjectness and elementary physics: humans assume that their environment should\\nbe parsed into ‚Äúobjects‚Äù characterized by principles of cohesion (objects move as\\ncontinuous, connected, bounded wholes), persistence (objects do not suddenly cease\\nto exist and do not suddenly materialize), and contact (objects do not act at a distance\\nand cannot interpenetrate).\\n\\x0fAgentness and goal-directedness: humans assume that, while some objects in their\\nenvironment are inanimate, some other objects are ‚Äúagents‚Äù, possessing intentions of\\ntheir own, acting so as to achieve goals (e.g. if we witness an object A following\\nanother moving object B, we may infer that A is pursuing B and that B is Ô¨Çeeing\\nA), and showing efÔ¨Åciency in their goal-directed actions. We expect that these agents\\nmay act contingently and reciprocally.\\n\\x0fNatural numbers and elementary arithmetic: humans possess innate, abstract number\\nrepresentations for small numbers, which can be applied to entities observed through\\nany sensory modality. These number representations may be added or subtracted, and\\nmay be compared to each other, or sorted.\\n\\x0fElementary geometry and topology: this core knowledge system captures notions\\nof distance, orientation, in/out relationships for objects in our environment and for\\nourselves. It underlies humans‚Äô innate facility for orienting themselves with respect\\nto their surroundings and navigating 2D and 3D environments.\\n8Core Knowledge has been written into our DNA by natural evolution. Natural evolution is an extremely\\nlow-bandwidth, highly selective mechanism for transferring information from the surrounding environment to an\\norganism‚Äôs genetic code. It can only transfer information associated with evolutionary pressures, and it can only\\nwrite about aspects of the environment that are stable over sufÔ¨Åciently long timescales. As such, it would not\\nbe reasonable to expect humans to possess vast amounts of human-speciÔ¨Åc prior knowledge; core knowledge is\\nevolutionarily ancient and largely shared across many species, in particular non-human primates.\\n26While cognitive developmental psychology has not yet determined with a high degree of\\ncertainty the exact set of innate priors that humans possess, we consider the Core Knowl-\\nedge theory to offer a credible foundation suitable to the needs of a test of human-like\\ngeneral intelligence. We therefore propose that an actionable test of general intelligence\\nthat would be fair for both humans and machines should only feature tasks that assume the\\nfour core knowledge systems listed above, and should not involve any acquired knowledge\\noutside of these priors. We also argue, in agreement with [51], that general AI systems\\nshould hard-code as fundamental priors these core knowledge principles.\\nII.2 DeÔ¨Åning intelligence: a formal synthesis\\nII.2.1 Intelligence as skill-acquisition efÔ¨Åciency\\nSo far, we have introduced the following informally-described intuitions:\\n\\x0fIntelligence lies in broad or general-purpose abilities; it is marked by Ô¨Çexibility and\\nadaptability (i.e. skill-acquisition and generalization), rather than skill itself. The\\nhistory of AI has been a slow climb along the spectrum of generalization.\\n\\x0fA measure of intelligence should imperatively control for experience and priors, and\\nshould seek to quantify generalization strength, since unlimited priors or experience\\ncan produce systems with little-to-no generalization power (or intelligence) that ex-\\nhibit high skill at any number of tasks.\\n\\x0fIntelligence and its measure are inherently tied to a scope of application. As such,\\ngeneral AI should be benchmarked against human intelligence and should be founded\\non a similar set of knowledge priors.\\nLet us now formalize these intuitions. In what follows, we provide a series of deÔ¨Ånitions\\nfor key concepts necessary to ground a formal deÔ¨Ånition of intelligence and its measure.\\nWe will leverage the tools of Algorithmic Information Theory. These deÔ¨Ånitions lead up to\\na formal way of expressing the following central idea:\\nThe intelligence of a system is a measure of its skill-acquisition efÔ¨Åciency over a scope\\nof tasks, with respect to priors, experience, and generalization difÔ¨Åculty.\\nIntuitively, if you consider two systems that start from a similar set of knowledge priors,\\nand that go through a similar amount of experience (e.g. practice time) with respect to a\\nset of tasks not known in advance, the system with higher intelligence is the one that ends\\nup with greater skills (i.e. the one that has turned its priors and experience into skill more\\nefÔ¨Åciently). This deÔ¨Ånition of intelligence encompasses meta-learning priors, memory, and\\nÔ¨Çuid intelligence. It is distinct from skill itself: skill is merely the output of the process of\\nintelligence.\\nBefore we start, let us emphasize that many possible deÔ¨Ånitions of intelligence may\\nbe valid, across many different contexts, and we do not purport that the deÔ¨Ånition above\\nand the formalism below represent the ‚Äúone true‚Äù deÔ¨Ånition. Nor is our deÔ¨Ånition meant\\n27to achieve broad consensus. Rather, the purpose of our deÔ¨Ånition is to be actionable, to\\nserve as a useful perspective shift for research on broad cognitive abilities, and to function\\nas a quantitative foundation for new general intelligence benchmarks, such as the one we\\npropose in part III. As per George Box‚Äôs aphorism, ‚Äúall models are wrong, but some are\\nuseful‚Äù : our only aim here is to provide a useful North Star towards Ô¨Çexible and general\\nAI. We discuss in II.2.3 the concrete ways in which our formalism is useful and actionable.\\nPosition of the problem\\nFirst, we must introduce basic deÔ¨Ånitions to establish our problem setup. It should be\\nimmediately clear to the reader that our choice of problem setup is sufÔ¨Åcient to model\\nFully-Supervised Learning, Partially-Supervised Learning, and Reinforcement Learning.\\nWe consider the interaction between a ‚Äútask‚Äù and an ‚Äúintelligent system‚Äù. This interac-\\ntion is mediated by a ‚Äúskill program‚Äù (generated by the intelligent system) and a ‚Äúscoring\\nfunction‚Äù (part of the task).\\nWe implicitly consider the existence of a Ô¨Åxed universal Turing machine on which\\nour programs run (including the skill programs, as well as programs part of the task and\\npart of the intelligent system). We also assume the existence of a Ô¨Åxed ‚Äúsituation space‚Äù\\nSituationSpace and ‚Äúresponse space‚Äù ResponseSpace . Each of these spaces deÔ¨Ånes the\\nset of binary strings that are allowed as input (and output, respectively) of all skill pro-\\ngrams we will consider henceforth. They may be, for instance, the sensor space and the\\nmotor space of an animal or robot.\\nFigure 2: Position of the problem: an intelligent system generates a skill program to interact\\nwith a task.\\nAtaskTconsists of four objects:\\n\\x0fA task state TaskState (binary string).\\n\\x0fA ‚Äúsituation generation‚Äù function SituationGen :TaskState!Situation . It\\nmay be stochastic.\\n‚ÄìASituation is a binary string belonging to SituationSpace .\\n28\\x0fA ‚Äúscoring function‚Äù Scoring : [Situation;Response;TaskState ]![Score;Feedback ].\\nIt may be stochastic.\\n‚ÄìAResponse is a binary string belonging to ResponseSpace .\\n‚ÄìA ‚Äúscore‚ÄùScore is a scalar. It is meant to measure the appropriateness of a\\nresponse to a situation.\\n‚ÄìA piece of ‚Äúfeedback‚Äù Feedback is a binary string. It may encode full or par-\\ntial information about the current score, or about scores corresponding to past\\nresponses (which may be known to the task state).\\n‚ÄìNote: The parameter Situation is technically optional since it may be known\\nto the task state at runtime ‚Äì we include it here for maximum explicitness.\\n\\x0fA self-update function TaskUpdate : [Response;TaskState ]!TaskState , which\\nmutates the task state based on the response to the latest situation. It may be stochas-\\ntic.\\nFor instance, a game such as chess or WarCraft III (as well as what we call ‚Äútask‚Äù in the\\nARC benchmark presented in III) would constitute a task. A given chess board position,\\nscreen frame in WarCraft III, or input grid in ARC, would constitute a situation.\\nAnintelligent system ISconsists of three objects:\\n\\x0fA system state ISState (binary string).\\n\\x0fA ‚Äúskill program generation function‚Äù:\\nSkillProgramGen :ISState![SkillProgram;SPState ].\\nIt may be stochastic.\\n‚ÄìA skill program SkillProgram : [Situation;SPState ]![Response;SPState ]\\nis a function that maps an input situation to a valid response (part of ResponseSpace ),\\npotentially using some working memory ( SPState ). It may be stochastic. Be-\\ncause it possesses a state SPState (a binary string), it may be used to au-\\ntonomously handle a series of connected situations without further communi-\\ncation with the intelligent system that generated it.\\n‚ÄìA skill program may be, for instance, any game-speciÔ¨Åc program capable of\\nplaying new levels in a given video game.\\n‚ÄìIn what follows, we refer to ‚Äúskill program‚Äù as the combination of the SkillProgram\\nfunction and the initial skill program state SPState (i.e. skill programs are con-\\nsidered stateful).\\n‚ÄìA skill program represents a frozen version of the system‚Äôs task-speciÔ¨Åc capa-\\nbilities (including the ability to adapt to novel situations within the task). We\\nuse the concept of skill program as a conceptual device to formalize the level\\nof task-speciÔ¨Åc skill and task-speciÔ¨Åc generalization capabilities of an agent at\\na given point in time.\\n29\\x0fA self-update function ISUpdate : [Situation;Response;Feedback;ISState ]!\\nISState , which mutates the system‚Äôs state based on the latest situation and corre-\\nsponding feedback. It may be stochastic.\\nFor instance, a neural network generation and training algorithm for games would be an\\n‚Äúintelligent system‚Äù, and the inference-mode game-speciÔ¨Åc network it would output at the\\nend of a training run on one game would be a ‚Äúskill program‚Äù. A program synthesis engine\\ncapable of looking at an ARC task and outputting a solution program would be an ‚Äúintelli-\\ngent system‚Äù, and the resulting solution program capable of handling future input grids for\\nthis task would be a ‚Äúskill program‚Äù.\\nThe interaction between task, intelligent system, and skill programs is structured in two\\nphases: a training phase and an evaluation phase. The goal of the training phase is for the\\nISto generate a high-skill skill program that will generalize to future evaluation situations.\\nThe goal of the evaluation phase is to assess the capability of this skill program to handle\\nnew situations.\\nThe training phase consists of the repetition of the following steps (we note the current step\\nast). Before we start, we consider two separate initial task states, trainTaskState t=0and\\ntestTaskState e=0.\\n\\x0fWe generate a training situation: situation t SituationGen (trainTaskState t)\\n\\x0fTheISgenerates a new skill program (without knowledge of the current situation):\\n[skillProgram t;spStatet] SkillProgramGen (isStatet)\\n‚ÄìImplicitly, we assume that the ‚Äúgoal‚Äù of the ISis to generate highly-skilled\\nprograms, i.e. programs that would have performed well on past situations, that\\nwill perform well on the next situation, and that would perform well on any\\npossible situation for this task (in particular evaluation situations, which may\\nfeature signiÔ¨Åcant novelty and uncertainty). We do not attempt to model why\\ntheISshould pursue this goal.\\n‚ÄìspStatetrepresents the working memory of the skill program at time t. Note\\nthat, because the skill program is generated anew with each training step, state-\\nfulness across situations via SPState is not actually required during training.\\nHowever, statefulness is important during evaluation when handling tasks that\\nrequire maintaining information across situations. Note that in many games or\\nreal-world tasks, situations are all independent and thus skill programs don‚Äôt\\nrequire statefulness at all (e.g. ARC, or any fully-observable game, like chess).\\n\\x0fThe skill program outputs a response to the situation:\\n[response t;spStatet+1] skillProgram t(Situation t;spStatet)\\n‚ÄìskillProgram tis only called once, and spStatet+1is discarded, since the skill\\nprogram is generated anew by the intelligent system at each training step.\\n‚ÄìIn practice, in partially-observable games where consecutive situations are very\\nclose to each other (e.g. two consecutive screen frames in WarCraft III), one\\n30may assume that skillProgram tattandskillProgram t+1would not actu-\\nally be generated independently from scratch and would stay very close to each\\nother (i.e. the IS‚Äôs understanding of the task would be evolving continuously in\\nprogram space); spStatet+1as generated by skillProgram tandspStatet+1\\nas generated by SkillProgramGen att+ 1would likewise stay very close to\\neach other.\\n\\x0fThe task scoring function assigns a score to the response and generates a piece of\\nfeedback: [scoret;feedback t] Scoring (Situation t;response t;trainTaskState t)\\n‚ÄìNote: The scalar score is meant to encode how appropriate the response is, and\\nthe feedback data is meant to be used by the intelligent system to update its\\nstate. In simple cases (e.g. fully-supervised learning), the feedback data is the\\nsame as the scalar score, meaning that the intelligent agent would have complete\\nand immediate information about the appropriateness of its response. In other\\ncases, the feedback data may only contain partial information, no information,\\nor information that is only relevant to responses generated for prior situations\\n(delayed feedback).\\n\\x0fTheISupdates its internal state based on the feedback received from the task:\\nisStatet+1 ISUpdate (Situation t;response t;feedback t;isStatet)\\n\\x0fThe task updates its internal state based on the response received to the situation:\\ntrainTaskState t+1 TaskUpdate (response t;trainTaskState t)\\nThe training phase ends at the discretion of the SituationGen function (e.g. SituationGen\\nreturns a ‚ÄúSTOP‚Äù situation), at which time SkillProgramGen would generate its last skill\\nprogram, including an initial state (initial working memory) meant to perform well during\\nevaluation (e.g. blank).\\nThe evaluation phase is superÔ¨Åcially similar to the training phase, with the differences that\\n1) the task starts from testTaskState e=0and consists of an independent series of situ-\\nations, 2) it only involves a single Ô¨Åxed skill program testSkillProgram starting with\\nstatetestSPState e=0. Crucially, it no longer involves the intelligent system. Note that\\ntestTaskState e=0could be chosen stochastically. For instance, different randomly cho-\\nsen initialtestTaskState e=0could be different randomly-generated levels of a game.\\nLike the separation between skill program and intelligent system, the evaluation phase\\nshould be understood as a conceptual device used to quantify the task-speciÔ¨Åc skill and\\ntask-speciÔ¨Åc generalization capabilities demonstrated by a system at a given point in time.\\nThe evaluation phase should notbe seen as being conceptually similar to a child taking a\\nschool test or an IQ test. In real-world evaluation situations, evaluation involves the entire\\nintelligent system, dynamically adapting its understanding of the task at hand. A real-world\\nevaluation situation would be represented in our formalism as being part of the training cur-\\nriculum ‚Äì a series of training situations with blank feedback.\\n31The evaluation phase consists of the repetition of the following steps (the current step is\\nnotede):\\n\\x0fWe generate a test situation: situation e SituationGen (testTaskState e)\\n\\x0fThe skill program considered produces a response:\\n[response e;testSPState e+1] testSkillProgram (situation e;testSPState e)\\n‚ÄìNote that the update of the skill program state enables the skill program to\\nmaintain a working memory throughout the evaluation phase. This is useful for\\npartially-observable games. This is irrelevant to many games (including ARC)\\nand many real-world tasks, where skill programs would be stateless.\\n\\x0fThe task scoring function assigns a score to the response (the feedback is discarded):\\nscoree Scoring (Situation e;response e;testTaskState e)\\n\\x0fThe task updates its internal state based on the response received:\\ntestTaskState e+1 TaskUpdate (response e;testTaskState e)\\nThe evaluation phase also ends at the discretion of the SituationGen function.\\nNote that for the sake of simpliÔ¨Åcation, we consider that the IS‚Äôs state does not transfer\\nacross tasks; the ISwould start with a ‚Äúblank‚Äù state at the beginning of the training phase\\nfor each new task (i.e. only possessing built-in priors). However, the setup above and def-\\ninitions below may be readily extended to consider lifelong learning, to bring it closer to\\nreal-world biological intelligent systems, which learn continuously across a multiplicity of\\npartially overlapping tasks with often no clear boundaries.\\nBased on the setup described thus far, we can deÔ¨Åne the following useful concepts:\\n\\x0fEvaluation result : Sum of the scalar scores obtained by a Ô¨Åxed skill program over\\na speciÔ¨Åc evaluation phase instance for a task. Since all objects involved (skill pro-\\ngram, situation generation program, task update program, initial task state) may be\\nstochastic, this quantity may also be stochastic. Likewise, we deÔ¨Åne training-time\\nperformance as the sum of the scalar scores obtained during a given training phase.\\nTraining-time performance is tied to a speciÔ¨Åc sequence of training situations.\\n\\x0fSkill: Probabilistic average of evaluation results over all possible evaluation phase\\ninstances, i.e. average of per-evaluation sum of scores obtained after running the\\nevaluation phase inÔ¨Ånitely many times. Skill is a property of a skill program. Note\\nthat other distributional reduction functions could be used, such as median or mini-\\nmum.\\n\\x0fOptimal skill : Maximum skill theoretically achievable by the best possible skill pro-\\ngram on the task. It is a property of a task.\\n\\x0fSufÔ¨Åcient skill threshold , noted\\x12T: Subjective threshold of skill associated with a\\ntask, above which a skill program can be said to ‚Äúsolve‚Äù the task. It is a property of a\\ntask.\\n32\\x0fTask and skill value function : We deÔ¨Åne a value function over task space (note that\\ntask space may be inÔ¨Ånite), associating a scalar value to the combination of a task\\nand a threshold of skill \\x12for the task : TaskValue :Task;\\x12!!T;\\x12. Values\\nare assumed positive or zero, and TaskValue is assumed monotonous as a function\\nof\\x12(for a given task, higher skill always has higher value). This value function\\ncaptures the relative importance of skill at each task and deÔ¨Ånes the subjective frame\\nof reference of our intelligence deÔ¨Ånition (for instance, if we wish to evaluate human-\\nlike intelligence, we would place high value on achieving high skill at human-relevant\\ntasks and place no value on tasks that are irrelevant to the human experience). The\\nvalue!T;\\x12of a skill level at a task is chosen so that the quantity !T;\\x12can be compared\\nfairly across different tasks (i.e. it should capture the value we place on achieving skill\\n\\x12at taskT). This enables us to homogeneously aggregate skill across different tasks\\nwithout worrying about the scale of the their respective scoring functions.\\n\\x0fTask value , noted!T: This is the value of achieving sufÔ¨Åcient skill level at T, i.e.\\n!T=!T;\\x12T.\\n\\x0fOptimal solution : Any skill program that can achieve optimal skill on a task. Like-\\nwise we deÔ¨Åne a training-time optimal solution as any skill program that can achieve\\noptimal training-time performance over a speciÔ¨Åc sequence of training situations.\\n\\x0fSufÔ¨Åcient solution : Any skill program that can achieve sufÔ¨Åcient skill \\x12Ton a task.\\n\\x0fCurriculum : Sequence of interactions (situations, responses, and feedback) between\\na task and an intelligent system over a training phase. For a given task and intelligent\\nsystem, there exists a space of curricula, parameterized by the stochastic components\\nof the underlying programs. A curriculum emerges from the interaction between the\\nsystem and a task: this can model both teaching and active learning.\\n\\x0fOptimal curriculum : curriculum which leads an intelligent system to produce the best\\n(highest skill) skill program it can generate for this task. It is speciÔ¨Åc to a task and an\\nintelligent system. There may be more than one optimal curriculum.\\n\\x0fSufÔ¨Åcient curriculum : curriculum which leads an intelligent system to a sufÔ¨Åcient\\nsolution. It is speciÔ¨Åc to a task and an intelligent system. There may be more than\\none sufÔ¨Åcient curriculum.\\n\\x0fTask-speciÔ¨Åc potential , noted\\x12max\\nT;IS: Skill of the best possible skill program that can\\nbe generated by a given intelligent system on a task (after an optimal curriculum). It\\nis a scalar value speciÔ¨Åc to a task and an intelligent system.\\n\\x0fIntelligent system scope : Subspace of task space including all tasks for which task\\nvalue!Tis non-zero and for which the intelligent system is capable of producing a\\nsufÔ¨Åcient solution after a training phase. This space may be inÔ¨Ånite. ‚ÄúTo be capable\\nof producing a sufÔ¨Åcient solution‚Äù means that there exists a sufÔ¨Åcient curriculum for\\nthe intelligent system and task considered. A scope is a property of an intelligent\\nsystem.\\n\\x0fIntelligent system potential : Set of task-speciÔ¨Åc potential values over all tasks in the\\nsystem‚Äôs scope. Potential is a property of an intelligent system.\\n33We Ô¨Ånd that in most cases it is more useful to consider sufÔ¨Åcient skill and sufÔ¨Åcient so-\\nlutions rather than optimal skill and optimal solutions ‚Äì in application settings, we seek\\nto achieve sufÔ¨Åcient performance using as little resources as possible; it is rarer and less\\npractical to seek to achieve maximum possible performance using unlimited resources.\\nQuantifying generalization difÔ¨Åculty, experience, and priors using Algorith-\\nmic Information Theory\\nAlgorithmic Information Theory (AIT) may be seen as a computer science extension of In-\\nformation Theory. AIT concerns itself with formalizing useful computer science intuitions\\nregarding complexity, randomness, information, and computation. Central to AIT is the\\nnotion of Algorithmic Complexity. Algorithmic Complexity (also known as Kolmogorov\\nComplexity or Algorithmic Entropy) was independently investigated, in different contexts,\\nby R.J. Solomonoff, A.N. Kolmogorov and G.J. Chaitin in the 1960s. For an extensive\\nintroduction, see [15, 14, 30, 55].\\nMuch like the concept of Entropy in Information Theory, Algorithmic Complexity is a\\nmeasure of the ‚Äúinformation content‚Äù of mathematical objects. For our own needs, we will\\nonly consider the speciÔ¨Åc case of binary strings. Indeed, all objects we have introduced\\nso far have been either scalar values (score, potential), or binary strings (states, programs,\\nsituations, and responses), since any program may be represented as a binary string.\\nThe Algorithmic Complexity (noted H(s)) of a string sis the length of the shortest\\ndescription of the string in a Ô¨Åxed universal language, i.e. the length of the shortest pro-\\ngram that outputs the string when running on a Ô¨Åxed universal Turing machine. Since\\nany universal Turing machine can emulate any other universal Turing machine, H(s)is\\nmachine-independent to a constant.\\nWe can use Algorithmic Complexity to deÔ¨Åne the information content that a string s2\\npossesses about a string s1(called ‚ÄúRelative Algorithmic Complexity‚Äù and noted H(s1js2)),\\nas the length of the shortest program that, taking s2as input, produces s1. ‚ÄúTo takes2as\\ninput‚Äù means that s2is part of the description of the program, but the length of s2would\\nnot be taken into account when counting the program‚Äôs length.\\nBecause any program may be represented as a binary string, we can use Relative Al-\\ngorithmic Complexity to describe how closely related two programs are. Based on this\\nobservation, we propose to deÔ¨Åne the intuitive notion of ‚ÄúGeneralization DifÔ¨Åculty‚Äù of a\\ntask as follows:\\nConsider:\\n\\x0fA taskT,\\n\\x0fSol\\x12\\nT, the shortest of all possible solutions of Tof threshold \\x12(shortest skill program\\nthat achieves at least skill \\x12during evaluation),\\n\\x0fTrainSolopt\\nT;C, the shortest optimal training-time solution given a curriculum (shortest\\nskill program that achieves optimal training-time performance over the situations in\\nthe curriculum).\\n34We then deÔ¨Åne Generalization DifÔ¨Åculty as:\\nGeneralization DifÔ¨Åculty of a task given a curriculum Cand a skill threshold \\x12, noted\\nGD\\x12\\nT;C: Fraction of the Algorithmic Complexity of solution Sol\\x12\\nTthat is explained by the\\nshortest optimal training-time solution TrainSolopt\\nT;C(i.e. length of the shortest program\\nthat, taking as input the shortest possible program that performs optimally over the situa-\\ntions in curriculum C, produces a program that performs at a skill level of at least \\x12during\\nevaluation, normalized by the length of that skill program). Note that this quantity is be-\\ntween 0 and 1 by construction.\\nGD\\x12\\nT;C=H(Sol\\x12\\nTjTrainSolopt\\nT;C)\\nH(Sol\\x12\\nT)\\nThus, a task with high ‚Äúgeneralization difÔ¨Åculty‚Äù is one where the evaluation-time behav-\\nior needs to differ signiÔ¨Åcantly from the simplest possible optimal training-time behavior\\nin order to achieve sufÔ¨Åcient skill. Relative Algorithmic Complexity provides us with a\\nmetric to quantify this difference: GD is a measure of how much the shortest training-time\\nsolution program needs to be edited in order to become an appropriate evaluation-time so-\\nlution program. If the shortest skill program that performs optimally during training also\\nhappens to perform at a sufÔ¨Åcient skill level during evaluation, the task has zero general-\\nization difÔ¨Åculty (i.e. it does not involve uncertainty). A generalizable skill program is one\\nthat ‚Äúcovers more ground‚Äù in situation space than the exact training situations it is familiar\\nwith: a program that is capable of dealing with future uncertainty.\\nNote that this deÔ¨Ånition of generalization difÔ¨Åculty may seem counter-intuitive. Oc-\\ncam‚Äôs razor principle would seem to suggest that the simplest program that works on the\\ntraining situations should also be a program that generalizes well. However, generalization\\ndescribes the capability to deal with future uncertainty, not the capability to compress the\\nbehavior that would have been optimal in the past ‚Äì being prepared for future uncertainty\\nhas a cost, which is antagonistic to policy compression9. By necessity, TrainSolopt\\nT;Cdoes\\naway with any information or capability that isn‚Äôt strictly necessary in order to produce\\nthe correct response to training situations, and in doing so, it may discard information or\\ncapabilities that would have been useful to process evaluation situations. If it is in fact the\\ncase thatTrainSolopt\\nT;Cdoes not need to discard any such information (i.e. the simplest\\nbehavior that was optimal in the past is still sufÔ¨Åcient in the future), this implies that the\\nevaluation features no need for adaptation (no non-trivial novelty or uncertainty), and thus\\nthe task does not involve generalization, potentially given some starting point (such as the\\nsolution of another task).\\nAnother way to express the same idea is that generalization requires to reinterpret the\\n9As a philosophical aside: this is why the education of children involves practicing games and ingesting knowl-\\nedge of seemingly no relevance to their past or present decision-making needs, but which prepare them for future\\nsituations (a process often driven by curiosity). A 10-year-old who has only learned the simplest behavioral pol-\\nicy that would have maximized their extrinsic rewards (e.g. candy intake) during ages 0-10 would not be well\\neducated, and would not generalize well in future situations.\\n35task when new data arrives (e.g. at evaluation time). This implies the need to store repre-\\nsentations of past data that would be seemingly useless from the perspective of the past but\\nmay prove useful in the future. For example, consider the following labeled points along a\\nline:(x=\\x000:75;label =False );(x= 0:15;label =True );(x=\\x000:1;label =True ).\\nWhen training a classiÔ¨Åcation program on the Ô¨Årst two of these points, some of the shortest\\noptimal training-time solutions may be \\x15(x) :x>0or\\x15(x) :bool(ceil(x)). When applied\\nto the last point (x=\\x000:1;label =True ), these solutions would fail, while an algorithm\\nthat instead stores all past data points and uses nearest-neighbors to return a response at\\nevaluation time would work. The nearest-neighbors program would be better prepared for\\nfuture uncertainty, but would take signiÔ¨Åcantly more space to write down.\\nImportantly, this Ô¨Årst deÔ¨Ånition of generalization difÔ¨Åculty only captures system-centric\\ngeneralization, as it quantiÔ¨Åes the difÔ¨Åculty of handling evaluation situations that differ\\nfrom training situations regardless of the system‚Äôs preexisting capabilities. To capture\\ndeveloper-aware generalization, we need to take into account the system in its initial state\\nat the start of training, SkillProgramGen;ISUpdate;isState t=0:\\nDeveloper-aware Generalization DifÔ¨Åculty of a task for an intelligent system given\\na curriculum Cand a skill threshold \\x12, notedGD\\x12\\nIS;T;C : Fraction of the Algorithmic\\nComplexity of solution Sol\\x12\\nTthat is explained by TrainSolopt\\nT;Cand the initial state of the\\nsystemISt=0, i.e. length of the shortest program that, taking as input the initial system plus\\nthe shortest possible program that performs optimally over the situations in curriculum C,\\nproduces a skill program that performs at a skill level of at least \\x12during evaluation, nor-\\nmalized by the length of that skill program. Note that this quantity is between 0 and 1 by\\nconstruction.\\nGD\\x12\\nIS;T;C =H(Sol\\x12\\nTjTrainSolopt\\nT;C;ISt=0)\\nH(Sol\\x12\\nT)\\nIn which we note: ISt=0=SkillProgramGen;ISUpdate;isState t=0\\nDeveloper-aware generalization thus represents the amount of uncertainty about the short-\\nest evaluation-time solution given that you have at your disposal both the initial system\\nand the shortest training-time solution, i.e. the amount of modiÔ¨Åcations you would have to\\nmake to the shortest training-time solution to obtain the evaluation-time solution, provided\\nthat these edits can make use of the contents of the initial system.\\nLikewise, we can deÔ¨Åne the Generalization DifÔ¨Åculty from task T1to taskT2(sufÔ¨Åcient\\ncase) asH(Sol\\x12T2\\nT2jSol\\x12T1\\nT1)=H(Sol\\x12T2\\nT2). We can also extend these deÔ¨Ånitions to a set of\\ntasks (e.g. Generalization DifÔ¨Åculty from a set of practice task to a set of test tasks), which\\ncan be useful to quantify the Generalization DifÔ¨Åculty of an entire test suite. These notions\\nare related to the concept of intrinsic task difÔ¨Åculty (regardless of generalization) deÔ¨Åned\\nin [37] (section 8.6) as the effort necessary to construct a solution.\\n36Next, we can also use Relative Algorithmic Complexity to formally quantify the Priors\\nPIS;T possessed by an intelligent system about a task:\\nPriors of an intelligent system relative to a task Tand a skill threshold \\x12, notedP\\x12\\nIS;T:\\nFraction of the Algorithmic Complexity of the shortest solution of Tof skill threshold \\x12\\nthat is explained by the initial system (at the start of the training phase). This is the length\\n(normalized by H(Sol\\x12\\nT)) of the shortest possible program that, taking as input the initial\\nsystemSkillProgramGen;ISUpdate;isState t=0(notedISt=0), produces the shortest\\nsolution ofTthat performs at a skill level of at least \\x12during evaluation. Note that the\\nintelligent system does not need to be able to produce this speciÔ¨Åc solution. Note that this\\nquantity is between 0 and 1 by construction.\\nP\\x12\\nIS;T=H(Sol\\x12\\nT)\\x00H(Sol\\x12\\nTjISt=0)\\nH(Sol\\x12\\nT)\\n‚ÄúPriors‚Äù thus deÔ¨Åned can be interpreted as a measure of how close from a sufÔ¨Åcient or op-\\ntimal solution the system starts, i.e. the ‚Äúamount of relevant information‚Äù embedded in the\\ninitial system. Note that this is different from the ‚Äúamount of information‚Äù embedded in the\\ninitial system (which would merely be the Algorithmic Complexity of the initial system).\\nAs such, our measure only minimally penalizes large systems that contain prior knowledge\\nthat is irrelevant to the task at hand (the only added cost is due to knowledge indexing and\\nretrieval overhead).\\nFurther, we can use Relative Algorithmic Complexity to deÔ¨Åne the Experience EIS;T;C\\naccumulated by an intelligent system about a task during a curriculum.\\nConsider a single step tduring training:\\n\\x0fAtt, the system receives some new data in the form of the binary strings situation t,\\nresponse t, andfeedback t(althoughresponse tmay be omitted since, being the\\noutput of a skill program previously generated by the IS, it can be assumed to be\\nknown by the ISas soon assituation tis known).\\n\\x0fOnly some of this data is relevant to solving the task (the data may be noisy or other-\\nwise uninformative).\\n\\x0fOnly some of the data contains novel information for the intelligent system (situations\\nand responses may be repetitive, and the intelligent system may be a slow learner that\\nneeds information to be repeated multiple times or presented in multiple ways). Note\\nthat we use the term ‚Äúnovel‚Äù to characterize information that would appear novel to\\nthe system, rather than information that has never appeared before in the curriculum\\n(the difference between the two lies in the system‚Äôs learning efÔ¨Åciency).\\nWe informally deÔ¨Åne the amount of experience accrued at step tas the amount of relevant,\\nnovel information received by the system at t. This corresponds to the amount of potential\\n37uncertainty reduction about the solution that is made available by the task in the current\\nsituation data and feedback data (i.e. how much the IScould reduce its uncertainty about\\nthe solution using the step data if it were optimally intelligent).\\nFormally:\\nExperience accrued at step t, notedE\\x12\\nIS;T;t :\\nE\\x12\\nIS;T;t =H(Sol\\x12\\nTjISt)\\x00H(Sol\\x12\\nTjISt;datat)\\nIn which we note:\\n\\x0fISt=SkillProgramGen;ISUpdate;isState t\\n\\x0fdatat=Situation t;response t;feedback t\\nBy summing over all steps, we obtain the following deÔ¨Ånition of total experience (note that\\nwe normalize by the Algorithmic Complexity of the solution considered, as we did for pri-\\nors):\\nExperience E\\x12\\nIS;T;C over a curriculum C:\\nE\\x12\\nIS;T;C =1\\nH(Sol\\x12\\nT)P\\ntE\\x12\\nIS;T;t\\n‚ÄúExperience‚Äù thus deÔ¨Åned can be interpreted as a measure of the amount of relevant in-\\nformation received by the system about the task over the course of a curriculum, only\\naccounting for novel information at each step.\\nBecause this is different from the ‚Äúamount of information‚Äù contained in the curriculum\\n(i.e. the Algorithmic Complexity of the curriculum), our measure does not penalize systems\\nthat go through noisy curricula.\\nIn addition, because we use an eager sum of relevant and novel information at each step\\ninstead of globally pooling the information content of the curriculum, we penalize learners\\nthat are slower to absorb the relevant information that is presented to them.\\nLastly, because our sum is different from ‚Äúamount of relevant information (novel or not)\\nat each step summed over all steps‚Äù, we do not penalize systems that go through repetitive\\ncurricula. If a fast learner absorbs sufÔ¨Åcient information over the Ô¨Årst ten steps of a Ô¨Åxed\\ncurriculum, but a slow learner needs 90 more steps of the same curriculum to achieve the\\nsame, we will not count as experience for the fast learner the redundant last 90 steps during\\nwhich it did not learn anything, but we will count all 100 steps for the slow learner.\\nDeÔ¨Åning intelligence\\nWe have now established sufÔ¨Åcient context and notations to formally express the intuitive\\ndeÔ¨Ånition of intelligence stated earlier, ‚Äúthe intelligence of a system is a measure of its\\nskill-acquisition efÔ¨Åciency over a scope of tasks, with respect to priors, experience, and\\n38generalization difÔ¨Åculty. ‚Äù\\nWe consider an intelligent system IS. We noteCur\\x12T\\nTthe space of curricula that result in\\nISgenerating a solution of sufÔ¨Åcient skill \\x12Tfor a taskT, andCuropt\\nTthe space of cur-\\nricula that result in ISgenerating its highest-skill solution (solution reaching the system‚Äôs\\npotential\\x12max\\nT;IS). Note that system‚Äôs potential may be lower than the optimal solution for\\nthe task, as the system may not be able to learn to optimally solve the task.\\nTo simplify notations, we will denote \\x12max\\nT;IS as\\x02. We noteAvg the averaging function\\n(used to average over task space). We note PCthe probability of a given curriculum C.\\nWe then deÔ¨Åne the intelligence of I, tied to a scope of tasks scope , as:\\nIntelligence of system ISoverscope (sufÔ¨Åcient case):\\nI\\x12T\\nIS;scope =Avg\\nT2scope\"\\n!T\\x01\\x12T\\x06\\nC2Cur\\x12T\\nT\\x14\\nPC\\x01GD\\x12T\\nIS;T;C\\nP\\x12T\\nIS;T+E\\x12T\\nIS;T;C\\x15#\\nIntelligence of system ISoverscope (optimal case):\\nIopt\\nIS;scope =Avg\\nT2scope\"\\n!T;\\x02\\x01\\x02 \\x06\\nC2Curopt\\nT\\x14\\nPC\\x01GD\\x02\\nIS;T;C\\nP\\x02\\nIS;T+E\\x02\\nIS;T;C\\x15#\\nNote that:\\n\\x0fPIS;T+EIS;T;C (priors plus experience) represents the total exposure of the sys-\\ntem to information about the problem, including the information it starts with at the\\nbeginning of training.\\n\\x0fThe sum over a curriculum subspace, weighted by the probability of each curriculum,\\nrepresents the expected outcome for the system after training. Note that the sum is\\nover a subspace of curricula (curricula that lead to at least a certain skill level), and\\nthus the probabilities would sum to a total lower than one: as such, we are penalizing\\nlearners that only reach sufÔ¨Åcient skill or optimal skill some of the time.\\n\\x0f!T\\x01\\x12Trepresents the subjective value we place on achieving sufÔ¨Åcient skill at T,\\nand!T;\\x02\\x01\\x02represents the subjective value we place on achieving the skill level\\ncorresponding to the system‚Äôs full potential \\x12max\\nT;IS atT.\\n\\x0fSchematically, the contribution of each task is: Expectationh\\nskill\\x01generalization\\npriors +experiencei\\n,\\nwhich is further weighted by a value !which enables us to homogeneously compare\\nskill at different tasks independently of the scale of their respective scoring functions.\\nThus, we equate the intelligence of a system to a measure of the information-efÔ¨Åciency\\nwith which the system acquires its Ô¨Ånal task-speciÔ¨Åc skill (sufÔ¨Åcient skill or highest possi-\\n39ble skill) on average (probabilistic average over all applicable curricula), weighted by the\\ndeveloper-aware generalization difÔ¨Åculty of the task considered (as well as the task value\\n!, which makes skill commensurable across tasks), averaged over all tasks in the scope.\\nOr, in plain English: intelligence is the rate at which a learner turns its experience and\\npriors into new skills at valuable tasks that involve uncertainty and adaptation.\\nNote that our deÔ¨Ånition is not the Ô¨Årst formal deÔ¨Ånition of intelligence based on Algorith-\\nmic Information Theory. We are aware of three other AIT-based deÔ¨Ånitions: the C-Test\\n[40], the AIXI model [43], and the ‚ÄúUniversal Intelligence‚Äù model [54] (closely related to\\nAIXI). It should be immediately clear to a reader familiar with these deÔ¨Ånitions that our\\nown approach represents a very different perspective.\\nWe bring the reader‚Äôs attention to a number of key observations about our formalism (see\\nalso II.2.3):\\n\\x0fA high-intelligence system is one that can generate high-skill solution programs for\\nhigh generalization difÔ¨Åculty tasks (i.e. tasks that feature high uncertainty about the\\nfuture) using little experience and priors, i.e. it is a system capable of making highly\\nefÔ¨Åcient use of all of the information it has at its disposition to cover as much ground\\nas possible in unknown parts of the situation space. Intelligence is, in a way, a con-\\nversion rate between information about part of the situation space, and the ability\\nto perform well over a maximal area of future situation space, which will involve\\nnovelty and uncertainty (Ô¨Ågure 3).\\n\\x0fThe measure of intelligence is tied to a choice of scope (space of tasks and value\\nfunction over tasks). It can also optionally be tied to a choice of sufÔ¨Åcient skill levels\\nacross the tasks in the scope (sufÔ¨Åcient case).\\n\\x0fSkill is not possessed by an intelligent system, it is a property of the output artifact of\\nthe process of intelligence (a skill program). High skill is not high intelligence: these\\nare different concepts altogether.\\n\\x0fIntelligence must involve learning and adaptation, i.e. operationalizing information\\nextracted from experience in order to handle future uncertainty: a system that starts\\nout with the ability to perform well on evaluation situations for a task would have a\\nvery low developer-aware generalization difÔ¨Åculty for this task, and thus would score\\npoorly on our intelligence metric.\\n\\x0fIntelligence is not curve-Ô¨Åtting: a system that merely produces the simplest possible\\nskill program consistent with known data points could only perform well on tasks that\\nfeature zero generalization difÔ¨Åculty, by our deÔ¨Ånition. An intelligent system must\\ngenerate behavioral programs that account for future uncertainty.\\n\\x0fThe measure of intelligence is tied to curriculum optimization: a better curriculum\\nspace will lead to greater realized skill (on average) and to greater expressed intelli-\\ngence (greater skill-acquisition efÔ¨Åciency).\\n40Figure 3: Higher intelligence ‚Äúcovers more ground‚Äù in future situation space using the same\\ninformation.\\nII.2.2 Computation efÔ¨Åciency, time efÔ¨Åciency, energy efÔ¨Åciency, and risk ef-\\nÔ¨Åciency\\nIn the above, we only considered the information-efÔ¨Åciency (prior-efÔ¨Åciency and experience-\\nefÔ¨Åciency with respect to generalization difÔ¨Åculty) of intelligent systems. Indeed, we be-\\nlieve this is the most actionable and relevant angle today to move AI research forward (cf\\nII.2.3). But it isn‚Äôt the only angle one may want to consider. Several alternatives that could\\nbe incorporated into our deÔ¨Ånition in various ways (e.g. as a regularization term) come to\\nmind:\\n\\x0fComputation efÔ¨Åciency of skill programs: for settings in which training data is abun-\\ndant but inference-time computation is expensive, one may want to encourage the\\ngeneration of the skill programs that have minimal computational resource consump-\\ntion.\\n\\x0fComputation efÔ¨Åciency of the intelligent system: for settings in which training-time\\ncomputation is expensive, one may want to expend a minimal amount of computation\\nresources to generate a skill program.\\n\\x0fTime efÔ¨Åciency: in time-constrained settings, one may want to minimize the latency\\nwith which the intelligent system generates skill programs.\\n\\x0fEnergy efÔ¨Åciency: in biological systems in particular, one may want to minimize the\\namount of energy expended in producing a skill program, in running a skill program,\\nor in going through a curriculum.\\n\\x0fRisk efÔ¨Åciency: for settings in which going through a curriculum (i.e. collecting\\nexperience) involves risk for the intelligent system, one might want to encourage safe\\ncurricula at the expense of resource efÔ¨Åciency or information efÔ¨Åciency. Much like\\n41energy efÔ¨Åciency, this is highly relevant to biological systems and natural evolution,\\nin which certain novelty-seeking behaviors that would lead to faster learning may\\nalso be more dangerous.\\nIn fact, one may note that information efÔ¨Åciency acts in many settings as a proxy for\\nenergy efÔ¨Åciency and risk efÔ¨Åciency.\\nWe expect that these alternative ways to quantify efÔ¨Åciency will become relevant in\\nspecialized AI application contexts in the future, and we bring them to the reader‚Äôs attention\\nto encourage others to develop new formal deÔ¨Ånitions of intelligence incorporating them in\\naddition to information efÔ¨Åciency.\\nII.2.3 Practical implications\\nThe deÔ¨Ånitions above provide a formal framework as well as quantitative tools to reason\\nabout the intuitive notions we have been introducing so far, in particular the concepts of\\n‚Äúgeneralization difÔ¨Åculty‚Äù, ‚Äúintelligence as skill-acquisition efÔ¨Åciency‚Äù, and what it means\\nto control for priors and experience when evaluating intelligence, as opposed to looking\\npurely at task-speciÔ¨Åc skill.\\nThe main value of this framework is to provide an actionable perspective shift in how\\nwe understand and evaluate Ô¨Çexible or general artiÔ¨Åcial intelligence. We argue that this\\nperspective shift has the following practical consequences:\\na. Consequences for research directions towards Ô¨Çexible or general AI:\\n\\x0fIt clearly spells out that the process of creating an intelligent system can be ap-\\nproached as an optimization problem, where the objective function would be a com-\\nputable approximation of our quantitative intelligence formula. As pointed out in\\nII.2.2, this objective function could be further reÔ¨Åned by incorporating regularization\\nterms that would take into account alternative forms of efÔ¨Åciency.\\n\\x0fIt encourages a focus on developing broad or general-purpose abilities rather than\\npursuing skill alone, by proposing a target metric that penalises excessive reliance on\\nexperience or priors, and discounting tasks that feature low generalization difÔ¨Åculty.\\n\\x0fIt encourages interest in program synthesis, by suggesting that we stop thinking of\\n‚Äúagents‚Äù as monolithic black boxes that take in sensory input and produce behavior (a\\nvision inherited from Reinforcement Learning [88]): our formalism clearly separates\\nthe part of the system that possesses intelligence (‚Äúintelligent system‚Äù, a program-\\nsynthesis engine) from the part that achieves skill or implements behavior (‚Äúskill\\nprogram‚Äù, the non-intelligent output artifact of the process of intelligence), and places\\nfocus to the former. As we point out throughout this paper, we believe that this\\nconfusion between process and artifact has been an ongoing fundamental issue in the\\nconceptualization of AI.\\n\\x0fIt encourages interest in curriculum development, by leveraging the notion of an ‚Äúop-\\ntimal curriculum‚Äù and drawing attention to the fact that a better curriculum increases\\nthe intelligence manifested by a learning system.\\n42\\x0fIt encourages interest in building systems based on human-like knowledge priors (e.g.\\nCore Knowledge) by drawing attention to the importance of priors in evaluating in-\\ntelligence.\\nb. Consequences for evaluating Ô¨Çexible or general AI systems:\\n\\x0fBy deÔ¨Åning and quantifying generalization difÔ¨Åculty, it offers a way to formally rea-\\nson about what it means to perform ‚Äúlocal generalization‚Äù, ‚Äúbroad generalization‚Äù,\\nand ‚Äúextreme generalization‚Äù (cf. the spectrum of generalization introduced in I.3.2),\\nand to weed out tests that feature zero generalization difÔ¨Åculty.\\n\\x0fIt suggests concrete guidelines for comparing AI and human intelligence: such a\\ncomparison requires starting from a shared scope of tasks and shared priors, and\\nwould seek to compare experience-efÔ¨Åciency in achieving speciÔ¨Åc levels of skill. We\\ndetail this idea in II.3.1.\\n\\x0fIt shows the importance of taking into account generalization difÔ¨Åculty when devel-\\noping a test set to evaluate a task. We detail this idea in II.3.2. This should hopefully\\nlead us to evaluation metrics that are able to discard solutions that rely on shortcuts\\nthat do not generalize (e.g. reliance on local textures as opposed to global semantics\\nin computer vision).\\n\\x0fIt provides a set of practical questions to ask about any intelligent system to rigorously\\ncharacterize it:\\n‚ÄìWhat is its scope?\\n‚ÄìWhat is its ‚Äúpotential‚Äù over this scope (maximum achievable skill)?\\n‚ÄìWhat priors does it possess?\\n‚ÄìWhat is its skill-acquisition efÔ¨Åciency (intelligence)?\\n‚ÄìWhat curricula would maximize its skill or skill-acquisition efÔ¨Åciency?\\nII.3 Evaluating intelligence in this light\\nEarlier in this document, we have detailed how measuring skill alone does not move us\\nforward when it comes to the development of broad abilities, we have suggested that AI\\nevaluation should learn from its more mature sister Ô¨Åeld psychometrics (echoing the thesis\\nof Psychometric AI and Universal Psychometrics), and we have provided a new formalism\\nwith practical implications for AI evaluation, pointing out the importance of the concept\\nof scope, potential, generalization difÔ¨Åculty, experience, and priors. The following section\\nsummarizes key practical conclusions with respect to AI evaluation.\\nII.3.1 Fair comparisons between intelligent systems\\nWe mentioned in II.2.3 that our formalism suggests concrete guidelines for comparing the\\nintelligence of systems of different nature, such as human intelligence and artiÔ¨Åcial in-\\ntelligence. Being able to make such comparisons in a fair and rigorous way is essential to\\n43progress towards human-like general AI. Here we argue how such intelligence comparisons\\nbetween systems entail speciÔ¨Åc requirements with regard to the target systems‚Äô scope, po-\\ntential , and priors . We also detail how such comparisons should proceed given that these\\nrequirements are met.\\nScope and potential requirements. In II.1.2, we argued that intelligence is necessar-\\nily tied to a scope of application, an idea also central to the formalism introduced in II.2.\\nAs such, a comparison scale must be tied to a well-deÔ¨Åned scope of tasks that is shared by\\nthe target systems (all target systems should be able to learn to perform the same tasks).\\nFurther, we must consider that the target systems may have different potential (maxi-\\nmum achievable skill) over their shared scope. An intelligence comparison should focus\\non skill-acquisition efÔ¨Åciency, but skill-acquisition efÔ¨Åciency cannot be meaningfully com-\\npared between systems that arrive at vastly different levels of skills. As such, a comparison\\nscale must be tied to a Ô¨Åxed threshold of skill over the scope of tasks considered . This skill\\nthreshold should be achievable by all target systems.\\nFor instance, comparing a generally-intelligent system to human intelligence would\\nonly make sense if the scope of tasks that can be learned by the system is the same scope\\nof tasks that can be learned by a typical human, and the comparison should focus on the\\nefÔ¨Åciency with which the system achieves the same level of skill as a human expert. Com-\\nparing maximum realized skill does not constitute an intelligence comparison.\\nPrior knowledge requirements. Since the formalism of II.2 summarizes priors into\\na single scalar score, which is homogeneous to the score used to quantify experience, it is\\nnot strictly necessary for the two systems being compared to share the same priors. For\\ninstance, if two systems achieve the same skill using the same amount of experience (the\\nexact nature of this experience, determined by the curriculum used, may differ), the system\\nthat has the least amount of prior knowledge would be considered more intelligent.\\nHowever, it would be generally impractical to fully quantify prior knowledge. As such,\\nwe recommend only comparing the intelligence of systems that assume a sufÔ¨Åciently similar\\nset of priors . This implies that any measure of intelligence should explicitly and exhaus-\\ntively list the priors it assumes, an idea we detail below, in II.3.2. Further, this implies that\\nsystems that aim at implementing human-like general intelligence should leverage Core\\nKnowledge priors.\\nIf the above conditions are met (shared scope, well-deÔ¨Åned skill threshold over scope,\\nand comparable knowledge priors), then a fair intelligence comparison would then consist\\nof contrasting the skill-acquisition efÔ¨Åciency proÔ¨Åle of the target systems. The more in-\\ntelligent system would be the one that uses the least amount of experience to arrive at the\\ndesired skill threshold in the average case. Alternatively, computation efÔ¨Åciency, energy\\nefÔ¨Åciency, and risk efÔ¨Åciency may also be considered, as per II.2.2.\\n44II.3.2 What to expect of an ideal intelligence benchmark\\nThe recommendations below synthesizes the conclusions of this document with regard to\\nthe properties that a candidate benchmark of human-like general intelligence should pos-\\nsess.\\n\\x0fIt should describe its scope of application and its own predictiveness with regard to\\nthis scope (i.e. it should establish validity ). In practice, this would be achieved by\\nempirically determining the statistical relationship between success on the benchmark\\nand success on a range of real-world tasks.\\n\\x0fIt should be reliable (i.e. reproducible ). If an evaluation session includes stochastic\\nelements, sampling different values for these elements should not meaningfully af-\\nfect the results. Different researchers independently evaluating the same system or\\napproach using the benchmark should arrive at the same conclusions.\\n\\x0fIt should set out to measure broad abilities and developer-aware generalization:\\n‚ÄìIt should not be solely measuring skill or potential (maximum achievable skill).\\n‚ÄìIt should not feature in its evaluation set any tasks that are known in advance,\\neither to the test-taking system itself or to the developers of the system (cf.\\ndeveloper-aware generalization as deÔ¨Åned in I.3.2).\\n‚ÄìIt should seek to quantify the generalization difÔ¨Åculty it measures (cf. formal\\ndeÔ¨Ånition from II.2), or at least provide qualitative guidelines with regard to its\\ngeneralization difÔ¨Åculty: it should at least be made clear whether the benchmark\\nseeks to measure local generalization (robustness), broad generalization (Ô¨Çexi-\\nbility), or extreme generalization (general intelligence), as deÔ¨Åned in I.3.2. Tak-\\ning into account generalization difÔ¨Åculty minimizes the possibility that a given\\nbenchmark could be ‚Äúhacked‚Äù by solvers that take undesired shortcuts that by-\\npass broad abilities (e.g. leveraging surface textures instead of semantic content\\nin image recognition).\\n\\x0fIt should control for the amount of experience leveraged by test-taking systems dur-\\ning training. It should not be possible to ‚Äúbuy‚Äù performance on the benchmark by\\nsampling unlimited training data. The benchmark should avoid tasks for which new\\ndata can be generated at will. It should be, in effect, a game for which it is not possible\\nto practice in advance of the evaluation session.\\n\\x0fIt should explicitly and exhaustively describe the set of priors it assumes. Any task\\nis going to involve priors, but in many tasks used for AI evaluation today, priors\\nstay implicit, and the existence of implicit hidden priors may often give an unfair\\nadvantage to either humans or machines.\\n\\x0fIt should work for both humans and machines, fairly, by only assuming the same\\npriors as possessed by humans (e.g. Core Knowledge) and only requiring a human-\\nsized amount of practice time or training data.\\nThese recommendations for general AI evaluation wouldn‚Äôt be complete without a con-\\ncrete effort to implement them. In part III, we present our initial attempt.\\n45III A benchmark proposal: the ARC dataset\\nIn this last part, we introduce the Abstraction and Reasoning Corpus (ARC), a dataset\\nintended to serve as a benchmark for the kind of general intelligence deÔ¨Åned in II.2. ARC\\nis designed to incorporate as many of the recommendations of II.3 as possible.\\nIII.1 Description and goals\\nIII.1.1 What is ARC?\\nARC can be seen as a general artiÔ¨Åcial intelligence benchmark, as a program synthesis\\nbenchmark, or as a psychometric intelligence test. It is targeted at both humans and artiÔ¨Å-\\ncially intelligent systems that aim at emulating a human-like form of general Ô¨Çuid intelli-\\ngence. It is somewhat similar in format to Raven‚Äôs Progressive Matrices [47], a classic IQ\\ntest format going back to the 1930s.\\nARC has the following top-level goals:\\n\\x0fStay close in format to psychometric intelligence tests (while addressing issues found\\nin previous uses of such tests for AI evaluation, as detailed in III.1.3), so as to be\\napproachable by both humans and machines; in particular it should be solvable by\\nhumans without any speciÔ¨Åc practice or training.\\n\\x0fFocus on measuring developer-aware generalization, rather than task-speciÔ¨Åc skill, by\\nonly featuring novel tasks in the evaluation set (assumed unknown to the developer\\nof a test-taker).\\n\\x0fFocus on measuring a qualitatively ‚Äúbroad‚Äù form of generalization (cf. I.3.2), by\\nfeaturing highly abstract tasks that must be understood by a test-taker using very few\\nexamples.\\n\\x0fQuantitatively control for experience by only providing a Ô¨Åxed amount of training\\ndata for each task and only featuring tasks that do not lend themselves well to artiÔ¨Å-\\ncially generating new data.\\n\\x0fExplicitly describe the complete set of priors it assumes (listed in III.1.2), and en-\\nable a fair general intelligence comparison between humans and machines by only\\nrequiring priors close to innate human prior knowledge (cf. II.3.2).\\nARC comprises a training set and an evaluation set. The training set features 400 tasks,\\nwhile the evaluation set features 600 tasks. The evaluation set is further split into a public\\nevaluation set (400 tasks) and a private evaluation set (200 tasks). All tasks are unique, and\\nthe set of test tasks and the set of training tasks are disjoint. The task data is available at\\ngithub.com/fchollet/ARC.\\nEach task consists of a small number of demonstration examples (3.3 on average), and\\na small number of test examples (generally 1, although it may be 2 or 3 in rare cases). Each\\nexample consists of an ‚Äúinput grid‚Äù and an ‚Äúoutput grid‚Äù. Each ‚Äúgrid‚Äù is a literal grid of\\n46symbols (each symbol is typically visualized via a unique color), as seen in Ô¨Ågure 4. There\\nare 10 unique symbols (or colors). A grid can be any height or width between 1x1 and\\n30x30, inclusive (the median height is 9 and the median width is 10).\\nWhen solving an evaluation task, a test-taker has access to the training examples for the\\ntask (both the input and output grids), as well as the input grid of the test examples for the\\ntask. The test-taker must construct on its own the output grid corresponding to the input grid\\nof each test example. ‚ÄúConstructing the output grid‚Äù is done entirely from scratch, meaning\\nthat the test-taker must decide what the height and width of the output grid should be, what\\nsymbols it should place on the grid, and where. The task is successfully solved if the test-\\ntaker can produce the exact correct answer on all test examples for the task (binary measure\\nof success). For each test example in a task, the test-taker (either human or machine) is\\nallowed 3 trials10. The only feedback received after a trial is binary (correct answer or\\nincorrect answer). The score of an intelligent system on ARC is the fraction of tasks in\\nthe evaluation set that it can successfully solve. Crucially, it is assumed that neither the\\ntest-taker nor its developer would have had any prior information about the tasks featured\\nin the evaluation set: ARC seeks to measure ‚Äúdeveloper aware generalization‚Äù as deÔ¨Åned\\nin I.3.2. The existence of a private evaluation set enables us to strictly enforce this in the\\nsetting of a public competition.\\nA test-taker is also assumed to have access to the entirety of the training set, although\\nthe training data isn‚Äôt strictly necessary in order to be successful on the validation set, as all\\ntasks are unique and do not assume any knowledge other than the priors described in III.1.2.\\nA typical human can solve most of the ARC evaluation set without any previous training.\\nAs such, the purpose of the training set primarily to serve as a development validation set\\nfor AI system developers, or as a mock test for human test-takers. It could also be used as\\na way to familiarize an algorithm with the content of Core Knowledge priors. We do not\\nexpect that practice on the training set would increase human performance on the test set\\n(albeit this hypothesis would need to be concretely tested).\\nIII.1.2 Core Knowledge priors\\nAny test of intelligence is going to involve prior knowledge. ARC seeks to control for its\\nown assumptions by explicitly listing the priors it assumes, and by avoiding reliance on\\nany information that isn‚Äôt part of these priors (e.g. acquired knowledge such as language).\\nThe ARC priors are designed to be as close as possible to Core Knowledge priors, so as\\nto provide a fair ground for comparing human intelligence and artiÔ¨Åcial intelligence, as per\\nour recommendations in II.3.1.\\nThe Core Knowledge priors assumed by ARC are as follows:\\n10We consider 3 trials to be enough to account for cases in which the task may be slightly ambiguous or in which\\nthe test-taker may commit mechanical errors when inputting an answer grid.\\n47Figure 4: A task where the implicit goal is to complete a symmetrical pattern. The nature of the\\ntask is speciÔ¨Åed by three input/output examples. The test-taker must generate the output grid\\ncorresponding to the input grid of the test input (bottom right).\\na. Objectness priors:\\nObject cohesion: Ability to parse grids into ‚Äúobjects‚Äù based on continuity criteria including\\ncolor continuity or spatial contiguity (Ô¨Ågure 5), ability to parse grids into zones, partitions.\\nObject persistence: Objects are assumed to persist despite the presence of noise (Ô¨Ågure\\n6) or occlusion by other objects. In many cases (but not all) objects from the input persist\\non the output grid, often in a transformed form. Common geometric transformations of\\nobjects are covered in category 4, ‚Äúbasic geometry and topology priors‚Äù.\\nFigure 5: Left, objects deÔ¨Åned by spatial contiguity. Right, objects deÔ¨Åned by color continuity.\\n48Figure 6: A denoising task.\\nObject inÔ¨Çuence via contact: Many tasks feature physical contact between objects (e.g.\\none object being translated until it is in contact with another (Ô¨Ågure 7), or a line ‚Äúgrowing‚Äù\\nuntil it ‚Äúrebounds‚Äù against another object (Ô¨Ågure 8).\\nFigure 7: The red object ‚Äúmoves‚Äù towards the blue object until ‚Äúcontact‚Äù.\\nb. Goal-directedness prior:\\nWhile ARC does not feature the concept of time, many of the input/output grids can be\\neffectively modeled by humans as being the starting and end states of a process that in-\\nvolves intentionality (e.g. Ô¨Ågure 9). As such, the goal-directedness prior may not be strictly\\nnecessary to solve ARC, but it is likely to be useful.\\nc. Numbers and Counting priors:\\nMany ARC tasks involve counting or sorting objects (e.g. sorting by size), comparing\\nnumbers (e.g. which shape or symbol appears the most (e.g. Ô¨Ågure 10)? The least? The\\nsame number of times? Which is the largest object? The smallest? Which objects are the\\nsame size?), or repeating a pattern for a Ô¨Åxed number of time. The notions of addition and\\nsubtraction are also featured (as they are part of the Core Knowledge number system as per\\n[85]). All quantities featured in ARC are smaller than approximately 10.\\n49Figure 8: A task where the implicit goal is to extrapolate a diagonal line that ‚Äúrebounds‚Äù upon\\ncontact with a red obstacle.\\nd. Basic Geometry and Topology priors:\\nARC tasks feature a range of elementary geometry and topology concepts, in particular:\\n\\x0fLines, rectangular shapes (regular shapes are more likely to appear than complex\\nshapes).\\n\\x0fSymmetries (e.g. Ô¨Ågure 11), rotations, translations.\\n\\x0fShape upscaling or downscaling, elastic distortions.\\n\\x0fContaining / being contained / being inside or outside of a perimeter.\\n\\x0fDrawing lines, connecting points, orthogonal projections.\\n\\x0fCopying, repeating objects.\\nIII.1.3 Key differences with psychometric intelligence tests\\nWe have pointed out in I.3.4 the reasons why using existing psychometric intelligence tests\\n(or ‚ÄúIQ tests‚Äù) does not constitute a sound basis for AI evaluation. Albeit ARC stays delib-\\nerately close in format to traditional IQ tests (as well as related efforts such as Hern ¬¥andez-\\nOrallo‚Äôs C-Test [40]), its design differs from them in fundamental ways. We argue that\\nthese differences address the shortcomings of psychometric intelligence tests in the context\\nof AI evaluation. In particular:\\n\\x0fUnlike some psychometric intelligence tests, ARC is not interested in assessing crys-\\ntallized intelligence or crystallized cognitive abilities. ARC only assesses a general\\nform of Ô¨Çuid intelligence, with a focus on reasoning and abstraction. ARC does\\nnot involve language, pictures of real-world objects, or real-world common sense.\\nARC seeks to only involve knowledge that stays close to Core Knowledge priors,\\n50Figure 9: A task that combines the concepts of ‚Äúline extrapolation‚Äù, ‚Äúturning on obstacle‚Äù, and\\n‚ÄúefÔ¨Åciently reaching a goal‚Äù (the actual task has more demonstration pairs than these three).\\nand avoids knowledge that would have to be acquired by humans via task-speciÔ¨Åc\\npractice.\\n\\x0fThe tasks featured in the ARC evaluation set are unique and meant to be unknown to\\ndevelopers of test-taking systems (as ARC seeks to assess developer-aware general-\\nization). This prevents developers from solving the tasks themselves and hard-coding\\ntheir solution in program form. This can be strictly enforced in competition settings\\nvia the existence of a private evaluation set.\\n\\x0fARC has greater task diversity than typical psychometric intelligence tests (hundreds\\nof unique tasks with limited overlap between tasks), which reduces the likelihood that\\nhard-coding task-speciÔ¨Åc solutions would represent a practical shortcut for develop-\\ners, even for the public evaluation set.\\n\\x0fUnlike tasks from the C-Test [40], ARC tasks are in majority not programmatically\\ngenerated. We perceive programmatic generation from a static ‚Äúmaster‚Äù program\\nas a weakness, as it implies that merely reverse-engineering the generative program\\nshared across tasks (presumably a simple program, since it had to be written down by\\nthe test developer) would be sufÔ¨Åcient to fully solve all tasks. Manual task generation\\nincreases task diversity and reduces the risk of existence of an unforeseen shortcut\\nthat could be used to by-pass the need for broad abilities in solving the test.\\nIII.1.4 What a solution to ARC may look like, and what it would imply for\\nAI applications\\nWe have found ARC to be fully solvable by humans. While many ARC test tasks are\\nintellectually challenging, human test-takers appear to be able to solve the majority of tasks\\non their Ô¨Årst try without any practice or verbal explanations. Each task included in ARC\\nhas been successfully solved by at least one member of a group of three high-IQ humans\\n51Figure 10: A task where the implicit goal is to count unique objects and select the object that\\nappears the most times (the actual task has more demonstration pairs than these three).\\nFigure 11: Drawing the symmetrized version of a shape around a marker. Many tasks involve\\nsome form of symmetry.\\n(who did not communicate with each other), which demonstrates task feasibility. In the\\nfuture, we hope to be able to further investigate human performance on ARC by gathering\\na statistically signiÔ¨Åcant amount of human testing data, in particular with regard to the\\nrelationship between CHC cognitive abilities and ARC performance.\\nCrucially, to the best of our knowledge, ARC does not appear to be approachable by\\nany existing machine learning technique (including Deep Learning), due to its focus on\\nbroad generalization and few-shot learning, as well as the fact that the evaluation set only\\nfeatures tasks that do not appear in the training set.\\nFor a researcher setting out to solve it, ARC is perhaps best understood as a program\\nsynthesis benchmark. Program synthesis [31, 32] is a subÔ¨Åeld of AI interested in the gener-\\nation of programs that satisfy a high-level speciÔ¨Åcation, often provided in the form of pairs\\nof example inputs and outputs for the program ‚Äì which is exactly the ARC format.\\nA hypothetical ARC solver may take the form of a program synthesis engine that uses\\nthe demonstration examples of a task to generate candidates that transform input grids into\\ncorresponding output grids. Schematically:\\n52\\x0fStart by developing a domain-speciÔ¨Åc language (DSL) capable of expressing all pos-\\nsible solution programs for any ARC task. Since the exact set of ARC tasks is pur-\\nposely not formally deÔ¨Ånable, this may be challenging (the space of tasks is deÔ¨Åned\\nasanything expressible in terms of ARC pairs that would only involve Core Knowl-\\nedge). It would require harding-coding the Core Knowledge priors from III.1.2 in a\\nsufÔ¨Åciently abstract and combinable program form, to serve as basis functions for a\\nkind of ‚Äúhuman-like reasoning DSL‚Äù. We believe that solving this speciÔ¨Åc subprob-\\nlem is critical to general AI progress.\\n\\x0fGiven a task, use the DSL to generate a set of candidate programs that turn the in-\\nputs grids into the corresponding output grids. This step would reuse and recombine\\nsubprograms that previously proved useful in other ARC tasks.\\n\\x0fSelect top candidates among these programs based on a criterion such as program\\nsimplicity or program likelihood (such a criterion may be trained on solution pro-\\ngrams previously generated using the ARC training set). Note that we do not expect\\nthat merely selecting the simplest possible program that works on training pairs will\\ngeneralize well to test pairs (cf. our deÔ¨Ånition of generalization difÔ¨Åculty from II.2).\\n\\x0fUse the top three candidates to generate output grids for the test examples.\\nWe posit that the existence of a human-level ARC solver would represent the ability to\\nprogram an AI from demonstrations alone (only requiring a handful of demonstrations to\\nspecify a complex task) to do a wide range of human-relatable tasks of a kind that would\\nnormally require human-level, human-like Ô¨Çuid intelligence. As supporting evidence, we\\nnote that human performance on psychometric intelligence tests (which are similar to ARC)\\nis predictive of success across all human cognitive tasks. Further, we posit that, since an\\nARC solver and human intelligence would both be founded on the same knowledge priors,\\nthe scope of application of an ARC solver would be close to that of human cognition,\\nmaking such a solver both practically valuable (i.e. it could solve useful, human-relevant\\nproblems) and easy to interact with (i.e. it would readily understand human demonstrations\\nand would produce behavior that is in line with human expectations).\\nOur claims are highly speculative and may well prove fully incorrect, much like Newell‚Äôs\\n1973 hopes that progress on chess playing would translate into meaningful progress on\\nachieving a range of broad cognitive abilities ‚Äì especially if ARC turns out to feature un-\\nforeseen vulnerabilities to unintelligent shortcuts. We expect our claims to be validated or\\ninvalidated in the near future once we make sufÔ¨Åcient progress on solving ARC.\\nIII.2 Weaknesses and future reÔ¨Ånements\\nIt is important to note that ARC is a work in progress, not a deÔ¨Ånitive solution; it does not\\nÔ¨Åt all of the requirements listed in II.3.2, and it features a number of key weaknesses:\\n\\x0fGeneralization is not quantiÔ¨Åed. While ARC is explicitly designed to measure\\n‚Äúbroad generalization‚Äù as opposed to ‚Äúlocal generalization‚Äù or ‚Äúextreme generaliza-\\ntion‚Äù, we do not offer a quantitative measure of the generalization of the evaluation\\n53set given the test set, or the generalization difÔ¨Åculty of each task (considered inde-\\npendently). We plan on conducting future work to empirically address this issue by\\nusing human performance on a task (considered over many human subjects) to esti-\\nmate the generalization difÔ¨Åculty it represents. We would be particularly interested\\nin attempting to correlate human performance on a task with an approximation of\\nthe AIT measure of generalization difÔ¨Åculty proposed in II.2 (such an approximation\\nshould become available as we make progress on ARC solver programs). Finding\\nhigh correlation, or a lack of correlation, would provide a degree of validation or\\ninvalidation of our formal measure.\\n\\x0fTest validity is not established. Validity represents the predictiveness of test perfor-\\nmance with regard to performance on other non-test activities. The validity of ARC\\nshould be investigated via large-sample size statistical studies on humans, following\\nthe process established by psychometrics. Further, when AI ARC solvers become\\na reality, we will also be able to study how well ARC performance translates into\\nreal-world usefulness across a range of tasks.\\n\\x0fDataset size and diversity may be limited. ARC only features 1,000 tasks in total,\\nand there may be some amount of conceptual overlap across many tasks. This could\\nmake ARC potentially vulnerable to shortcut strategies that could solve the tasks\\nwithout featuring intelligence. We plan on running public AI competitions (using the\\nprivate evaluation set) as a way to crowd-source attempts to produce such shortcuts (if\\na shortcut exists, it should arise quickly in a competition setting). Further, to mitigate\\npotential vulnerability against such shortcuts, we intend to keep adding new tasks to\\nARC in the future, possibly by crowd-sourcing them.\\n\\x0fThe evaluation format is overly close-ended and binary. The score of a test-taker\\non an evaluation task is either 0 or 1, which lacks granularity. Further, real-world\\nproblem-solving often takes the form of an interactive process where hypotheses are\\nformulated by the test-taker then empirically tested, iteratively. In ARC, this approach\\nis possible to an extent since the test-taker is allowed 3 trials for each test example\\nin a task. However, this format remains overly limiting. A better approach may be\\nlet the test taker dynamically interact with an example generator for the task: the test\\ntaker would be able to ask for a new test input at will, would propose a solution for\\nthe test input, and would receive feedback on their solution, repeatedly, until the test-\\ntaker is reliably able to produce the correct answer. The test-taker‚Äôs score on the task\\nwould then be a measure of the amount of feedback it required until it became able\\nto reliably generate the correct solution for any new input. This represents a more\\ndirect measure of intelligence as formally deÔ¨Åned in II.2, where the input generator\\nis in control of the curriculum.\\n\\x0fCore Knowledge priors may not be well understood and may not be well cap-\\ntured in ARC. Central to ARC is the notion that it only relies on innate human prior\\nknowledge and does not feature signiÔ¨Åcant amounts of acquired knowledge. How-\\never, the exact nature of innate human prior knowledge is still an open problem, and\\nwhether these priors are correctly captured in ARC is unclear.\\n54III.3 Possible alternatives\\nARC is merely one attempt to create a human-like general intelligence benchmark that\\nembodies as many of the guidelines listed in II.3 as possible. While ARC stays very close\\nto the format of psychometric intelligence tests, many other possible approaches could be\\nexplored. In this section, we offer some suggestions for alternatives.\\nIII.3.1 Repurposing skill benchmarks to measure broad generalization\\nWe noted in I.3.5 the ongoing fascination of the AI research community in developing\\nsystems that surpass human skill at board games and video games. We propose repurposing\\nsuch tests of skills into tests of intelligence.\\nConsider an AI developer interested in solving game X. While the AI would be trained\\non instances of X, an evaluation arbiter would create multiple variants of X(X1,X2,\\nXn). These alternative games would be designed to represent a meaningful amount of\\ngeneralization difÔ¨Åculty over X(as deÔ¨Åned in II.2): the simplest game-playing program\\nthat is optimal on instances of X(e.g. game levels of X) would not be optimal on Xi.\\nAs such, these alternative games would not be mere ‚Äúnew levels‚Äù of X, but would feature\\nrelated-yet-novel gameplay, so as to measure broad generalization as opposed to local gen-\\neralization. These alternative games would stay unknown to the AI developers, so as to\\nmeasure developer-aware generalization. This proposed setup is thus markedly different\\nfrom e.g. CoinRun [17] or Obstacle Tower [49], where the evaluation environments are not\\nalternative games, but only levels of the same game (local generalization, or generalization\\nto known unknowns), randomly sampled from a level generator which is known in advance\\nto the AI developers (no evaluation of developer-aware generalization).\\nThe AI trained on X, once ready, would then be tasked with learning to solve X1,\\nXn. Its evaluation score would then be a measure of the amount of experience it required\\non each alternative game in order to reach a speciÔ¨Åc threshold of skill, modulated by the\\namount of generalization difÔ¨Åculty represented by each alternative game. A measure of the\\ngeneral intelligence of such an AI would then be an average of these evaluation scores over\\na large number of different source games X.\\nFor instance, consider the game DotA2: an AI trained on DotA2 may be evaluated by\\nmeasuring the efÔ¨Åciency with which it can learn to play new games from the same genre,\\nsuch as League of Legends or Heroes of the Storm. As an even simpler (but weaker)\\nalternative, an AI trained on 16 speciÔ¨Åc DotA2 characters may be evaluated by measuring\\nthe efÔ¨Åciency with which it can learn to master a set of brand new characters it would not\\nhave played before ‚Äì for example, a strong human DotA2 player can play at a high level\\nwith a new character upon Ô¨Årst try.\\nIII.3.2 Open-ended adversarial or collaborative approaches\\nWe have pointed out in III.2 some of the key limitations of having to craft evaluation tasks\\nmanually: it is a labor-intensive process that makes it difÔ¨Åcult to formally control for gen-\\neralization difÔ¨Åculty, that could potentially result in a low-diversity set of tasks, and that\\n55is not easily scalable (although crowd-sourcing tasks may partially address this problem).\\nThe diversity and scalablility points are especially critical given that we need a constant\\nsupply of substantially new tasks in order to guarantee that the benchmark is measuring\\ndeveloper-aware generalization.\\nA solution may be to instead programmatically generate new tasks. We noted in III.1.3\\nthat programmatic generation from a static ‚Äúmaster‚Äù program is not desirable, as it places a\\nceiling on the diversity and complexity of the set of tasks that can be generated, and it offers\\na potential avenue to ‚Äúcheat‚Äù on the benchmark by reverse-engineering the master program.\\nWe propose instead to generate tasks via an ever-learning program called a ‚Äúteacher‚Äù pro-\\ngram, interacting in a loop with test-taking systems, called ‚Äústudent‚Äù programs (Ô¨Ågure 12).\\nThe teacher program would optimize task generation for novelty and interestingness for a\\ngiven student (tasks should be new and challenging, while still being solvable by the stu-\\ndent), while students would evolve to learn to solve increasingly difÔ¨Åcult tasks. This setup\\nis also favorable to curriculum optimization, as the teacher program may be conÔ¨Ågured to\\nseek to optimize the learning efÔ¨Åciency of its students. This idea is similar to the ‚Äúanytime\\nintelligence test‚Äù proposed in [38] and to the POET system proposed in [96].\\nIn order to make sure that the space of generated tasks retains sufÔ¨Åcient complexity and\\nnovelty over time, the teacher program should draw information from an external source\\n(assumed to feature incompressible complexity), such as the real world. This external\\nsource of complexity makes the setup truly open-ended. A teacher program that generates\\nnovel tasks that partially emulate human-relevant tasks would have the added advantage\\nthat it would guide the resulting student programs towards a form of intelligence that could\\ntransfer to real-world human-relevant problems.\\nFigure 12: Teacher-student learning and evaluation system.\\nTaking stock\\nThe study of general artiÔ¨Åcial intelligence is a Ô¨Åeld still in its infancy, and we do not wish to\\nconvey the impression that we have provided a deÔ¨Ånitive solution to the problem of charac-\\nterizing and measuring the intelligence held by an AI system. Rather, we have introduced\\na new perspective on deÔ¨Åning and evaluating intelligence, structured around the following\\nideas:\\n56\\x0fIntelligence is the efÔ¨Åciency with which a learning system turns experience and priors\\ninto skill at previously unknown tasks.\\n\\x0fAs such, a measure of intelligence must account for priors, experience, and general-\\nization difÔ¨Åculty.\\n\\x0fAll intelligence is relative to a scope of application. Two intelligent systems may only\\nbe meaningfully compared within a shared scope and if they share similar priors.\\n\\x0fAs such, general AI should be benchmarked against human intelligence and should\\nbe founded on a similar set of knowledge priors (e.g. Core Knowledge).\\nWe also have provided a new formalism based on Algorithmic Information Theory (cf.\\nII.2) to rigorously and quantitatively reason about these ideas, as well as a set of concrete\\nguidelines to follow for developing a benchmark of general intelligence (cf. II.3.1 and\\nII.3.2).\\nOur deÔ¨Ånition, formal framework, and evaluation guidelines, which do notcapture all\\nfacets of intelligence, were developed to be actionable, explanatory, and quantiÔ¨Åable, rather\\nthan being descriptive, exhaustive, or consensual. They are not meant to invalidate other\\nperspectives on intelligence, rather, they are meant to serve as a useful objective function to\\nguide research on broad AI and general AI, as outlined in II.2.3. Our hope is for some part\\nof the AI community interested in general AI to break out of a longstanding and ongoing\\ntrend of seeking to achieve raw skill at challenging tasks, given unlimited experience and\\nunlimited prior knowledge.\\nTo ground our ideas and enable others to build upon them, we are also providing an\\nactual benchmark, the Abstraction and Reasoning Corpus, or ARC:\\n\\x0fARC takes the position that intelligence testing should control for scope, priors, and\\nexperience: every test task should be novel (measuring the ability to understand a\\nnew task, rather than skill) and should assume an explicit set of priors shared by all\\ntest-takers.\\n\\x0fARC explicitly assumes the same Core Knowledge priors innately possessed by hu-\\nmans.\\n\\x0fARC can be fully solved by humans, but cannot be meaningfully approached by\\ncurrent machine learning techniques, including Deep Learning.\\n\\x0fARC may offer an interesting playground for AI researchers who are interested in\\ndeveloping algorithms capable of human-like broad generalization. It could also offer\\na way to compare human intelligence and machine intelligence, as we assume the\\nsame priors.\\nImportantly, ARC is still a work in progress, with known weaknesses listed in III.2. We\\nplan on further reÔ¨Åning the dataset in the future, both as a playground for research and as a\\njoint benchmark for machine intelligence and human intelligence.\\nThe measure of the success of our message will be its ability to divert the attention\\nof some part of the community interested in general AI, away from surpassing humans at\\n57tests of skill, towards investigating the development of human-like broad cognitive abilities,\\nthrough the lens of program synthesis, Core Knowledge priors, curriculum optimization,\\ninformation efÔ¨Åciency, and achieving extreme generalization through strong abstraction.\\nReferences\\n[1] Sam S Adams, Guruduth Banavar, and Murray Campbell. I-athlon: Towards a mul-\\ntidimensional turing test. AI Magazine , (1):78‚Äì84, 2016.\\n[2] John R. Anderson and Christian Lebiere. The newell test for a theory of cognition.\\nBehavioral and Brain Sciences , pages 587‚Äì601, 2003.\\n[3] Aristotle. De Anima . c. 350 BC.\\n[4] Minoru Asada et al. Cognitive developmental robotics: A survey. IEEE Transactions\\non Autonomous Mental Development , pages 12‚Äì34, 2009.\\n[5] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learn-\\ning to drive by imitating the best and synthesizing the worst. arXiv preprint\\narXiv:1812.03079 , 2018.\\n[6] Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade\\nlearning environment: An evaluation platform for general agents. J. Artif. Int. Res. ,\\n(1):253‚Äì279, May 2013.\\n[7] Benjamin Beyret, Jos Hernndez-Orallo, Lucy Cheke, Marta Halina, Murray Shana-\\nhan, and Matthew Crosby. The animal-ai environment: Training and testing animal-\\nlike artiÔ¨Åcial cognition, 2019.\\n[8] Alfred Binet and Thodore Simon. Mthodes nouvelles pour le diagnostic du niveau\\nintellectuel des anormaux. L‚Äôanne psychologique , pages 191‚Äì244, 1904.\\n[9] Selmer Bringsjord and Bettina Schimanski. What is artiÔ¨Åcial intelligence? psycho-\\nmetric ai as an answer. In Proceedings of the 18th International Joint Conference on\\nArtiÔ¨Åcial Intelligence , IJCAI‚Äô03, pages 887‚Äì893, San Francisco, CA, USA, 2003.\\nMorgan Kaufmann Publishers Inc.\\n[10] Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee.\\nSample-efÔ¨Åcient reinforcement learning with stochastic ensemble value expansion,\\n2018.\\n[11] Martin Buehler, Karl Iagnemma, and Sanjiv Singh. The 2005 DARPA Grand Chal-\\nlenge: The Great Robot Race . Springer Publishing Company, Incorporated, 1st\\nedition, 2007.\\n[12] Murray Campbell, A. Joseph Hoane, Jr., and Feng-hsiung Hsu. Deep blue. Artif.\\nIntell. , (1-2):57‚Äì83, 2002.\\n[13] Raymond B. Cattell. Abilities: Their structure, growth, and action. 1971.\\n[14] G. Chaitin. Algorithmic Information Theory . Cambridge University Press, 1987.\\n58[15] Gregory J Chaitin. A theory of program size formally identical to information theory.\\nJournal of the ACM (JACM) , (3):329‚Äì340, 1975.\\n[16] Francois Chollet. Deep Learning with Python . Manning Publications, 2017.\\n[17] Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman.\\nQuantifying generalization in reinforcement learning. CoRR , 2018.\\n[18] Ebinepre A Cocodia. Cultural perceptions of human intelligence. Journal of Intelli-\\ngence , 2(4):180‚Äì196, 2014.\\n[19] L. Cosmides and J. Tooby. Origins of domain speciÔ¨Åcity: the evolution of functional\\norganization. page 85116, 1994.\\n[20] Linda Crocker and James Algina. Introduction to classical and modern test theory.\\nERIC, 1986.\\n[21] Charles Darwin. The Origin of Species . 1859.\\n[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-\\nScale Hierarchical Image Database. In CVPR09 , 2009.\\n[23] D. K. Detterman. A challenge to watson. Intelligence , page 7778, 2011.\\n[24] T.G. Evans. A program for the solution of a class of geometric-analogy intelligence-\\ntest questions. pages 271‚Äì353, 1968.\\n[25] James R Flynn. What is intelligence?: Beyond the Flynn effect . Cambridge Univer-\\nsity Press, 2007.\\n[26] Richard M Friedberg. A learning machine: Part i. IBM Journal of Research and\\nDevelopment , 2(1):2‚Äì13, 1958.\\n[27] Manuela Veloso Gary Marcus, Francesca Rossi. Beyond the Turing Test (workshop) ,\\n2014.\\n[28] B. Goertzel and C. Pennachin, editors. ArtiÔ¨Åcial general intelligence . Springer, New\\nYork, 2007.\\n[29] Bert F Green Jr. Intelligence and computer simulation. Transactions of the New York\\nAcademy of Sciences , 1964.\\n[30] Peter D. Gr ¬®unwald and Paul M. B. Vit ¬¥anyi. Algorithmic information theory. 2008.\\n[31] Sumit Gulwani, Jos ¬¥e Hern ¬¥andez-Orallo, Emanuel Kitzelmann, Stephen H Muggle-\\nton, Ute Schmid, and Benjamin Zorn. Inductive programming meets the real world.\\nCommunications of the ACM , 58(11):90‚Äì99, 2015.\\n[32] Sumit Gulwani, Alex Polozov, and Rishabh Singh. Program Synthesis . 2017.\\n[33] William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noburu\\nKuno, Stephanie Milani, Sharada Prasanna Mohanty, Diego Perez Liebana, Rus-\\nlan Salakhutdinov, Nicholay Topin, Manuela Veloso, and Phillip Wang. The minerl\\ncompetition on sample efÔ¨Åcient reinforcement learning using human priors. CoRR ,\\n2019.\\n59[34] R. Hambleton, H. Swaminathan, and H. Rogers. Fundamentals of Item Response\\nTheory . Sage Publications, Inc., 1991.\\n[35] Islam R. Bachman P. Pineau J. Precup D. Henderson, P. and D. Meger. Deep rein-\\nforcement learning that matters. 2018.\\n[36] Jos ¬¥e Hern ¬¥andez-Orallo. Evaluation in artiÔ¨Åcial intelligence: from task-oriented to\\nability-oriented measurement. ArtiÔ¨Åcial Intelligence Review , pages 397‚Äì447, 2017.\\n[37] Jos ¬¥e Hern ¬¥andez-Orallo. The Measure of All Minds: Evaluating Natural and ArtiÔ¨Åcial\\nIntelligence . Cambridge University Press, 2017.\\n[38] Jos ¬¥e Hern ¬¥andez-Orallo and David L Dowe. Measuring universal intelligence: To-\\nwards an anytime intelligence test. ArtiÔ¨Åcial Intelligence , 174(18):1508‚Äì1539, 2010.\\n[39] Jos ¬¥e Hern ¬¥andez-Orallo, David L. Dowe, and M.Victoria Hern ¬¥andez-Lloreda. Uni-\\nversal psychometrics. Cogn. Syst. Res. , (C):50‚Äì74, March 2014.\\n[40] Jos ¬¥e Hern ¬¥andez-Orallo and Neus Minaya-Collado. A formal deÔ¨Ånition of intelli-\\ngence based on an intensional variant of algorithmic complexity. 1998.\\n[41] G.E. Hinton. How neural networks learn from experience. Mind and brain: Read-\\nings from the ScientiÔ¨Åc American magazine , page 113124, 1993.\\n[42] Thomas Hobbes. Human Nature: or The fundamental Elements of Policie . 1650.\\n[43] Marcus Hutter. Universal artiÔ¨Åcial intelligence: Sequential decisions based on al-\\ngorithmic probability . Springer Science & Business Media, 2004.\\n[44] D.L. Dowe J. Hernndez-Orallo. Iq tests are not for machines, yet. Intelligence , page\\n7781, 2012.\\n[45] Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the\\ngeneralization gap in deep networks with margin distributions. ArXiv , 2018.\\n[46] Jason Jo and Yoshua Bengio. Measuring the tendency of cnns to learn surface sta-\\ntistical regularities. ArXiv , 2017.\\n[47] Raven J. John. Raven Progressive Matrices . Springer, Boston, MA, 2003.\\n[48] Wendy Johnson and Thomas J.Bouchard Jr. The structure of human intelligence: It is\\nverbal, perceptual, and image rotation (vpr), not Ô¨Çuid and crystallized. Intelligence ,\\npages 393‚Äì416, 2005.\\n[49] Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng,\\nHunter Henry, Adam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A\\ngeneralization challenge in vision, control, and planning. Proceedings of the Twenty-\\nEighth International Joint Conference on ArtiÔ¨Åcial Intelligence , Aug 2019.\\n[50] Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Ju-\\nlian Togelius, and Sebastian Risi. Illuminating generalization in deep reinforcement\\nlearning through procedural level generation. arXiv preprint arXiv:1806.10729 ,\\n2018.\\n60[51] Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gersh-\\nman. Building machines that learn and think like people. CoRR , 2016.\\n[52] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature ,\\n(7553):436, 2015.\\n[53] Shane Legg and Marcus Hutter. A collection of deÔ¨Ånitions of intelligence. 2007.\\n[54] Shane Legg and Marcus Hutter. Universal intelligence: A deÔ¨Ånition of machine\\nintelligence. Minds and machines , 17(4):391‚Äì444, 2007.\\n[55] Ming Li, Paul Vit ¬¥anyi, et al. An introduction to Kolmogorov complexity and its\\napplications , volume 3. Springer.\\n[56] John Locke. An Essay Concerning Human Understanding . 1689.\\n[57] James Macgregor and Yun Chu. Human performance on the traveling salesman and\\nrelated problems: A review. The Journal of Problem Solving , 3, 02 2011.\\n[58] James Macgregor and Thomas Ormerod. Human performance on the traveling sales-\\nman problem. Perception & psychophysics , 58:527‚Äì39, 06 1996.\\n[59] Gary Marcus. Deep learning: A critical appraisal. arXiv preprint arXiv:1801.00631 ,\\n2018.\\n[60] John McCarthy. Generality in artiÔ¨Åcial intelligence. Communications of the ACM ,\\n30(12):1030‚Äì1035, 1987.\\n[61] Pamela McCorduck. Machines Who Think: A Personal Inquiry into the History and\\nProspects of ArtiÔ¨Åcial Intelligence . AK Peters Ltd, 2004.\\n[62] Kevin McGrew. The cattell-horn-carroll theory of cognitive abilities: Past, present,\\nand future. Contemporary Intellectual Assessment: Theories, Tests, and Issues , 01\\n2005.\\n[63] Marvin Minsky. Society of mind . Simon and Schuster, 1988.\\n[64] May-Britt Moser, David C Rowland, and Edvard I Moser. Place cells, grid cells, and\\nmemory. Cold Spring Harbor perspectives in biology , 7(2):a021808, 2015.\\n[65] Shane Mueller, Matt Jones, Brandon Minnery, Ph Julia, and M Hiland. The bica cog-\\nnitive decathlon: A test suite for biologically-inspired cognitive agents. Proceedings\\nof the 16th Conference on Behavior Representation in Modeling and Simulation ,\\n2007.\\n[66] A. Newell. You cant play 20 questions with nature and win: Projective comments\\non the papers of this symposium. 1973.\\n[67] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Ex-\\nploring generalization in deep learning. In Advances in Neural Information Process-\\ning Systems , pages 5947‚Äì5956, 2017.\\n[68] Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre\\nSaraiva, Katrina McKinney, Tor Lattimore, Csaba Szepezvari, Satinder Singh, et al.\\nBehaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568 , 2019.\\n61[69] A. E. Howe P. R. Cohen. How evaluation guides ai research: the message still counts\\nmore than the medium. AI Mag , page 35, 1988.\\n[70] Charles Packer, Katelyn Gao, Jernej Kos, Philipp Kr ¬®ahenb ¬®uhl, Vladlen Koltun, and\\nDawn Xiaodong Song. Assessing generalization in deep reinforcement learning.\\nArXiv , 2018.\\n[71] Diego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noboru Sean\\nKuno, Andre Kramer, Sam Devlin, Raluca D. Gaina, and Daniel Ionita. The multi-\\nagent reinforcement learning in malm (marl) competition. Technical report, 2019.\\n[72] Diego Perez-Liebana, Jialin Liu, Ahmed Khalifa, Raluca D Gaina, Julian Togelius,\\nand Simon M Lucas. General video game ai: a multi-track framework for evaluating\\nagents, games and content generation algorithms. arXiv preprint arXiv:1802.10363 ,\\n2018.\\n[73] Joelle Pineau. Reproducible, Reusable, and Robust Reinforcement Learning , 2018.\\nNeural Information Processing Systems.\\n[74] S. Pinker. The blank slate: The modern denial of human nature . Viking, New York,\\n2002.\\n[75] David M. W. Powers. The total Turing test and the loebner prize. In New Methods\\nin Language Processing and Computational Natural Language Learning , 1998.\\n[76] Lowrey K. Todorov E. V . Rajeswaran, A. and S. M. Kakade. Towards generalization\\nand simplicity in continuous control. 2017.\\n[77] Fred Reed. Promise of AI not so bright , 2006.\\n[78] Jean-Jacques Rousseau. Emile, or On Education . 1762.\\n[79] & McClelland J.L. Rumelhart, D.E. Distributed memory and the representation of\\ngeneral and speciÔ¨Åc information. Journal of Experimental Psychology , page 159188,\\n1985.\\n[80] P. Sanghi and D. L. Dowe. A computer program capable of passing iq tests. page\\n570575, 2003.\\n[81] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew\\nLai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel,\\net al. Mastering chess and shogi by self-play with a general reinforcement learning\\nalgorithm. arXiv preprint arXiv:1712.01815 , 2017.\\n[82] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja\\nHuang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,\\net al. Mastering the game of go without human knowledge. Nature , 550(7676):354,\\n2017.\\n[83] C. E. Spearman. ‚Äògeneral intelligence‚Äô, objectively determined and measured. Amer-\\nican Journal of Psychology , page 201293, 1904.\\n[84] C. E. Spearman. The Abilities of Man . Macmillan, London, 1927.\\n62[85] Elizabeth S. Spelke and Katherine D. Kinzler. Core knowledge. Developmental\\nscience , pages 89‚Äì96, 2007.\\n[86] Robert Sternberg. Culture and intelligence. The American psychologist , 59:325‚Äì38,\\n07 2004.\\n[87] Robert Sternberg and Douglas Detterman. What is Intelligence? Contemporary\\nViewpoints on Its Nature and DeÔ¨Ånition . 1986.\\n[88] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction\\n(Second Edition) . MIT Press, Cambridge, MA, 2018.\\n[89] OpenAI team. OpenAI Five , 2019. https://openai.com/blog/\\nopenai-five/ Accessed: 2019-09-30.\\n[90] OpenAI team. OpenAI Five Arena Results , 2019. https://arena.openai.\\ncom/#/results Accessed: 2019-09-30.\\n[91] A. M. Turing. Computing machinery and intelligence. 1950.\\n[92] Vladimir N. Vapnik. The Nature of Statistical Learning Theory . Springer-Verlag,\\nBerlin, Heidelberg, 1995.\\n[93] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha\\nVezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K ¬®uttler, John Agapiou, Ju-\\nlian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan,\\nTom Schaul, Hado van Hasselt, David Silver, Timothy P. Lillicrap, Kevin Calderone,\\nPaul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and\\nRodney Tsing. Starcraft ii: A new challenge for reinforcement learning. ArXiv ,\\nabs/1708.04782, 2017.\\n[94] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\\nFelix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark\\nfor general-purpose language understanding systems. 2019.\\n[95] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\\nBowman. Glue: A multi-task benchmark and analysis platform for natural language\\nunderstanding. 2018.\\n[96] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Paired open-ended\\ntrailblazer (poet): Endlessly generating increasingly complex and diverse learning\\nenvironments and their solutions. ArXiv , abs/1901.01753, 2019.\\n[97] David H Wolpert. What the no free lunch theorems really mean; how to improve\\nsearch algorithms.\\n[98] D.H. Wolpert and W.G. Macready. No free lunch theorems for optimization. IEEE\\nTransactions on Evolutionary Computation , pages 67‚Äì82, 1997.\\n[99] Stephen G. Wozniak. Three minutes with steve wozniak. PC World , 2007.\\n[100] Shih-Ying Yang and Robert J Sternberg. Taiwanese chinese people‚Äôs conceptions of\\nintelligence. Intelligence , 25(1):21‚Äì36, 1997.\\n63[101] Amy Zhang, Nicolas Ballas, and Joelle Pineau. A dissection of overÔ¨Åtting and gen-\\neralization in continuous reinforcement learning. arXiv preprint arXiv:1806.07937 ,\\n2018.\\n[102] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.\\nUnderstanding deep learning requires rethinking generalization. 2017.\\n64'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_text = ''\n",
    "# for page in pages:\n",
    "#     raw_text += page.page_content\n",
    "# raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
     ]
    }
   ],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1995, which is longer than the specified 256\n",
      "Created a chunk of size 544, which is longer than the specified 256\n",
      "Created a chunk of size 327, which is longer than the specified 256\n",
      "Created a chunk of size 421, which is longer than the specified 256\n",
      "Created a chunk of size 387, which is longer than the specified 256\n",
      "Created a chunk of size 639, which is longer than the specified 256\n",
      "Created a chunk of size 947, which is longer than the specified 256\n",
      "Created a chunk of size 633, which is longer than the specified 256\n",
      "Created a chunk of size 722, which is longer than the specified 256\n",
      "Created a chunk of size 641, which is longer than the specified 256\n",
      "Created a chunk of size 574, which is longer than the specified 256\n",
      "Created a chunk of size 626, which is longer than the specified 256\n",
      "Created a chunk of size 766, which is longer than the specified 256\n",
      "Created a chunk of size 476, which is longer than the specified 256\n",
      "Created a chunk of size 692, which is longer than the specified 256\n",
      "Created a chunk of size 775, which is longer than the specified 256\n",
      "Created a chunk of size 433, which is longer than the specified 256\n",
      "Created a chunk of size 519, which is longer than the specified 256\n",
      "Created a chunk of size 861, which is longer than the specified 256\n",
      "Created a chunk of size 738, which is longer than the specified 256\n",
      "Created a chunk of size 582, which is longer than the specified 256\n",
      "Created a chunk of size 284, which is longer than the specified 256\n",
      "Created a chunk of size 520, which is longer than the specified 256\n",
      "Created a chunk of size 525, which is longer than the specified 256\n",
      "Created a chunk of size 374, which is longer than the specified 256\n",
      "Created a chunk of size 454, which is longer than the specified 256\n",
      "Created a chunk of size 950, which is longer than the specified 256\n",
      "Created a chunk of size 821, which is longer than the specified 256\n",
      "Created a chunk of size 1435, which is longer than the specified 256\n",
      "Created a chunk of size 413, which is longer than the specified 256\n",
      "Created a chunk of size 1167, which is longer than the specified 256\n",
      "Created a chunk of size 995, which is longer than the specified 256\n",
      "Created a chunk of size 283, which is longer than the specified 256\n",
      "Created a chunk of size 337, which is longer than the specified 256\n",
      "Created a chunk of size 742, which is longer than the specified 256\n",
      "Created a chunk of size 761, which is longer than the specified 256\n",
      "Created a chunk of size 883, which is longer than the specified 256\n",
      "Created a chunk of size 1342, which is longer than the specified 256\n",
      "Created a chunk of size 439, which is longer than the specified 256\n",
      "Created a chunk of size 770, which is longer than the specified 256\n",
      "Created a chunk of size 898, which is longer than the specified 256\n",
      "Created a chunk of size 1037, which is longer than the specified 256\n",
      "Created a chunk of size 821, which is longer than the specified 256\n",
      "Created a chunk of size 417, which is longer than the specified 256\n",
      "Created a chunk of size 1484, which is longer than the specified 256\n",
      "Created a chunk of size 1057, which is longer than the specified 256\n",
      "Created a chunk of size 548, which is longer than the specified 256\n",
      "Created a chunk of size 1044, which is longer than the specified 256\n",
      "Created a chunk of size 415, which is longer than the specified 256\n",
      "Created a chunk of size 347, which is longer than the specified 256\n",
      "Created a chunk of size 369, which is longer than the specified 256\n",
      "Created a chunk of size 271, which is longer than the specified 256\n",
      "Created a chunk of size 262, which is longer than the specified 256\n",
      "Created a chunk of size 336, which is longer than the specified 256\n",
      "Created a chunk of size 645, which is longer than the specified 256\n",
      "Created a chunk of size 323, which is longer than the specified 256\n",
      "Created a chunk of size 719, which is longer than the specified 256\n",
      "Created a chunk of size 535, which is longer than the specified 256\n",
      "Created a chunk of size 1120, which is longer than the specified 256\n",
      "Created a chunk of size 1268, which is longer than the specified 256\n",
      "Created a chunk of size 870, which is longer than the specified 256\n",
      "Created a chunk of size 943, which is longer than the specified 256\n",
      "Created a chunk of size 1464, which is longer than the specified 256\n",
      "Created a chunk of size 806, which is longer than the specified 256\n",
      "Created a chunk of size 263, which is longer than the specified 256\n",
      "Created a chunk of size 1898, which is longer than the specified 256\n",
      "Created a chunk of size 1066, which is longer than the specified 256\n",
      "Created a chunk of size 535, which is longer than the specified 256\n",
      "Created a chunk of size 648, which is longer than the specified 256\n",
      "Created a chunk of size 1506, which is longer than the specified 256\n",
      "Created a chunk of size 762, which is longer than the specified 256\n",
      "Created a chunk of size 1270, which is longer than the specified 256\n",
      "Created a chunk of size 1022, which is longer than the specified 256\n",
      "Created a chunk of size 746, which is longer than the specified 256\n",
      "Created a chunk of size 717, which is longer than the specified 256\n",
      "Created a chunk of size 550, which is longer than the specified 256\n",
      "Created a chunk of size 1309, which is longer than the specified 256\n",
      "Created a chunk of size 1160, which is longer than the specified 256\n",
      "Created a chunk of size 702, which is longer than the specified 256\n",
      "Created a chunk of size 364, which is longer than the specified 256\n",
      "Created a chunk of size 1121, which is longer than the specified 256\n",
      "Created a chunk of size 517, which is longer than the specified 256\n",
      "Created a chunk of size 697, which is longer than the specified 256\n",
      "Created a chunk of size 823, which is longer than the specified 256\n",
      "Created a chunk of size 314, which is longer than the specified 256\n",
      "Created a chunk of size 1060, which is longer than the specified 256\n",
      "Created a chunk of size 897, which is longer than the specified 256\n",
      "Created a chunk of size 471, which is longer than the specified 256\n",
      "Created a chunk of size 396, which is longer than the specified 256\n",
      "Created a chunk of size 325, which is longer than the specified 256\n",
      "Created a chunk of size 295, which is longer than the specified 256\n",
      "Created a chunk of size 432, which is longer than the specified 256\n",
      "Created a chunk of size 581, which is longer than the specified 256\n",
      "Created a chunk of size 689, which is longer than the specified 256\n",
      "Created a chunk of size 291, which is longer than the specified 256\n",
      "Created a chunk of size 281, which is longer than the specified 256\n",
      "Created a chunk of size 385, which is longer than the specified 256\n",
      "Created a chunk of size 379, which is longer than the specified 256\n",
      "Created a chunk of size 360, which is longer than the specified 256\n",
      "Created a chunk of size 453, which is longer than the specified 256\n",
      "Created a chunk of size 291, which is longer than the specified 256\n",
      "Created a chunk of size 276, which is longer than the specified 256\n",
      "Created a chunk of size 640, which is longer than the specified 256\n",
      "Created a chunk of size 693, which is longer than the specified 256\n",
      "Created a chunk of size 297, which is longer than the specified 256\n",
      "Created a chunk of size 323, which is longer than the specified 256\n",
      "Created a chunk of size 570, which is longer than the specified 256\n",
      "Created a chunk of size 259, which is longer than the specified 256\n",
      "Created a chunk of size 540, which is longer than the specified 256\n",
      "Created a chunk of size 259, which is longer than the specified 256\n",
      "Created a chunk of size 557, which is longer than the specified 256\n",
      "Created a chunk of size 258, which is longer than the specified 256\n",
      "Created a chunk of size 370, which is longer than the specified 256\n",
      "Created a chunk of size 346, which is longer than the specified 256\n",
      "Created a chunk of size 484, which is longer than the specified 256\n",
      "Created a chunk of size 386, which is longer than the specified 256\n",
      "Created a chunk of size 424, which is longer than the specified 256\n",
      "Created a chunk of size 557, which is longer than the specified 256\n",
      "Created a chunk of size 393, which is longer than the specified 256\n",
      "Created a chunk of size 579, which is longer than the specified 256\n",
      "Created a chunk of size 294, which is longer than the specified 256\n",
      "Created a chunk of size 534, which is longer than the specified 256\n",
      "Created a chunk of size 701, which is longer than the specified 256\n",
      "Created a chunk of size 301, which is longer than the specified 256\n",
      "Created a chunk of size 520, which is longer than the specified 256\n",
      "Created a chunk of size 499, which is longer than the specified 256\n",
      "Created a chunk of size 342, which is longer than the specified 256\n",
      "Created a chunk of size 1103, which is longer than the specified 256\n",
      "Created a chunk of size 495, which is longer than the specified 256\n",
      "Created a chunk of size 430, which is longer than the specified 256\n",
      "Created a chunk of size 358, which is longer than the specified 256\n",
      "Created a chunk of size 556, which is longer than the specified 256\n",
      "Created a chunk of size 441, which is longer than the specified 256\n",
      "Created a chunk of size 380, which is longer than the specified 256\n",
      "Created a chunk of size 419, which is longer than the specified 256\n",
      "Created a chunk of size 273, which is longer than the specified 256\n",
      "Created a chunk of size 520, which is longer than the specified 256\n",
      "Created a chunk of size 890, which is longer than the specified 256\n",
      "Created a chunk of size 1164, which is longer than the specified 256\n",
      "Created a chunk of size 509, which is longer than the specified 256\n",
      "Created a chunk of size 891, which is longer than the specified 256\n",
      "Created a chunk of size 430, which is longer than the specified 256\n",
      "Created a chunk of size 651, which is longer than the specified 256\n",
      "Created a chunk of size 432, which is longer than the specified 256\n",
      "Created a chunk of size 506, which is longer than the specified 256\n",
      "Created a chunk of size 689, which is longer than the specified 256\n",
      "Created a chunk of size 566, which is longer than the specified 256\n",
      "Created a chunk of size 294, which is longer than the specified 256\n",
      "Created a chunk of size 502, which is longer than the specified 256\n",
      "Created a chunk of size 257, which is longer than the specified 256\n",
      "Created a chunk of size 542, which is longer than the specified 256\n",
      "Created a chunk of size 272, which is longer than the specified 256\n",
      "Created a chunk of size 451, which is longer than the specified 256\n",
      "Created a chunk of size 420, which is longer than the specified 256\n",
      "Created a chunk of size 271, which is longer than the specified 256\n",
      "Created a chunk of size 411, which is longer than the specified 256\n",
      "Created a chunk of size 638, which is longer than the specified 256\n",
      "Created a chunk of size 377, which is longer than the specified 256\n",
      "Created a chunk of size 321, which is longer than the specified 256\n",
      "Created a chunk of size 465, which is longer than the specified 256\n",
      "Created a chunk of size 304, which is longer than the specified 256\n",
      "Created a chunk of size 390, which is longer than the specified 256\n",
      "Created a chunk of size 402, which is longer than the specified 256\n",
      "Created a chunk of size 257, which is longer than the specified 256\n",
      "Created a chunk of size 701, which is longer than the specified 256\n",
      "Created a chunk of size 321, which is longer than the specified 256\n",
      "Created a chunk of size 272, which is longer than the specified 256\n",
      "Created a chunk of size 369, which is longer than the specified 256\n",
      "Created a chunk of size 600, which is longer than the specified 256\n",
      "Created a chunk of size 269, which is longer than the specified 256\n",
      "Created a chunk of size 288, which is longer than the specified 256\n",
      "Created a chunk of size 361, which is longer than the specified 256\n",
      "Created a chunk of size 507, which is longer than the specified 256\n",
      "Created a chunk of size 427, which is longer than the specified 256\n",
      "Created a chunk of size 524, which is longer than the specified 256\n",
      "Created a chunk of size 469, which is longer than the specified 256\n",
      "Created a chunk of size 516, which is longer than the specified 256\n",
      "Created a chunk of size 302, which is longer than the specified 256\n",
      "Created a chunk of size 322, which is longer than the specified 256\n",
      "Created a chunk of size 297, which is longer than the specified 256\n",
      "Created a chunk of size 675, which is longer than the specified 256\n",
      "Created a chunk of size 388, which is longer than the specified 256\n",
      "Created a chunk of size 292, which is longer than the specified 256\n",
      "Created a chunk of size 397, which is longer than the specified 256\n",
      "Created a chunk of size 303, which is longer than the specified 256\n",
      "Created a chunk of size 399, which is longer than the specified 256\n",
      "Created a chunk of size 268, which is longer than the specified 256\n",
      "Created a chunk of size 1319, which is longer than the specified 256\n",
      "Created a chunk of size 791, which is longer than the specified 256\n",
      "Created a chunk of size 482, which is longer than the specified 256\n",
      "Created a chunk of size 338, which is longer than the specified 256\n",
      "Created a chunk of size 324, which is longer than the specified 256\n",
      "Created a chunk of size 511, which is longer than the specified 256\n",
      "Created a chunk of size 493, which is longer than the specified 256\n",
      "Created a chunk of size 412, which is longer than the specified 256\n",
      "Created a chunk of size 383, which is longer than the specified 256\n",
      "Created a chunk of size 298, which is longer than the specified 256\n",
      "Created a chunk of size 581, which is longer than the specified 256\n",
      "Created a chunk of size 350, which is longer than the specified 256\n",
      "Created a chunk of size 330, which is longer than the specified 256\n",
      "Created a chunk of size 317, which is longer than the specified 256\n",
      "Created a chunk of size 352, which is longer than the specified 256\n",
      "Created a chunk of size 620, which is longer than the specified 256\n",
      "Created a chunk of size 420, which is longer than the specified 256\n",
      "Created a chunk of size 953, which is longer than the specified 256\n",
      "Created a chunk of size 438, which is longer than the specified 256\n",
      "Created a chunk of size 696, which is longer than the specified 256\n",
      "Created a chunk of size 464, which is longer than the specified 256\n",
      "Created a chunk of size 647, which is longer than the specified 256\n",
      "Created a chunk of size 1148, which is longer than the specified 256\n",
      "Created a chunk of size 384, which is longer than the specified 256\n",
      "Created a chunk of size 335, which is longer than the specified 256\n",
      "Created a chunk of size 1119, which is longer than the specified 256\n",
      "Created a chunk of size 485, which is longer than the specified 256\n",
      "Created a chunk of size 552, which is longer than the specified 256\n",
      "Created a chunk of size 281, which is longer than the specified 256\n",
      "Created a chunk of size 302, which is longer than the specified 256\n",
      "Created a chunk of size 1080, which is longer than the specified 256\n",
      "Created a chunk of size 577, which is longer than the specified 256\n",
      "Created a chunk of size 374, which is longer than the specified 256\n",
      "Created a chunk of size 273, which is longer than the specified 256\n",
      "Created a chunk of size 662, which is longer than the specified 256\n",
      "Created a chunk of size 269, which is longer than the specified 256\n",
      "Created a chunk of size 258, which is longer than the specified 256\n",
      "Created a chunk of size 269, which is longer than the specified 256\n",
      "Created a chunk of size 281, which is longer than the specified 256\n",
      "Created a chunk of size 317, which is longer than the specified 256\n",
      "Created a chunk of size 327, which is longer than the specified 256\n",
      "Created a chunk of size 257, which is longer than the specified 256\n",
      "Created a chunk of size 297, which is longer than the specified 256\n",
      "Created a chunk of size 501, which is longer than the specified 256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=256, \n",
    "    chunk_overlap=64,\n",
    "    # separator=\"\\n\",\n",
    "    # length_function=len\n",
    ")\n",
    "\n",
    "# texts = text_splitter.split_text(raw_text)\n",
    "documents = text_splitter.split_documents(document)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9 1 0 2\\n\\nv o N 5 2\\n\\n] I\\n\\nA . s c [\\n\\n2 v 7 4 5 1 0 . 1 1 9 1 : v i X r a\\n\\nOn the Measure of Intelligence\\n\\nFranc¬∏ois Chollet ‚àó Google, Inc. fchollet@google.com\\n\\nNovember 5, 2019\\n\\nAbstract'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make deliberate progress towards more intelligent and more human-like artiÔ¨Åcial systems, we need to be following an appropriate feedback signal: we need to be able to deÔ¨Åne and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abun- dance of attempts to deÔ¨Åne and measure intelligence, across both the Ô¨Åelds of psychology and AI. We summarize and critically assess these deÔ¨Ånitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates to- wards benchmarking intelligence by comparing the skill exhibited by AIs and humans at speciÔ¨Åc tasks, such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow ex- perimenters to ‚Äúbuy‚Äù arbitrary levels of skills for a system, in a way that masks the system‚Äôs own generalization power. We then articulate a new formal deÔ¨Ånition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efÔ¨Åciency and highlighting the concepts of scope, generalization difÔ¨Åculty, priors, and experience, as critical pieces to be accounted for in characterizing intelligent systems. Using this deÔ¨Å- nition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a new benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general Ô¨Çuid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].page_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora convertiremos cada p√°gina en un embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_23757/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">4265502344.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_23757/4265502344.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/juan/.local/lib/python3.10/site-packages/langchain/embeddings/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">huggingface.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">72</span> in         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">embed_documents</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 69 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #808000; text-decoration-color: #808000\">Returns:</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 70 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #808000; text-decoration-color: #808000\">List of embeddings, one for each text.</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 71 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span> 72 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span>texts = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">map</span>(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: x.replace(<span style=\"color: #808000; text-decoration-color: #808000\">\"\\n\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\" \"</span>), texts))                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 73 ‚îÇ   ‚îÇ   </span>embeddings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.client.encode(texts, **<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encode_kwargs)                       <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 74 ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> embeddings.tolist()                                                         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 75 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/juan/.local/lib/python3.10/site-packages/langchain/embeddings/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">huggingface.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">72</span> in         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;lambda&gt;</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 69 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #808000; text-decoration-color: #808000\">Returns:</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 70 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">‚îÇ   ‚îÇ   ‚îÇ   </span><span style=\"color: #808000; text-decoration-color: #808000\">List of embeddings, one for each text.</span>                                         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 71 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">‚îÇ   ‚îÇ   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span> <span style=\"color: #800000; text-decoration-color: #800000\">‚ù± </span> 72 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">‚îÇ   ‚îÇ   </span>texts = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">map</span>(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> x: x.replace(<span style=\"color: #808000; text-decoration-color: #808000\">\"\\n\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\" \"</span>), texts))                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 73 ‚îÇ   ‚îÇ   </span>embeddings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.client.encode(texts, **<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encode_kwargs)                       <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 74 ‚îÇ   ‚îÇ   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> embeddings.tolist()                                                         <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 75 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">‚îÇ</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Document'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'replace'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m‚ï≠‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/tmp/ipykernel_23757/\u001b[0m\u001b[1;33m4265502344.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_23757/4265502344.py'\u001b[0m                        \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/home/juan/.local/lib/python3.10/site-packages/langchain/embeddings/\u001b[0m\u001b[1;33mhuggingface.py\u001b[0m:\u001b[94m72\u001b[0m in         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[92membed_documents\u001b[0m                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 69 \u001b[0m\u001b[2;33m‚îÇ   ‚îÇ   \u001b[0m\u001b[33mReturns:\u001b[0m                                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 70 \u001b[0m\u001b[2;33m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[33mList of embeddings, one for each text.\u001b[0m                                         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 71 \u001b[0m\u001b[2;33m‚îÇ   ‚îÇ   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m 72 \u001b[2m‚îÇ   ‚îÇ   \u001b[0mtexts = \u001b[96mlist\u001b[0m(\u001b[96mmap\u001b[0m(\u001b[94mlambda\u001b[0m x: x.replace(\u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m\"\u001b[0m), texts))                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 73 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0membeddings = \u001b[96mself\u001b[0m.client.encode(texts, **\u001b[96mself\u001b[0m.encode_kwargs)                       \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 74 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m embeddings.tolist()                                                         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 75 \u001b[0m                                                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[2;33m/home/juan/.local/lib/python3.10/site-packages/langchain/embeddings/\u001b[0m\u001b[1;33mhuggingface.py\u001b[0m:\u001b[94m72\u001b[0m in         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[92m<lambda>\u001b[0m                                                                                         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m                                                                                                  \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 69 \u001b[0m\u001b[2;33m‚îÇ   ‚îÇ   \u001b[0m\u001b[33mReturns:\u001b[0m                                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 70 \u001b[0m\u001b[2;33m‚îÇ   ‚îÇ   ‚îÇ   \u001b[0m\u001b[33mList of embeddings, one for each text.\u001b[0m                                         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 71 \u001b[0m\u001b[2;33m‚îÇ   ‚îÇ   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m \u001b[31m‚ù± \u001b[0m 72 \u001b[2m‚îÇ   ‚îÇ   \u001b[0mtexts = \u001b[96mlist\u001b[0m(\u001b[96mmap\u001b[0m(\u001b[94mlambda\u001b[0m x: x.replace(\u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m\"\u001b[0m), texts))                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 73 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0membeddings = \u001b[96mself\u001b[0m.client.encode(texts, **\u001b[96mself\u001b[0m.encode_kwargs)                       \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 74 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0m\u001b[94mreturn\u001b[0m embeddings.tolist()                                                         \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚îÇ\u001b[0m   \u001b[2m 75 \u001b[0m                                                                                           \u001b[31m‚îÇ\u001b[0m\n",
       "\u001b[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'Document'\u001b[0m object has no attribute \u001b[32m'replace'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "query_result = embeddings.embed_documents(documents[:1])\n",
    "# query_result = embeddings.embed_query(texts[0])\n",
    "\n",
    "# doc_result = embeddings.embed_documents([text])\n",
    "\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# vectorstore = Chroma.from_texts(texts, embeddings)\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMarcus Hutter, Shane Legg, and Marcus Hutter. Universal intelligence: A deÔ¨Ånition of machine intelligence. 2007.\\n\\nA:\\n\\nThe paper is by Marcus Hutter, Shane Legg, and Marcus Hutter.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"Who is the author of the paper?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nIntelligence is the efÔ¨Åciency with which a learning system turns experience and priors into skill at previously unknown tasks.\\n\\nA:\\n\\nThe answer is that intelligence is the ability to learn new things. This is a very broad term, and it is not limited to the ability to learn new things. It is also not limited to the ability to learn new things in a particular domain. It is also not limited to the ability to learn new things in a particular way. It is also not limited to the ability to learn new things in a particular way in a particular domain. It is also not limited to the ability to learn new things in a particular way in a particular domain in a particular way. It is also not limited to the ability to learn new things in a particular way in a particular domain in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in a particular way in'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat_history = [(query, result[\"answer\"])]\n",
    "chat_history = []\n",
    "query = \"What is the definition of intelligence?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'56\\n\\nIntelligence is the efÔ¨Åciency with which a learning system turns experience and priors\\n\\ninto skill at previously unknown tasks.\\n\\nAs such, a measure of intelligence must account for priors, experience, and general-\\n\\nization difÔ¨Åculty.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['source_documents'][1].page_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Componentes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Data loading](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo multiple chain para auto-gpt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
