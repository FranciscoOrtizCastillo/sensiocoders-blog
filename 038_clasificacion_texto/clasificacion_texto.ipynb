{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/038_clasificacion_texto/clasificacion_texto.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificaci√≥n de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el [post](https://sensioai.com/blog/037_charRNN) anterior aprendimos a c√≥mo podemos entrenar una `red neuronal recurrente` para generar texto letra a letra, una tarea muy interesante dentro del mundo del `procesado de lenguaje natur` o `NLP` (*natural language processing*) por sus siglas en ingl√©s. A√∫n as√≠, posiblemente la tarea m√°s interesante desde un punto de vista pr√°ctico y con m√°s aplicaciones en la industria sea la de `clasificaci√≥n de texto`. De la misma manera que podemos entrenar `redes neuronales` para [clasificaci√≥n de im√°genes](https://sensioai.com/blog/033_receta_entrenamiento), es posible entrenar modelos de `machine learning` capaces de asignar una etiqueta determinada a un trozo de texto. Podemos encontrar este tipo de aplicaciones en redes sociales, por ejemplo, para detectar autom√°ticamente mensaje ofensivos o en opiniones de usuarios sobre art√≠culos para medir su satisfacci√≥n. En este post vamos a ver c√≥mo podemos entrenar una `red neuronal recurrente` para clasificar *reviews* de pel√≠culas, una tarea tambi√©n conocida por el nombre de `sentiment analysis`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## El *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el post anterior descargamos el libro *Don Quijote de la Mancha* en formato *txt*, luego lo cargamos en `Python` para proceder al proceso de `tokenizaci√≥n`. Si bien implementamos nuestra propia l√≥gica de procesado de texto, a la pr√°ctica es m√°s conveniente utilizar herramientas de terceros bien testeadas y optimizadas. Entre las diferentes librer√≠as que existen de `NLP`, nosotros utilizaremos [torchtext](https://pytorch.org/text/) ya que est√° bien integrada con el ecosistema de `Pytorch`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.15.2-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/paco/anaconda3/lib/python3.11/site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/paco/anaconda3/lib/python3.11/site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.0.1 in /Users/paco/anaconda3/lib/python3.11/site-packages (from torchtext) (2.0.1)\n",
      "Requirement already satisfied: numpy in /Users/paco/anaconda3/lib/python3.11/site-packages (from torchtext) (1.24.3)\n",
      "Collecting torchdata==0.6.1 (from torchtext)\n",
      "  Downloading torchdata-0.6.1-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/paco/anaconda3/lib/python3.11/site-packages (from torch==2.0.1->torchtext) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/paco/anaconda3/lib/python3.11/site-packages (from torch==2.0.1->torchtext) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/paco/anaconda3/lib/python3.11/site-packages (from torch==2.0.1->torchtext) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/paco/anaconda3/lib/python3.11/site-packages (from torch==2.0.1->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/paco/anaconda3/lib/python3.11/site-packages (from torch==2.0.1->torchtext) (3.1.2)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/paco/anaconda3/lib/python3.11/site-packages (from torchdata==0.6.1->torchtext) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/paco/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/paco/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/paco/anaconda3/lib/python3.11/site-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/paco/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.0.1->torchtext) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/paco/anaconda3/lib/python3.11/site-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n",
      "Installing collected packages: torchdata, torchtext\n",
      "Successfully installed torchdata-0.6.1 torchtext-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:39:11.207060Z",
     "start_time": "2020-09-01T16:39:10.641059Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `torchtext` tenemos disponible multitud de datasets que podemos utilizar, los cuales son ideales cuando estamos aprendiendo a trabajar con `redes neuronales` para tareas de `NLP`. En este caso descargaremos el dataset `IMDB` que contiene opiniones sobre pel√≠culas. Nuestro objetivo ser√°, dada una de estas *reviews* asignarle una etiqueta binaria (opini√≥n positiva o negativa). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:09.615532Z",
     "start_time": "2020-09-01T16:39:11.208061Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Package `portalocker` is required to be installed to use this datapipe.Please use `pip install 'portalocker>=2.0.0'` or`conda install -c conda-forge 'portalocker>=2/0.0'`to install the package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdata/datapipes/iter/util/cacheholder.py:38\u001b[0m, in \u001b[0;36m_assert_portalocker\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mportalocker\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'portalocker'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#TEXT = torchtext.legacy.data.Field(tokenize = 'spacy')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#LABEL = torchtext.data.LabelField(dtype = torch.long)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#En la nueva API \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_iter, test_iter \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mIMDB(split\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/data/datasets_utils.py:193\u001b[0m, in \u001b[0;36m_create_dataset_directory.<locals>.decorator.<locals>.wrapper\u001b[0;34m(root, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(new_root):\n\u001b[1;32m    192\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(new_root, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(root\u001b[38;5;241m=\u001b[39mnew_root, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/data/datasets_utils.py:155\u001b[0m, in \u001b[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001b[0;34m(root, split, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m _check_default_set(split, splits, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(fn(root, item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_datasets(\u001b[38;5;28mtuple\u001b[39m(result), split)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/datasets/imdb.py:95\u001b[0m, in \u001b[0;36mIMDB\u001b[0;34m(root, split)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage `torchdata` not found. Please install following instructions at https://github.com/pytorch/data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     93\u001b[0m url_dp \u001b[38;5;241m=\u001b[39m IterableWrapper([URL])\n\u001b[0;32m---> 95\u001b[0m cache_compressed_dp \u001b[38;5;241m=\u001b[39m url_dp\u001b[38;5;241m.\u001b[39mon_disk_cache(\n\u001b[1;32m     96\u001b[0m     filepath_fn\u001b[38;5;241m=\u001b[39mpartial(_filepath_fn, root),\n\u001b[1;32m     97\u001b[0m     hash_dict\u001b[38;5;241m=\u001b[39m{_filepath_fn(root): MD5},\n\u001b[1;32m     98\u001b[0m     hash_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmd5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m cache_compressed_dp \u001b[38;5;241m=\u001b[39m HttpReader(cache_compressed_dp)\u001b[38;5;241m.\u001b[39mend_caching(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m, same_filepath_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    102\u001b[0m labels \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/datapipes/datapipe.py:139\u001b[0m, in \u001b[0;36mIterDataPipe.register_datapipe_as_function.<locals>.class_function\u001b[0;34m(cls, enable_df_api_tracing, source_dp, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclass_function\u001b[39m(\u001b[38;5;28mcls\u001b[39m, enable_df_api_tracing, source_dp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 139\u001b[0m     result_pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(source_dp, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result_pipe, IterDataPipe):\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m enable_df_api_tracing \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source_dp, DFIterDataPipe):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdata/datapipes/iter/util/cacheholder.py:207\u001b[0m, in \u001b[0;36mOnDiskCacheHolderIterDataPipe.__init__\u001b[0;34m(self, source_datapipe, filepath_fn, hash_dict, hash_type, extra_check_fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    201\u001b[0m     source_datapipe: IterDataPipe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     extra_check_fn: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    206\u001b[0m ):\n\u001b[0;32m--> 207\u001b[0m     _assert_portalocker()\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_datapipe \u001b[38;5;241m=\u001b[39m source_datapipe\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filepath_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchdata/datapipes/iter/util/cacheholder.py:47\u001b[0m, in \u001b[0;36m_assert_portalocker\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage `portalocker` is required to be installed to use this datapipe.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportalocker>=2.0.0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`conda install -c conda-forge \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportalocker>=2/0.0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto install the package\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Package `portalocker` is required to be installed to use this datapipe.Please use `pip install 'portalocker>=2.0.0'` or`conda install -c conda-forge 'portalocker>=2/0.0'`to install the package"
     ]
    }
   ],
   "source": [
    "#TEXT = torchtext.legacy.data.Field(tokenize = 'spacy')\n",
    "#LABEL = torchtext.data.LabelField(dtype = torch.long)\n",
    "\n",
    "#train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "#En la nueva API \n",
    "# https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb#scrollTo=YHUYZ7yt0Lb5\n",
    "\n",
    "train_iter, test_iter = torchtext.datasets.IMDB(split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö° La clase `torchtext.data.Field` contiene toda la l√≥gica de tokenizaci√≥n y procesado de texto necesaria, lo cual nos facilitar√° mucho la vida para esta tarea. Puedes aprender m√°s sobre esta clase, y la librer√≠a en general, en su [documentaci√≥n](https://pytorch.org/text/data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso en concreto, disponemos de 25000 muestras tanto para el entrenamiento como evaluaci√≥n de nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:09.632530Z",
     "start_time": "2020-09-01T16:40:09.616535Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la siguiente manera podemos ver un ejemplo de muestra de nuestro dataset, que est√° compuesto por el texto y la valoraci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:09.647529Z",
     "start_time": "2020-09-01T16:40:09.633530Z"
    }
   },
   "outputs": [],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adem√°s de proveernos con varios datasets, `torchtext` tambi√©n nos facilita mucho la vida a la hora de llevar a cabo el proceso de `tokenizaci√≥n`. En este caso vamos a construir un vocabulario que contendr√° un n√∫mero determinado de palabras (ya que si aqueremos incluirlas los requisitos computacionales se disparar√≠an). Para ello el tokenizador calcular√° la frecuencia de cada palabra en el texto y se quedar√° con la cantidad que especifiquemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:10.841436Z",
     "start_time": "2020-09-01T16:40:09.648529Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "len(TEXT.vocab), len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pudes ver tenemos un vocabulario con la longitud determinada m√°s dos, estos dos *tokens* extra corresponden a los *tokens* `<unk>`, que se le asignar√°n a las palabras desconocidas y las menos frecuentes que no hayan pasado el primer filtro, y el *token* `<pad>`, que se usar√° para que todas las frases en un *batch* tengan la misma longitud (necesitamos tensores recutangulares para entrenar nuestra red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:10.883957Z",
     "start_time": "2020-09-01T16:40:10.843442Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:10.899961Z",
     "start_time": "2020-09-01T16:40:10.885959Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:10.915962Z",
     "start_time": "2020-09-01T16:40:10.900958Z"
    }
   },
   "outputs": [],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El √∫ltimo paso para tener nuestros datos listos para entrenar una red neuronal es construir el `DataLoader` encargado de alimentar nuestra red con *batches* de frases de manera eficiente. Para ello utilizamos la clase `torchtext.data.BucketIterator`, que adem√°s juntar√° frases de similar longitud minimazndo el *padding* necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:10.947957Z",
     "start_time": "2020-09-01T16:40:10.916958Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataloader = {\n",
    "    'train': torchtext.data.BucketIterator(train_data, batch_size=64, shuffle=True, sort_within_batch=True, device=device),\n",
    "    'test': torchtext.data.BucketIterator(test_data, batch_size=64, device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder clasificar texto utilizaremos una [red recurrente](https://sensioai.com/blog/034_rnn_intro) de tipo `many-to-one`, la cual recibir√° el texto palabra por palabra. Usaremos el √∫ltimo estado oculto (el cual contendr√° informaci√≥n de toda la frase) para generar nuestra predicci√≥n final. Cada palabra estar√° representada por un vector, el cual ser√° aprendido por la red en la capa `embedding` (puedes aprender m√°s sobre esta capa en nuestro [post](https://sensioai.com/blog/037_charRNN) anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:10.963957Z",
     "start_time": "2020-09-01T16:40:10.948957Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=128, hidden_dim=128, output_dim=2, num_layers=2, dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(2*hidden_dim if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]        \n",
    "        embedded = self.embedding(text)        \n",
    "        #embedded = [sent len, batch size, emb dim]        \n",
    "        output, hidden = self.rnn(embedded)        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        y = self.fc(output[-1,:,:].squeeze(0))     \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° A diferencia de los pots anteriores, ahora la dimensi√≥n *batch* NO es la primera. Este es el comportamiento por defecto de las capas recurrentes en `Pytorch`. Puedes modificar esto a√±adiendo la opci√≥n `batch_first=True` en la capa recurrente (y aseg√∫rate que tu dataloader utiliza tambi√©n la primera dimensi√≥n para el batch. En `torchtext` puedes indicarlo con el par√°metro `batch_first=True` a la hora de definir el `FIELD` en cuesti√≥n). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre, probamos que nuestra red est√© bien definida y las dimensiones encajen. A la entrada, esperamos tensores con dimensiones `longitud secuencia x batch`. Puedes verlo si sacamos unas muestras de nuestro dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:12.126961Z",
     "start_time": "2020-09-01T16:40:10.964958Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader['train']))\n",
    "\n",
    "batch.text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada palabra estr√° representada por un √≠ndice, que luego el `embedding` usar√° para extraer el vector determinado que representa la palabra. A la salida, nuestro modelo nos dar√° dos valores. Si el primer valor es mayor que el segundo, asignaremos la clase `0` (opini√≥n negativa) y viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:12.254958Z",
     "start_time": "2020-09-01T16:40:12.127961Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RNN(input_dim=len(TEXT.vocab))\n",
    "outputs = model(torch.randint(0, len(TEXT.vocab), (100, 64)))\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar nuestra red usamos el bucle est√°ndar que ya usamos en posts anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:40:12.270961Z",
     "start_time": "2020-09-01T16:40:12.256959Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def fit(model, dataloader, epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss, train_acc = [], []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            X, y = batch.text, batch.label\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
    "            train_acc.append(acc)\n",
    "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
    "        bar = tqdm(dataloader['test'])\n",
    "        val_loss, val_acc = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in bar:\n",
    "                X, y = batch.text, batch.label\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_hat = model(X)\n",
    "                loss = criterion(y_hat, y)\n",
    "                val_loss.append(loss.item())\n",
    "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
    "                val_acc.append(acc)\n",
    "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
    "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:36:22.941249Z",
     "start_time": "2020-09-01T16:34:05.448274Z"
    }
   },
   "outputs": [],
   "source": [
    "fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya podemos utilizar nuestro modelo para generar valoraciones de manera autom√°tica dada una opini√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:36:23.366252Z",
     "start_time": "2020-09-01T16:36:22.942250Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict(model, X):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(X).to(device)\n",
    "        pred = model(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:36:23.382248Z",
     "start_time": "2020-09-01T16:36:23.367255Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\"this film is terrible\", \"this film is great\", \"this film is good\", \"a waste of time\"]\n",
    "tokenized = [[tok.text for tok in nlp.tokenizer(sentence)] for sentence in sentences]\n",
    "indexed = [[TEXT.vocab.stoi[_t] for _t in t] for t in tokenized]\n",
    "tensor = torch.tensor(indexed).permute(1,0)\n",
    "predictions = torch.argmax(predict(model, tensor), axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso solo tenemos dos posibles clases, pero es f√°cil intuir que de ser capaces de construir un dataset con muchas m√°s clases que describan con mayor precisi√≥n el \"sentimiento\" en un texto podr√≠amos extraer much√≠sima informaci√≥n muy valiosa para, sobre todo, empresas que venden productos online m√°s all√° de las t√≠picas estrellas o puntuaciones que, pese a dar informaci√≥n valiosa, no aportan ning√∫n tipo de informaci√≥n accionable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Recurrentes Bidireccionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes recurrentes bidireccionales nos van a permitir, por norma general, obtener mejores resultados cuando trabajemos con datos secuenciales en los que \"podamos mirar al futuro\". En aplicaciones tales como la generaci√≥n de texto o la predicci√≥n de series temporales, esto no lo pod√≠amos hacer ya que el objetivo de la tarea es precisamente predecir valores futuros (y utilizar estos valores durante el entrenamiento no tendr√≠a sentido). Sin embargo, para la tarea de clasificaci√≥n de texto, s√≠ que podemos hacerlo.\n",
    "\n",
    "![](https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)\n",
    "\n",
    "Puedes conocer m√°s sobre este tipo de redes, as√≠ como otras mejoras, en este [post](https://sensioai.com/blog/036_rnn_mejoras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-01T16:39:19.400Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RNN(input_dim=len(TEXT.vocab), bidirectional=True)\n",
    "fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos aprendido a utilizar `redes neuronales recurrentes` para la tarea de clasificaci√≥n de texto. Esta tarea es muy √∫til en la industria, sobre todo para aquellos negocios que venden productos o servicios cuyos usuarios pueden valorar directamente de manera online de forma masiva. Tener un sistema automatizado que \"lea\" todas las opiniones y las clasifique en clases con significado, puede aportar mucha informaci√≥n valiosa a una empresa sobre la cual puede llevar a cabo acciones de mejora de manera r√°pida. Como hemos visto, el uso de la librer√≠a `torchtext` nos facilita mucho la vida a la hora de procesar el texto, y gracias a su integraci√≥n con `Pytorch` podremos entrenar modelos de manera r√°pida y sencilla. Para esta tarea en concreto, tambi√©n hemos visto que el uso de `redes recurrentes bidireccionales` nos puede dar un extra de precisi√≥n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
