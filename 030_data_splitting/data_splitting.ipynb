{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/030_data_splitting/data_splitting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjuntos de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos presentado la mayorÃ­a de conceptos fundamentales necesarios para entender cÃ³mo funciona una `red neuronal`, implementando desde cero los algoritmos que necesitamos para el diseÃ±o y entrenamiento de estos modelos. Sin embargo, de ahora en adelante utilizaremos la librerÃ­a de `redes neuronales` [Pytorch](https://pytorch.org/), la cual hemos introducido en los posts anteriores. A pesar de que ya conocemos cÃ³mo entrenar modelos simples de `PerceptrÃ³n Multicapa` en tareas de regresiÃ³n y clasificaciÃ³n, es posible que no obtengas los resultados deseados (al menos comparados con resultados que puedes encontrar en otras referencia). En los prÃ³ximos posts nos vamos a centrar en tÃ©cnicas concretas que van a permitirnos entrenar los mejores modelos posibles, terminando con una \"receta\" que podemos utilizar en nuestros proyectos. En este post empezamos viendo tÃ©cnicas de divisiÃ³n de datos necesarias para entrenar y validar nuestras `redes neuronales` de forma correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vamos a entrenar un `MLP` para clasificaciÃ³n de imÃ¡genes con el dataset MNIST, algo que ya hemos hecho en los posts anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T09:59:34.976907Z",
     "start_time": "2020-08-22T09:59:33.720873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Empezamos descargando nuestro dataset. En este primer ejemplo utilizaremos todos los datos disponibles para entrenar nuestra red. Al fin y al cabo, sabemos que cuÃ¡ntos mÃ¡s datos tengamos, mejor serÃ¡ nuestro modelo. AsÃ­ que, Â¿por quÃ© no usarlos todos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T09:59:53.224131Z",
     "start_time": "2020-08-22T09:59:34.977908Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paco/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mnist \u001b[38;5;241m=\u001b[39m fetch_openml(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist_784\u001b[39m\u001b[38;5;124m'\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m mnist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m], mnist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m, Y\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m      8\u001b[0m X_train\u001b[38;5;241m.\u001b[39mshape, y_train\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# descarga datos\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1,parser='auto')\n",
    "X, Y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X_train, y_train = X / 255., Y.astype(np.int)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:48.861556Z",
     "start_time": "2020-08-22T10:11:48.782563Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X).float().cuda()\n",
    "        self.Y = torch.from_numpy(Y).long().cuda()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, ix):\n",
    "        return self.X[ix], self.Y[ix]\n",
    "    \n",
    "dataset = Dataset(X_train, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> ðŸ’¡ Si no estÃ¡s familiarizado con `Pytorch`, te recomiendo que le eches un vistazo a nuestra [playlist](https://youtu.be/WL50sQVdQFg) para aprender a usar esta librerÃ­a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:49.053120Z",
     "start_time": "2020-08-22T10:11:49.039118Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(D_in=784, H=100, D_out=10):\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, H),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(H, D_out),\n",
    "    ).to(\"cuda\")\n",
    "    return model\n",
    "\n",
    "def fit(model, dataloader, epochs=10, log_each=1):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "    l = []\n",
    "    model.train()\n",
    "    for e in range(1, epochs+1): \n",
    "        _l = []\n",
    "        for x_b, y_b in dataloader:\n",
    "            y_pred = model(x_b)\n",
    "            loss = criterion(y_pred, y_b)\n",
    "            _l.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        l.append(np.mean(_l))\n",
    "        if not e % log_each:\n",
    "            print(f\"Epoch {e}/{epochs} loss {l[-1]:.5f}\")\n",
    "    return {'epoch': list(range(1, epochs+1)), 'loss': l}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:58.948180Z",
     "start_time": "2020-08-22T10:11:49.166441Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Muy bien, hemos entrenado nuestro clasificador de imÃ¡genes. Â¿CÃ³mo sabemos si es bueno? De momento la Ãºnica informaciÃ³n que tenemos es el valor de la funciÃ³n de pÃ©rdida, la medida del error que nuestro clasificador comete durante el entrenamiento. Esta valor decrece a medida que pasan las *epochs*, lo cual indica que nuestra `red` estÃ¡ aprendiendo y mejorando. Podemos observar esto la siguiente imÃ¡gen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:59.059221Z",
     "start_time": "2020-08-22T10:11:58.950179Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100)\n",
    "ax = plt.subplot(111)\n",
    "pd.DataFrame(hist).plot(x='epoch', grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "El siguiente paso podrÃ­a ser evaluar diferentes **mÃ©tricas**. En este [post](https://sensioai.com/blog/016_metricas_clasficiacion) hablamos extendidamente de este tema. La mÃ©trica mÃ¡s sencilla que podemos usar para evaluar un clasificador es la precisiÃ³n (cuÃ¡ntas imÃ¡genes nuestro modelo clasifica correctamente). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:59.075237Z",
     "start_time": "2020-08-22T10:11:59.060234Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(axis=-1,keepdims=True)\n",
    "\n",
    "def evaluate(x):\n",
    "    model.eval()\n",
    "    y_pred = model(x)\n",
    "    y_probas = softmax(y_pred)\n",
    "    return torch.argmax(y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:59.153439Z",
     "start_time": "2020-08-22T10:11:59.076239Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = evaluate(torch.from_numpy(X_train).float().cuda())\n",
    "accuracy_score(y_train, y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Superamos el $99 \\%$ de precisiÃ³n, Â¡nada mal!. Podemos incluir esta mÃ©trica en nuestro bucle de entrenamiento para ver su evoluciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:59.169442Z",
     "start_time": "2020-08-22T10:11:59.154439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit(model, dataloader, epochs=10, log_each=1):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "    model.train()\n",
    "    l, acc = [], []\n",
    "    for e in range(1, epochs+1): \n",
    "        _l, _acc = [], []\n",
    "        for x_b, y_b in dataloader:\n",
    "            y_pred = model(x_b)\n",
    "            loss = criterion(y_pred, y_b)\n",
    "            _l.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_probas = torch.argmax(softmax(y_pred), axis=1)            \n",
    "            _acc.append(accuracy_score(y_b.cpu().numpy(), y_probas.cpu().detach().numpy()))\n",
    "        l.append(np.mean(_l))\n",
    "        acc.append(np.mean(_acc))\n",
    "        if not e % log_each:\n",
    "            print(f\"Epoch {e}/{epochs} loss {l[-1]:.5f} acc {acc[-1]:.5f}\")\n",
    "    return {'epoch': list(range(1, epochs+1)), 'loss': l, 'acc': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:10.685015Z",
     "start_time": "2020-08-22T10:11:59.171442Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:10.921875Z",
     "start_time": "2020-08-22T10:12:10.686016Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200, figsize=(10,3))\n",
    "ax = plt.subplot(121)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='loss', grid=True, ax=ax)\n",
    "ax = plt.subplot(122)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='acc', grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hemos entrenado un clasificador de imÃ¡genes que alcanza el $99 \\%$ de precisiÃ³n en los datos utilizados para entrenar, ademÃ¡s viendo las curvas de entrenamiento parece que si entrenamos durante mÃ¡s *epochs*, podrÃ­amos incluso mejorar este resultado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:46.410853Z",
     "start_time": "2020-08-22T10:12:10.924875Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader, epochs=30, log_each=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:46.646171Z",
     "start_time": "2020-08-22T10:12:46.411852Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200, figsize=(10,3))\n",
    "ax = plt.subplot(121)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='loss', grid=True, ax=ax)\n",
    "ax = plt.subplot(122)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='acc', grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Llegamos a alcanzar el $100 \\%$ de precisiÃ³n. Nuestro modelo clasifica correctamente todas la imÃ¡genes en el dataset. Â¿Es posible que hayamos entrenado el mejor modelo de la historia?. Lamentablemente, la respuesta es NO. El procedimiento que hemos seguido tiene un fallo, y es que cuando este modelo estÃ© trabajando en el mundo real, las imÃ¡genes que va a recibir serÃ¡n diferentes a las utilizadas durante el entrenamiento, y ahora mismo no tenemos ni idea de cÃ³mo se va a comportar. Es por esto que necesitamos evaluar nuestro modelo en un conjunto de imÃ¡genes que no hayamos usado para entrenar para hacernos una idea de la *performance* de nuestro modelo en el mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## El conjunto de *test*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Para evaluar un modelo una vez entrenado, usamos el conjunto de datos de *test*. En multitud de ocasiones, este conjunto nos vendrÃ¡ dado directamente, sobretodo si el dataset con el que trabajamos se utiliza para comparar diferentes modelos en, por ejemplo, una competiciÃ³n o un *benchmark*. AdemÃ¡s, este conjunto de datos suele estar oculto y protegido de manera que todos los modelos sean siempre evaluados en el mismo conjunto de datos y, ademÃ¡s, en un conjunto de datos que nadie conoce. Esta es la manera mÃ¡s justa de comparar modelos, y la que utilizaremos nosotros para evaluar nuestros modelos una vez entrenados. En nuestro caso, simplemente separaremos un conjunto de imÃ¡genes de nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:46.834174Z",
     "start_time": "2020-08-22T10:12:46.647173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000] / 255., X[60000:] / 255., Y[:60000].astype(np.int), Y[60000:].astype(np.int)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ahora entrenaremos nuestro modelo sÃ³lo con las imÃ¡genes de entrenamiento, y lo evaluaremos en las imÃ¡genes de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:46.914170Z",
     "start_time": "2020-08-22T10:12:46.835173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(X_train, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:13:29.294456Z",
     "start_time": "2020-08-22T10:12:46.915170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader, epochs=30, log_each=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:13:29.531467Z",
     "start_time": "2020-08-22T10:13:29.295457Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200, figsize=(10,3))\n",
    "ax = plt.subplot(121)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='loss', grid=True, ax=ax)\n",
    "ax = plt.subplot(122)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='acc', grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:13:29.563467Z",
     "start_time": "2020-08-22T10:13:29.533467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = evaluate(torch.from_numpy(X_test).float().cuda())\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Oh oh... Nuestro modelo es perfecto en los datos de entrenamiento, sin embargo al evaluarlo en los datos de *test* (recuerda, datos no usados para entrenar) la precisiÃ³n se reduce. Este fenÃ³meno se conoce por el nombre de *overfitting*, y se da cuando un modelo se ajusta muy bien a los datos de entrenamiento pero luego falla en datos nuevos, no vistos en el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Capacidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hablamos de la capacidad de un modelo para referirnos a su potencia a la hora de representar un conjunto de datos.\n",
    "\n",
    "\n",
    "![](https://i0.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2017/12/generalizacion-machine-learning.png?resize=525%2C211)\n",
    "\n",
    "Si un modelo tiene poca capacidad, observaremos *underfitting*. El modelo no tiene suficientes parÃ¡metros para representar correctamente un dataset y tendremos una mala precisiÃ³n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:14:11.949413Z",
     "start_time": "2020-08-22T10:13:29.564468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(H=3)\n",
    "hist = fit(model, dataloader, epochs=30, log_each=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Nuestro modelo simple no es capaz de alcanzar la mÃ¡xima precisiÃ³n en los datos de entrenamiento, esta es la clara seÃ±al de *underfitting* y en este caso simplemente tendremos que aumentar el nÃºmero de parÃ¡metros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En el caso contrario, si nuestro modelo tiene mucha capacidad (mÃ¡s parÃ¡metros de los necesarios) se ajustarÃ¡ muy bien a los datos de entrenamiento pero no serÃ¡ capaz de generalizar a datos nos vistos durante el entrenamiento. Esto es lo que llamamos *overfitting*, y es el fenÃ³meno que estamos observando en nuestro ejemplo. Nuestro objetivo serÃ¡ encontrar un modelo con la capacidad \"correcta\", algo de lo que hablaremos en futuros posts.\n",
    "\n",
    "En este punto, podrÃ­as estar tentado a probar diferentes arquitecturas, modificando el nÃºmero de capas del `MLP` por ejemplo, y utilizar la mejor combinaciÃ³n de parÃ¡metros que maximice la mÃ©trica en el conjunto de test. Sin embargo, esta aproximaciÃ³n no es la correcta ya que lo Ãºnico que conseguiriamos serÃ­a hacer *overfitting* al conjunto de test. Es por esto que utilizamos los datos de *test* para evaluar nuestro modelo, y para elegir el mejor modelo utilizamos un tercer conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## El conjunto de *validaciÃ³n*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "El conjunto de datos de validaciÃ³n nos servirÃ¡ para iterar nuestro modelo, probar varias versiones para encontrar aquella con mejor capacidad que, Ãºltimamente, evaluaremos con los datos de test. Si tanto el conjunto de test como el de evaluaciÃ³n son representativos, podemos confiar que las mÃ©tricas obtenidas en validaciÃ³n se transferirÃ¡n a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:14:32.030380Z",
     "start_time": "2020-08-22T10:14:31.856392Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = X[:50000] / 255., X[50000:60000] / 255., X[60000:] / 255.\n",
    "y_train, y_val, y_test = Y[:50000].astype(np.int), Y[50000:60000].astype(np.int), Y[60000:].astype(np.int)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:14:32.110372Z",
     "start_time": "2020-08-22T10:14:32.031373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': Dataset(X_train, y_train),\n",
    "    'val': Dataset(X_val, y_val)\n",
    "}\n",
    "\n",
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=100, shuffle=True),\n",
    "    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=1000, shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:14:32.797735Z",
     "start_time": "2020-08-22T10:14:32.782735Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit(model, dataloader, epochs=10, log_each=1):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "    l, acc = [], []\n",
    "    val_l, val_acc = [], []\n",
    "    for e in range(1, epochs+1): \n",
    "        _l, _acc = [], []\n",
    "        model.train()\n",
    "        for x_b, y_b in dataloader['train']:\n",
    "            y_pred = model(x_b)\n",
    "            loss = criterion(y_pred, y_b)\n",
    "            _l.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_probas = torch.argmax(softmax(y_pred), axis=1)            \n",
    "            _acc.append(accuracy_score(y_b.cpu().numpy(), y_probas.cpu().detach().numpy()))\n",
    "        l.append(np.mean(_l))\n",
    "        acc.append(np.mean(_acc))\n",
    "        model.eval()\n",
    "        _l, _acc = [], []\n",
    "        with torch.no_grad():\n",
    "            for x_b, y_b in dataloader['val']:\n",
    "                y_pred = model(x_b)\n",
    "                loss = criterion(y_pred, y_b)\n",
    "                _l.append(loss.item())\n",
    "                y_probas = torch.argmax(softmax(y_pred), axis=1)            \n",
    "                _acc.append(accuracy_score(y_b.cpu().numpy(), y_probas.cpu().numpy()))\n",
    "        val_l.append(np.mean(_l))\n",
    "        val_acc.append(np.mean(_acc))\n",
    "        if not e % log_each:\n",
    "            print(f\"Epoch {e}/{epochs} loss {l[-1]:.5f} acc {acc[-1]:.5f} val_loss {val_l[-1]:.5f} val_acc {val_acc[-1]:.5f}\")\n",
    "    return {'epoch': list(range(1, epochs+1)), 'loss': l, 'acc': acc, 'val_loss': val_l, 'val_acc': val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:15:12.613995Z",
     "start_time": "2020-08-22T10:14:32.887804Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader, epochs=30, log_each=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:15:12.910609Z",
     "start_time": "2020-08-22T10:15:12.614995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200, figsize=(10,3))\n",
    "ax = plt.subplot(121)\n",
    "pd.DataFrame(hist).plot(x='epoch', y=['loss', 'val_loss'], grid=True, ax=ax)\n",
    "ax = plt.subplot(122)\n",
    "pd.DataFrame(hist).plot(x='epoch', y=['acc', 'val_acc'], grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes observar en las curvas de entrenamiento, la *loss* de entrenamiento disminuye mientras la precisiÃ³n aumenta hasta llegar al valor mÃ¡ximo. Sin embargo, si observamos las mÃ©tricas de validaciÃ³n, Ã©stas se estancan en el valor que hemos obtenido antes al evaluar nuestro modelo en los datos de test. Esta es la seÃ±al clara de que nuestro modelos estÃ¡ haciendo *overfitting* a los datos de entrenamiento, lo cual implica que no serÃ¡ capaz de generalizar bien a nuevos datos. \n",
    "\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-ad7d9595f354c89dc3d9245de5b1ebf6.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "SerÃ¡ pues el objetivo de los prÃ³ximos posts ver cÃ³mo tratamos con el *overfitting* para intentar reducir al mÃ¡ximo su impacto, ya que cÃ³mo hemos visto si no hacemos nada nuestros modelos tenderÃ¡n a hacer *overfitting* (este es el sino de las `redes neuronales`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ValidaciÃ³n cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que para evaluar correctamente un modelo necesitamos un conjunto de datos de *test*. AdemÃ¡s para poder iterar diferentes versiones de un modelo, necesitamos un conjunto de *validaciÃ³n*. Este es el procedimiento mÃ¡s utilizado cuando trabajamos con modelos o datasets muy grandes que requieren muchos recursos computacionales. Sin embargo existe una mejor aproximaciÃ³n que, si nos la podemos permitir, nos darÃ¡ muchos mejores resultados: la validaciÃ³n cruzada.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tÃ©cnica consiste en dividir nuestro conjunto de datos de entrenamiento en diferentes paquetes, o *folds* en inglÃ©s, y entrenar tantos modelos como *folds* tengamos, utilizando un *fold* diferente para validar en cada caso y entrenando con el resto de *folds*. De esta manera, habremos entrenado y validado con todos los datos de entrenamiento. Esta tÃ©cnica permite, ademÃ¡s, utilizar todos los modelos entrenados para generar las predicciones finales en los que se conoce como un ensamblado de modelos. La idea es que las predicciones generadas por este ensamblado serÃ¡n mejores que cualquier predicciÃ³n hecha por un modelo individual, ya que las debilidades de un modelo serÃ¡n compensadas por el resto. Esta tÃ©cnica es muy utilizada en competiciones para sacar ese extra de precisiÃ³n final que puede marcar la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:15:26.383615Z",
     "start_time": "2020-08-22T10:15:26.209904Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "FOLDS = 5\n",
    "kf = KFold(n_splits=FOLDS)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000] / 255., X[60000:] / 255., Y[:60000].astype(np.int), Y[60000:].astype(np.int)\n",
    "\n",
    "X_train.shape, X_test.shape, kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:15:34.858368Z",
     "start_time": "2020-08-22T10:15:26.618963Z"
    }
   },
   "outputs": [],
   "source": [
    "train_accs, val_accs = [], []\n",
    "for k, (train_index, val_index) in enumerate(kf.split(X_test)):\n",
    "    print(\"Fold:\", k+1)\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    dataset = {\n",
    "        'train': Dataset(X_train_fold, y_train_fold),\n",
    "        'val': Dataset(X_val_fold, y_val_fold)\n",
    "    }\n",
    "\n",
    "    dataloader = {\n",
    "        'train': torch.utils.data.DataLoader(dataset['train'], batch_size=100, shuffle=True),\n",
    "        'val': torch.utils.data.DataLoader(dataset['val'], batch_size=1000, shuffle=False)\n",
    "    }\n",
    "    \n",
    "    model = build_model()\n",
    "    hist = fit(model, dataloader)\n",
    "    \n",
    "    train_accs.append(hist['acc'][-1])   \n",
    "    val_accs.append(hist['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer validaciÃ³n cruzada nos permite dar un intervalo de confianza en las mÃ©tricas, lo cual nos permite conocer cÃ³mo de seguro estÃ¡ nuestro modelo en sus predicciones (informaciÃ³n que podemos tener en cuenta a la hora de escoger un modelo sobre otro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:16:26.444503Z",
     "start_time": "2020-08-22T10:16:26.435813Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(train_accs), np.std(train_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:16:26.568038Z",
     "start_time": "2020-08-22T10:16:26.560044Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(val_accs), np.std(val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto como tratar nuestros datos para entrenar un modelo de manera correcta. Esto incluye utilizar un conjunto de validaciÃ³n para iterar nuestro modelo y encontrar su capacidad \"correcta\" y un conjunto de test para evaluar nuestro modelo final. Ambos conjuntos no son utilizados durante el entrenamiento. TambiÃ©n hemos hablado de la capacidad de un modelo, que puede dar como resultado los fenÃ³menos de *underfitting* u *overfitting* dependiendo si la `red neurona` tiene poco parÃ¡metros o demasiados para representar nuestros datos, respectivamente. La aproximaciÃ³n mÃ¡s comÃºn a este problema es la de sobreparametrizar nuestro modelo y luego aplicar tÃ©cnicas de las cuales hablaremos mÃ¡s adelante para intentar reducir al mÃ¡ximo el *overfitting*. Por Ãºltimo, tambiÃ©n hemos hablado sobre la validaciÃ³n cruzada, una tÃ©cnica de validaciÃ³n muy interesante que debemos aplicar siempre que podemos permitÃ­rnoslo y que tambiÃ©n nos permite utilizar varios modelos en un ensamblado para mejorar las mÃ©tricas finales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
