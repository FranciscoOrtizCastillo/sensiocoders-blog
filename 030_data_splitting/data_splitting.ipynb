{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/030_data_splitting/data_splitting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjuntos de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos presentado la mayor√≠a de conceptos fundamentales necesarios para entender c√≥mo funciona una `red neuronal`, implementando desde cero los algoritmos que necesitamos para el dise√±o y entrenamiento de estos modelos. Sin embargo, de ahora en adelante utilizaremos la librer√≠a de `redes neuronales` [Pytorch](https://pytorch.org/), la cual hemos introducido en los posts anteriores. A pesar de que ya conocemos c√≥mo entrenar modelos simples de `Perceptr√≥n Multicapa` en tareas de regresi√≥n y clasificaci√≥n, es posible que no obtengas los resultados deseados (al menos comparados con resultados que puedes encontrar en otras referencia). En los pr√≥ximos posts nos vamos a centrar en t√©cnicas concretas que van a permitirnos entrenar los mejores modelos posibles, terminando con una \"receta\" que podemos utilizar en nuestros proyectos. En este post empezamos viendo t√©cnicas de divisi√≥n de datos necesarias para entrenar y validar nuestras `redes neuronales` de forma correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vamos a entrenar un `MLP` para clasificaci√≥n de im√°genes con el dataset MNIST, algo que ya hemos hecho en los posts anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T09:59:34.976907Z",
     "start_time": "2020-08-22T09:59:33.720873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Empezamos descargando nuestro dataset. En este primer ejemplo utilizaremos todos los datos disponibles para entrenar nuestra red. Al fin y al cabo, sabemos que cu√°ntos m√°s datos tengamos, mejor ser√° nuestro modelo. As√≠ que, ¬øpor qu√© no usarlos todos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T09:59:53.224131Z",
     "start_time": "2020-08-22T09:59:34.977908Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paco/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mnist \u001b[38;5;241m=\u001b[39m fetch_openml(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist_784\u001b[39m\u001b[38;5;124m'\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m mnist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m], mnist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m, Y\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m      8\u001b[0m X_train\u001b[38;5;241m.\u001b[39mshape, y_train\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# descarga datos\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1,parser='auto')\n",
    "X, Y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X_train, y_train = X / 255., Y.astype(np.int)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:48.861556Z",
     "start_time": "2020-08-22T10:11:48.782563Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X).float().cuda()\n",
    "        self.Y = torch.from_numpy(Y).long().cuda()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, ix):\n",
    "        return self.X[ix], self.Y[ix]\n",
    "    \n",
    "dataset = Dataset(X_train, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> üí° Si no est√°s familiarizado con `Pytorch`, te recomiendo que le eches un vistazo a nuestra [playlist](https://youtu.be/WL50sQVdQFg) para aprender a usar esta librer√≠a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:49.053120Z",
     "start_time": "2020-08-22T10:11:49.039118Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model(D_in=784, H=100, D_out=10):\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(D_in, H),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(H, D_out),\n",
    "    ).to(\"cuda\")\n",
    "    return model\n",
    "\n",
    "def fit(model, dataloader, epochs=10, log_each=1):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "    l = []\n",
    "    model.train()\n",
    "    for e in range(1, epochs+1): \n",
    "        _l = []\n",
    "        for x_b, y_b in dataloader:\n",
    "            y_pred = model(x_b)\n",
    "            loss = criterion(y_pred, y_b)\n",
    "            _l.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        l.append(np.mean(_l))\n",
    "        if not e % log_each:\n",
    "            print(f\"Epoch {e}/{epochs} loss {l[-1]:.5f}\")\n",
    "    return {'epoch': list(range(1, epochs+1)), 'loss': l}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:58.948180Z",
     "start_time": "2020-08-22T10:11:49.166441Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Muy bien, hemos entrenado nuestro clasificador de im√°genes. ¬øC√≥mo sabemos si es bueno? De momento la √∫nica informaci√≥n que tenemos es el valor de la funci√≥n de p√©rdida, la medida del error que nuestro clasificador comete durante el entrenamiento. Esta valor decrece a medida que pasan las *epochs*, lo cual indica que nuestra `red` est√° aprendiendo y mejorando. Podemos observar esto la siguiente im√°gen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:59.059221Z",
     "start_time": "2020-08-22T10:11:58.950179Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100)\n",
    "ax = plt.subplot(111)\n",
    "pd.DataFrame(hist).plot(x='epoch', grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "El siguiente paso podr√≠a ser evaluar diferentes **m√©tricas**. En este [post](https://sensioai.com/blog/016_metricas_clasficiacion) hablamos extendidamente de este tema. La m√©trica m√°s sencilla que podemos usar para evaluar un clasificador es la precisi√≥n (cu√°ntas im√°genes nuestro modelo clasifica correctamente). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:59.075237Z",
     "start_time": "2020-08-22T10:11:59.060234Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(axis=-1,keepdims=True)\n",
    "\n",
    "def evaluate(x):\n",
    "    model.eval()\n",
    "    y_pred = model(x)\n",
    "    y_probas = softmax(y_pred)\n",
    "    return torch.argmax(y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:59.153439Z",
     "start_time": "2020-08-22T10:11:59.076239Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = evaluate(torch.from_numpy(X_train).float().cuda())\n",
    "accuracy_score(y_train, y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Superamos el $99 \\%$ de precisi√≥n, ¬°nada mal!. Podemos incluir esta m√©trica en nuestro bucle de entrenamiento para ver su evoluci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:11:59.169442Z",
     "start_time": "2020-08-22T10:11:59.154439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit(model, dataloader, epochs=10, log_each=1):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "    model.train()\n",
    "    l, acc = [], []\n",
    "    for e in range(1, epochs+1): \n",
    "        _l, _acc = [], []\n",
    "        for x_b, y_b in dataloader:\n",
    "            y_pred = model(x_b)\n",
    "            loss = criterion(y_pred, y_b)\n",
    "            _l.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_probas = torch.argmax(softmax(y_pred), axis=1)            \n",
    "            _acc.append(accuracy_score(y_b.cpu().numpy(), y_probas.cpu().detach().numpy()))\n",
    "        l.append(np.mean(_l))\n",
    "        acc.append(np.mean(_acc))\n",
    "        if not e % log_each:\n",
    "            print(f\"Epoch {e}/{epochs} loss {l[-1]:.5f} acc {acc[-1]:.5f}\")\n",
    "    return {'epoch': list(range(1, epochs+1)), 'loss': l, 'acc': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:10.685015Z",
     "start_time": "2020-08-22T10:11:59.171442Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:10.921875Z",
     "start_time": "2020-08-22T10:12:10.686016Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200, figsize=(10,3))\n",
    "ax = plt.subplot(121)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='loss', grid=True, ax=ax)\n",
    "ax = plt.subplot(122)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='acc', grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hemos entrenado un clasificador de im√°genes que alcanza el $99 \\%$ de precisi√≥n en los datos utilizados para entrenar, adem√°s viendo las curvas de entrenamiento parece que si entrenamos durante m√°s *epochs*, podr√≠amos incluso mejorar este resultado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:46.410853Z",
     "start_time": "2020-08-22T10:12:10.924875Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader, epochs=30, log_each=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:46.646171Z",
     "start_time": "2020-08-22T10:12:46.411852Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200, figsize=(10,3))\n",
    "ax = plt.subplot(121)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='loss', grid=True, ax=ax)\n",
    "ax = plt.subplot(122)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='acc', grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Llegamos a alcanzar el $100 \\%$ de precisi√≥n. Nuestro modelo clasifica correctamente todas la im√°genes en el dataset. ¬øEs posible que hayamos entrenado el mejor modelo de la historia?. Lamentablemente, la respuesta es NO. El procedimiento que hemos seguido tiene un fallo, y es que cuando este modelo est√© trabajando en el mundo real, las im√°genes que va a recibir ser√°n diferentes a las utilizadas durante el entrenamiento, y ahora mismo no tenemos ni idea de c√≥mo se va a comportar. Es por esto que necesitamos evaluar nuestro modelo en un conjunto de im√°genes que no hayamos usado para entrenar para hacernos una idea de la *performance* de nuestro modelo en el mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## El conjunto de *test*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Para evaluar un modelo una vez entrenado, usamos el conjunto de datos de *test*. En multitud de ocasiones, este conjunto nos vendr√° dado directamente, sobretodo si el dataset con el que trabajamos se utiliza para comparar diferentes modelos en, por ejemplo, una competici√≥n o un *benchmark*. Adem√°s, este conjunto de datos suele estar oculto y protegido de manera que todos los modelos sean siempre evaluados en el mismo conjunto de datos y, adem√°s, en un conjunto de datos que nadie conoce. Esta es la manera m√°s justa de comparar modelos, y la que utilizaremos nosotros para evaluar nuestros modelos una vez entrenados. En nuestro caso, simplemente separaremos un conjunto de im√°genes de nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:46.834174Z",
     "start_time": "2020-08-22T10:12:46.647173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000] / 255., X[60000:] / 255., Y[:60000].astype(np.int), Y[60000:].astype(np.int)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ahora entrenaremos nuestro modelo s√≥lo con las im√°genes de entrenamiento, y lo evaluaremos en las im√°genes de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:12:46.914170Z",
     "start_time": "2020-08-22T10:12:46.835173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(X_train, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:13:29.294456Z",
     "start_time": "2020-08-22T10:12:46.915170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader, epochs=30, log_each=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:13:29.531467Z",
     "start_time": "2020-08-22T10:13:29.295457Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200, figsize=(10,3))\n",
    "ax = plt.subplot(121)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='loss', grid=True, ax=ax)\n",
    "ax = plt.subplot(122)\n",
    "pd.DataFrame(hist).plot(x='epoch', y='acc', grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:13:29.563467Z",
     "start_time": "2020-08-22T10:13:29.533467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = evaluate(torch.from_numpy(X_test).float().cuda())\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Oh oh... Nuestro modelo es perfecto en los datos de entrenamiento, sin embargo al evaluarlo en los datos de *test* (recuerda, datos no usados para entrenar) la precisi√≥n se reduce. Este fen√≥meno se conoce por el nombre de *overfitting*, y se da cuando un modelo se ajusta muy bien a los datos de entrenamiento pero luego falla en datos nuevos, no vistos en el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Capacidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hablamos de la capacidad de un modelo para referirnos a su potencia a la hora de representar un conjunto de datos.\n",
    "\n",
    "\n",
    "![](https://i0.wp.com/www.aprendemachinelearning.com/wp-content/uploads/2017/12/generalizacion-machine-learning.png?resize=525%2C211)\n",
    "\n",
    "Si un modelo tiene poca capacidad, observaremos *underfitting*. El modelo no tiene suficientes par√°metros para representar correctamente un dataset y tendremos una mala precisi√≥n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:14:11.949413Z",
     "start_time": "2020-08-22T10:13:29.564468Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model(H=3)\n",
    "hist = fit(model, dataloader, epochs=30, log_each=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Nuestro modelo simple no es capaz de alcanzar la m√°xima precisi√≥n en los datos de entrenamiento, esta es la clara se√±al de *underfitting* y en este caso simplemente tendremos que aumentar el n√∫mero de par√°metros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En el caso contrario, si nuestro modelo tiene mucha capacidad (m√°s par√°metros de los necesarios) se ajustar√° muy bien a los datos de entrenamiento pero no ser√° capaz de generalizar a datos nos vistos durante el entrenamiento. Esto es lo que llamamos *overfitting*, y es el fen√≥meno que estamos observando en nuestro ejemplo. Nuestro objetivo ser√° encontrar un modelo con la capacidad \"correcta\", algo de lo que hablaremos en futuros posts.\n",
    "\n",
    "En este punto, podr√≠as estar tentado a probar diferentes arquitecturas, modificando el n√∫mero de capas del `MLP` por ejemplo, y utilizar la mejor combinaci√≥n de par√°metros que maximice la m√©trica en el conjunto de test. Sin embargo, esta aproximaci√≥n no es la correcta ya que lo √∫nico que conseguiriamos ser√≠a hacer *overfitting* al conjunto de test. Es por esto que utilizamos los datos de *test* para evaluar nuestro modelo, y para elegir el mejor modelo utilizamos un tercer conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## El conjunto de *validaci√≥n*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "El conjunto de datos de validaci√≥n nos servir√° para iterar nuestro modelo, probar varias versiones para encontrar aquella con mejor capacidad que, √∫ltimamente, evaluaremos con los datos de test. Si tanto el conjunto de test como el de evaluaci√≥n son representativos, podemos confiar que las m√©tricas obtenidas en validaci√≥n se transferir√°n a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:14:32.030380Z",
     "start_time": "2020-08-22T10:14:31.856392Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = X[:50000] / 255., X[50000:60000] / 255., X[60000:] / 255.\n",
    "y_train, y_val, y_test = Y[:50000].astype(np.int), Y[50000:60000].astype(np.int), Y[60000:].astype(np.int)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:14:32.110372Z",
     "start_time": "2020-08-22T10:14:32.031373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': Dataset(X_train, y_train),\n",
    "    'val': Dataset(X_val, y_val)\n",
    "}\n",
    "\n",
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=100, shuffle=True),\n",
    "    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=1000, shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:14:32.797735Z",
     "start_time": "2020-08-22T10:14:32.782735Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fit(model, dataloader, epochs=10, log_each=1):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "    l, acc = [], []\n",
    "    val_l, val_acc = [], []\n",
    "    for e in range(1, epochs+1): \n",
    "        _l, _acc = [], []\n",
    "        model.train()\n",
    "        for x_b, y_b in dataloader['train']:\n",
    "            y_pred = model(x_b)\n",
    "            loss = criterion(y_pred, y_b)\n",
    "            _l.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            y_probas = torch.argmax(softmax(y_pred), axis=1)            \n",
    "            _acc.append(accuracy_score(y_b.cpu().numpy(), y_probas.cpu().detach().numpy()))\n",
    "        l.append(np.mean(_l))\n",
    "        acc.append(np.mean(_acc))\n",
    "        model.eval()\n",
    "        _l, _acc = [], []\n",
    "        with torch.no_grad():\n",
    "            for x_b, y_b in dataloader['val']:\n",
    "                y_pred = model(x_b)\n",
    "                loss = criterion(y_pred, y_b)\n",
    "                _l.append(loss.item())\n",
    "                y_probas = torch.argmax(softmax(y_pred), axis=1)            \n",
    "                _acc.append(accuracy_score(y_b.cpu().numpy(), y_probas.cpu().numpy()))\n",
    "        val_l.append(np.mean(_l))\n",
    "        val_acc.append(np.mean(_acc))\n",
    "        if not e % log_each:\n",
    "            print(f\"Epoch {e}/{epochs} loss {l[-1]:.5f} acc {acc[-1]:.5f} val_loss {val_l[-1]:.5f} val_acc {val_acc[-1]:.5f}\")\n",
    "    return {'epoch': list(range(1, epochs+1)), 'loss': l, 'acc': acc, 'val_loss': val_l, 'val_acc': val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:15:12.613995Z",
     "start_time": "2020-08-22T10:14:32.887804Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "hist = fit(model, dataloader, epochs=30, log_each=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:15:12.910609Z",
     "start_time": "2020-08-22T10:15:12.614995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200, figsize=(10,3))\n",
    "ax = plt.subplot(121)\n",
    "pd.DataFrame(hist).plot(x='epoch', y=['loss', 'val_loss'], grid=True, ax=ax)\n",
    "ax = plt.subplot(122)\n",
    "pd.DataFrame(hist).plot(x='epoch', y=['acc', 'val_acc'], grid=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Como puedes observar en las curvas de entrenamiento, la *loss* de entrenamiento disminuye mientras la precisi√≥n aumenta hasta llegar al valor m√°ximo. Sin embargo, si observamos las m√©tricas de validaci√≥n, √©stas se estancan en el valor que hemos obtenido antes al evaluar nuestro modelo en los datos de test. Esta es la se√±al clara de que nuestro modelos est√° haciendo *overfitting* a los datos de entrenamiento, lo cual implica que no ser√° capaz de generalizar bien a nuevos datos. \n",
    "\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-ad7d9595f354c89dc3d9245de5b1ebf6.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ser√° pues el objetivo de los pr√≥ximos posts ver c√≥mo tratamos con el *overfitting* para intentar reducir al m√°ximo su impacto, ya que c√≥mo hemos visto si no hacemos nada nuestros modelos tender√°n a hacer *overfitting* (este es el sino de las `redes neuronales`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validaci√≥n cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que para evaluar correctamente un modelo necesitamos un conjunto de datos de *test*. Adem√°s para poder iterar diferentes versiones de un modelo, necesitamos un conjunto de *validaci√≥n*. Este es el procedimiento m√°s utilizado cuando trabajamos con modelos o datasets muy grandes que requieren muchos recursos computacionales. Sin embargo existe una mejor aproximaci√≥n que, si nos la podemos permitir, nos dar√° muchos mejores resultados: la validaci√≥n cruzada.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta t√©cnica consiste en dividir nuestro conjunto de datos de entrenamiento en diferentes paquetes, o *folds* en ingl√©s, y entrenar tantos modelos como *folds* tengamos, utilizando un *fold* diferente para validar en cada caso y entrenando con el resto de *folds*. De esta manera, habremos entrenado y validado con todos los datos de entrenamiento. Esta t√©cnica permite, adem√°s, utilizar todos los modelos entrenados para generar las predicciones finales en los que se conoce como un ensamblado de modelos. La idea es que las predicciones generadas por este ensamblado ser√°n mejores que cualquier predicci√≥n hecha por un modelo individual, ya que las debilidades de un modelo ser√°n compensadas por el resto. Esta t√©cnica es muy utilizada en competiciones para sacar ese extra de precisi√≥n final que puede marcar la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:15:26.383615Z",
     "start_time": "2020-08-22T10:15:26.209904Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "FOLDS = 5\n",
    "kf = KFold(n_splits=FOLDS)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000] / 255., X[60000:] / 255., Y[:60000].astype(np.int), Y[60000:].astype(np.int)\n",
    "\n",
    "X_train.shape, X_test.shape, kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:15:34.858368Z",
     "start_time": "2020-08-22T10:15:26.618963Z"
    }
   },
   "outputs": [],
   "source": [
    "train_accs, val_accs = [], []\n",
    "for k, (train_index, val_index) in enumerate(kf.split(X_test)):\n",
    "    print(\"Fold:\", k+1)\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    dataset = {\n",
    "        'train': Dataset(X_train_fold, y_train_fold),\n",
    "        'val': Dataset(X_val_fold, y_val_fold)\n",
    "    }\n",
    "\n",
    "    dataloader = {\n",
    "        'train': torch.utils.data.DataLoader(dataset['train'], batch_size=100, shuffle=True),\n",
    "        'val': torch.utils.data.DataLoader(dataset['val'], batch_size=1000, shuffle=False)\n",
    "    }\n",
    "    \n",
    "    model = build_model()\n",
    "    hist = fit(model, dataloader)\n",
    "    \n",
    "    train_accs.append(hist['acc'][-1])   \n",
    "    val_accs.append(hist['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer validaci√≥n cruzada nos permite dar un intervalo de confianza en las m√©tricas, lo cual nos permite conocer c√≥mo de seguro est√° nuestro modelo en sus predicciones (informaci√≥n que podemos tener en cuenta a la hora de escoger un modelo sobre otro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:16:26.444503Z",
     "start_time": "2020-08-22T10:16:26.435813Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(train_accs), np.std(train_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-22T10:16:26.568038Z",
     "start_time": "2020-08-22T10:16:26.560044Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(val_accs), np.std(val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto como tratar nuestros datos para entrenar un modelo de manera correcta. Esto incluye utilizar un conjunto de validaci√≥n para iterar nuestro modelo y encontrar su capacidad \"correcta\" y un conjunto de test para evaluar nuestro modelo final. Ambos conjuntos no son utilizados durante el entrenamiento. Tambi√©n hemos hablado de la capacidad de un modelo, que puede dar como resultado los fen√≥menos de *underfitting* u *overfitting* dependiendo si la `red neurona` tiene poco par√°metros o demasiados para representar nuestros datos, respectivamente. La aproximaci√≥n m√°s com√∫n a este problema es la de sobreparametrizar nuestro modelo y luego aplicar t√©cnicas de las cuales hablaremos m√°s adelante para intentar reducir al m√°ximo el *overfitting*. Por √∫ltimo, tambi√©n hemos hablado sobre la validaci√≥n cruzada, una t√©cnica de validaci√≥n muy interesante que debemos aplicar siempre que podemos permit√≠rnoslo y que tambi√©n nos permite utilizar varios modelos en un ensamblado para mejorar las m√©tricas finales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
