{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/110_hf_intro/110_hf_intro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤗 Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Huggin Face](https://huggingface.co/) es una empresa con el objetivo de desarrollar herramientas de código abierto para el diseño, entrenamiento y puesta en producción de modelos de Inteligencia Artificial. Creada en 2016, empezó a ganar popularidad gracias a su librería [transformers](https://huggingface.co/docs/transformers/index), la cual permite crear y entrenar redes neuronales con la arquitectura [Transformer](https://arxiv.org/abs/1706.03762) de manera sencilla e independiente del framework agnóstica, pudiendo usar en el backend tanto Pytorch como Tensorflo o JAX. Hoy en día, sin embargo, ofrece muchas otras funcionalidades tales como Datasets, Modelos pre-entrenados, herramientas para el entrenamiento y puesta en producción de modelos a escala, etc. Recientemente incluso han incluído en su catálogo la librería [timm](https://huggingface.co/docs/timm/index), que hemos usado en muchos de los posts de este blog, o [diffusers](https://huggingface.co/docs/diffusers/index), para la generación de imágenes al estilo DALL-E o Stable Diffusion, indicando que Hugging Face está creciendo como comunidad, yendo más allá de los Transfromers y sus aplicaciones en tareas de lenguaje hacia otros campos como el de la visión artificial o el aprendizaje por refuerzo. En este post vamos a ver una introducción al ecosistema de Hugging Face para tareas de procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "![eco](./eco.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera pieza del ecosistema Hugging Face que vamos a ver es la librería [transformers](https://huggingface.co/docs/transformers/index). Como su propio nombre indica, nos va a permitir crear y entrenar modelos usando la arquitectura del [Transformer](https://arxiv.org/abs/1706.03762), de la cual ya hemos hablado en posts anteriores y que si no conoces te recomiendo que le eches un vistazo antes de continuar.\n",
    "\n",
    "> Puedes aprender más sobre Transformers [aquí](https://www.youtube.com/watch?v=ZpO5A9hUP6c&list=PLkgbkukKg_NpdJPhhHbemaWHNQCY-lmh1&index=28).\n",
    "\n",
    "Esto nos permitirá entrenar modelos como los derivados de las familias GPT, BERT, etc (siempre y cuando tengamos los recursos computacionales necesarios). Y, gracias a la librería, podremos hacerlo con una interfaz unificada, usando nuestro framework favorito (aquí usaremos Pytorch) y sin tener que preocuparnos por aspectos tediosos como el pre-procesado de los datos o su entrenamiento de forma eficaz. Empezaremos instalando la librería con el comando\n",
    "\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "La primera entidad que debemos conocer en esta librería es la `pipeline`, una capa de alto nivel que nos permite resolver una tarea de manera sencilla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998134970664978}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "classifier(\"We are very happy to learn about the 🤗 Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sencillo, ¿verdad?. Simplemente indicando la tarea que queremos resolver (puedes encontrarlas todas [aquí](https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/pipelines#transformers.pipeline)) y pasando un ejemplo de input, la `pipeline` nos devuelve el output. La primera vez que lo ejecutes verás que se descargan varios archivos, y esto es porque la librería usará un modelo por defecto. Aún así, podemos indicar el modelo que queremos usar fácilmente, seleccionando uno de su enorme [lista](https://huggingface.co/models). Vamos a ver un ejemplo para traducir texto de castellano a inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/miniconda3/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hey, how are you?'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traductor = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "traductor(\"Hola, ¿cómo estás?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera sencilla puedes resolver otras tareas como la generación de texto, análisis de sentimiento, generar resúmenes, responder preguntas, etc, con cualquiera de los modelos usados hoy en día como GPT-3, RoBERTa y muchos otros.\n",
    "\n",
    "![models](./models.png)\n",
    "\n",
    "Si indagamos en el funcionamiento de una `pipeline` podemos ver que está formada por 3 elementos\n",
    "\n",
    "![](./pipeline.png)\n",
    "\n",
    "La librería transformers nos da acceso a ellos por si queremos crear flujos más complejos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2428, 5959, 2122, 8466, 1012,  102],\n",
       "        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I really enjoy these posts.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos instanciar cualquier tokenizador usando el `checkpoint` del modelo que queramos. Una vez descargado, podremos pasarle texto para que nos devuelva los tensores que necesitamos como inputs de un modelo. De la misma manera, podemos crear un modelo y obtener sus outputs (los `logits`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.5305,  0.1890,  0.3755,  ...,  0.6808,  1.1315, -0.3421],\n",
       "         [ 0.8623,  0.3477,  0.3510,  ...,  0.5624,  1.2217, -0.3176],\n",
       "         [ 0.9040,  0.2511,  0.3321,  ...,  0.5247,  1.1064, -0.4115],\n",
       "         ...,\n",
       "         [ 0.6443,  0.2357,  0.4324,  ...,  0.4932,  1.0420, -0.1390],\n",
       "         [ 1.0914,  0.1559,  0.7788,  ...,  0.7382,  0.6909, -0.8387],\n",
       "         [ 0.9780,  0.1948,  0.6950,  ...,  0.7812,  0.8553, -0.7071]],\n",
       "\n",
       "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
       "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
       "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
       "         ...,\n",
       "         [-0.2443,  0.7035, -0.1199,  ..., -0.3341, -0.9158,  0.1711],\n",
       "         [-0.0289,  0.9006, -0.2142,  ..., -0.2135, -0.9266, -0.1390],\n",
       "         [ 0.0472,  0.3603, -0.1729,  ..., -0.3191, -0.9299, -0.1047]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, necesitamos convertir las salidas del modelo a algo que podamos entender, lo cual dependerá de la tarea en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `AutoModel` nos permite acceder a las salidas de un modelo, mientras que para post-procesar las salidas para una tarea concreta deberemos escoger el modelo adecuado de [esta lista](https://huggingface.co/docs/transformers/model_doc/auto).\n",
    "\n",
    "Finalmente, podemos ir más allá e instanciar directamente el modelo que queramos de aquellos [disponibles](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# random init\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# pre-entrando\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería [datasets](https://huggingface.co/docs/datasets/index) nos da acceso al catálogo de datasets no sólo para tareas de NLP sino para otras como computer vision o audio. Además, nos provee de funcionalidades para el procesado de los datos, trabajar con datos locales y métricas para la evaluación de los modelos. Para empezar, vamos a instalar la librería\n",
    "\n",
    "```\n",
    "pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos explorar la lista de datasets [disponibles](https://huggingface.co/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12334,\n",
       " ['acronym_identification',\n",
       "  'ade_corpus_v2',\n",
       "  'adversarial_qa',\n",
       "  'aeslc',\n",
       "  'afrikaans_ner_corpus'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import list_datasets, load_dataset\n",
    "\n",
    "all_datasets = list_datasets()\n",
    "len(all_datasets), all_datasets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y descargar el que queramos usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (/home/juan/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07344d3e60334eca9dc5d97963f87a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible cargar tus propios datos, por ejemplo podemos cargar El Quijote que usamos hace unos cuantos posts.\n",
    "\n",
    "![](./data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-12ddfefd75f12996\n",
      "Found cached dataset text (/home/juan/.cache/huggingface/datasets/text/default-12ddfefd75f12996/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32917be6704c42488936127b340744e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2186\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_quijote = load_dataset(\"text\", data_files=\"https://mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud/el_quijote.txt\")\n",
    "el_quijote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['DON QUIJOTE DE LA MANCHA',\n",
       "  'Miguel de Cervantes Saavedra',\n",
       "  '',\n",
       "  'PRIMERA PARTE',\n",
       "  'CAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha',\n",
       "  'En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor. Una olla de algo más vaca que carnero, salpicón las más noches, duelos y quebrantos los sábados, lentejas los viernes, algún palomino de añadidura los domingos, consumían las tres partes de su hacienda. El resto della concluían sayo de velarte, calzas de velludo para las fiestas con sus pantuflos de lo mismo, los días de entre semana se honraba con su vellori de lo más fino. Tenía en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que así ensillaba el rocín como tomaba la podadera. Frisaba la edad de nuestro hidalgo con los cincuenta años, era de complexión recia, seco de carnes, enjuto de rostro; gran madrugador y amigo de la caza. Quieren decir que tenía el sobrenombre de Quijada o Quesada (que en esto hay alguna diferencia en los autores que deste caso escriben), aunque por conjeturas verosímiles se deja entender que se llama Quijana; pero esto importa poco a nuestro cuento; basta que en la narración dél no se salga un punto de la verdad. Es, pues, de saber, que este sobredicho hidalgo, los ratos que estaba ocioso (que eran los más del año) se daba a leer libros de caballerías con tanta afición y gusto, que olvidó casi de todo punto el ejercicio de la caza, y aun la administración de su hacienda; y llegó a tanto su curiosidad y desatino en esto, que vendió muchas hanegas de tierra de sembradura, para comprar libros de caballerías en que leer; y así llevó a su casa todos cuantos pudo haber dellos; y de todos ningunos le parecían tan bien como los que compuso el famoso Feliciano de Silva: porque la claridad de su prosa, y aquellas intrincadas razones suyas, le parecían de perlas; y más cuando llegaba a leer aquellos requiebros y cartas de desafío, donde en muchas partes hallaba escrito: la razón de la sinrazón que a mi razón se hace, de tal manera mi razón enflaquece, que con razón me quejo de la vuestra fermosura, y también cuando leía: los altos cielos que de vuestra divinidad divinamente con las estrellas se fortifican, y os hacen merecedora del merecimiento que merece la vuestra grandeza. Con estas y semejantes razones perdía el pobre caballero el juicio, y desvelábase por entenderlas, y desentrañarles el sentido, que no se lo sacara, ni las entendiera el mismo Aristóteles, si resucitara para sólo ello. No estaba muy bien con las heridas que don Belianis daba y recibía, porque se imaginaba que por grandes maestros que le hubiesen curado, no dejaría de tener el rostro y todo el cuerpo lleno de cicatrices y señales; pero con todo alababa en su autor aquel acabar su libro con la promesa de aquella inacabable aventura, y muchas veces le vino deseo de tomar la pluma, y darle fin al pie de la letra como allí se promete; y sin duda alguna lo hiciera, y aun saliera con ello, si otros mayores y continuos pensamientos no se lo estorbaran.']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_quijote['train'][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería `datasets` ofrece funcionalidad para el procesado de los datos, sin embargo es común utilizar librerías como [Pandas](https://pandas.pydata.org/) para ello debido a su popularidad y herramientas de visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  the rock is destined to be the 21st century's ...      1\n",
       "1  the gorgeously elaborate continuation of \" the...      1\n",
       "2                     effective but too-tepid biopic      1\n",
       "3  if you sometimes like to go to the movies to h...      1\n",
       "4  emerges as something rare , an issue movie tha...      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "dataset.set_format(type='pandas')\n",
    "df = dataset['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label name\n",
       "0  the rock is destined to be the 21st century's ...      1  pos\n",
       "1  the gorgeously elaborate continuation of \" the...      1  pos\n",
       "2                     effective but too-tepid biopic      1  pos\n",
       "3  if you sometimes like to go to the movies to h...      1  pos\n",
       "4  emerges as something rare , an issue movie tha...      1  pos"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_int2str(row):\n",
    "\treturn dataset['train'].features['label'].int2str(row)\n",
    "\n",
    "df['name'] = df['label'].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    4265\n",
       "neg    4265\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['name'].value_counts().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando entrenamos modelos necesitamos acceder e iterar nuestros datos de la manera más rápida posible. La librería `datasets` está diseñada para poder trabajar de manera eficiente, aprovechando diversas estrategias como `memory mapping` o `streaming` para una máxima velocidad. Por último, puedes subir tus datasets fácilmente a través de la interfaz de Hugging Face para poder acceder a ellos desde cualquier parte o usando [huggingface_hub](https://huggingface.co/docs/huggingface_hub/index) direcamente desde tu código. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien en la sección anterior hemos visto como descargar o crear datasets, no podemos usar el texto directamente para entrenar un modelo. Para ello, primero deberemos convertir el texto a números en el proceso conocido como tokenización. La liberaría [tokenizers](https://huggingface.co/docs/tokenizers/index) ofrece diferentes estrategias de tokenización implementadas en Rust (por lo que son *blazingly fast*). Puedes instalar la librería con el comando\n",
    "\n",
    "```\n",
    "pip install tokenizers\n",
    "```\n",
    "\n",
    "aunque esta librería es usada internamente por la librería `transformers`, por lo que ya la deberías tener instalada.\n",
    "\n",
    "Como ya hemos visto anteriormente, la opción más simple es usar un tokenizador ya existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [4, 1734, 1019, 1063, 1140, 1713, 1059, 5], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "input = \"Hola, ¿qué tal?\"\n",
    "inputs = tokenizer(input)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero también podemos crear nuestros propios tokenizadores. Para ello, `tokenizers` nos da acceso a funcionalidades para: normalizar, pre-tokenizar, modelizar y post-procesar nuestro texto para dejarlo listo para entrenar modelos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](./tokenizer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La normalización consiste en la limpieza general del texto, eliminando espacios innecesarios, pasando el texto a minúsculas, quitando acentos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hola, ¿que tal?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")\n",
    "\n",
    "norm_input = tokenizer.normalizer.normalize_str(input)\n",
    "norm_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pre-tokenización consiste en separar el texto en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hola', (0, 4)),\n",
       " (',', (4, 5)),\n",
       " ('¿', (6, 7)),\n",
       " ('que', (7, 10)),\n",
       " ('tal', (11, 14)),\n",
       " ('?', (14, 15))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "pretoken_input = tokenizer.pre_tokenizer.pre_tokenize_str(norm_input)\n",
    "pretoken_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo (del tokenizador) se encargará de generar las entidades que usaremos para entrenar el modelo (la red neuroanl). Una terminología un poco confusa pero 🤷‍♂️. Existen varias opciones para ello, como `Byte-Pair encoding` o `WordPiece`. Estos modelos deben ser \"entrenados\" para generar el vocabulario, calcular frecuencia de palabras, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2022-10-23 19:18:48--  https://mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud/el_quijote.txt\n",
      "Resolving mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud (mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud)... 158.177.118.97\n",
      "Connecting to mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud (mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud)|158.177.118.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1060259 (1,0M) [application/octet-stream]\n",
      "Saving to: ‘el_quijote.txt’\n",
      "\n",
      "el_quijote.txt      100%[===================>]   1,01M   634KB/s    in 1,6s    \n",
      "\n",
      "2022-10-23 19:18:50 (634 KB/s) - ‘el_quijote.txt’ saved [1060259/1060259]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud/el_quijote.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "tokenizer.train([\"el_quijote.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', '##ol', '##a, ¿', '##que ', '##tal', '##?']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(input)\n",
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, la etapa de post-procesado añadirá los tokens especiales necesarios para entrenar el modelo (inicio y final de frase, por ejemplo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'h', '##ol', '##a, ¿', '##que ', '##tal', '##?', '[SEP]']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")\n",
    "\n",
    "encoding = tokenizer.encode(input)\n",
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ya podemos obtener los valores finales que usaremos durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 38, 218, 16578, 136, 597, 98, 3]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder re-utilizar tu tokenizador, es aconsejable añadir un decodificador para poder recuperar texto a partir de números (que usaremos para los outputs del modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hola, ¿que tal?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y puedes guardarlo en un archivo de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")\n",
    "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera que con los datasets, puedes subir tus tokenizers al HuggingFace HUB. Puedes aprender más sobre cómo crear tu propio tokenizador [aquí](https://huggingface.co/course/chapter6/8?fw=pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo simple con Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprende más sobre Huggingface con el [curso](https://huggingface.co/course/chapter1/1) gratis oficial o el [libro](https://www.amazon.es/Natural-Language-Processing-Transformers-Applications/dp/1098103246/ref=sr_1_1?__mk_es_ES=%C3%85M%C3%85%C5%BD%C3%95%C3%91&crid=2K5FSCNIFO784&keywords=nlp+with+transformers&qid=1666515915&qu=eyJxc2MiOiIwLjUzIiwicXNhIjoiMC4wMCIsInFzcCI6IjAuMDAifQ%3D%3D&sprefix=nlp+with+transformer%2Caps%2C141&sr=8-1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74dbfc52f168b3071122cf9c0781887d6121c12f9c1b29bca56ce221bccb2a07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
